,Article Title,Author,Journal Title,ISSN,ISBN,Publication Date,Volume,Issue,First Page,Page Count,Accession Number,DOI,Publisher,Doctype,Subjects,Keywords,Abstract,PLink
0,An Empirical Evaluation of Structured Argumentation Using the Toulmin Argument Formalism.,Leonard Adelman,"IEEE Transactions on Systems, Man & Cybernetics: Part A",10834427,,May2007,37,3,340,8.0,24858457,10.1109/TSMCA.2007.893488,IEEE,Article,STATISTICAL sampling; PARTICIPATION; SAMPLE size (Statistics); RESEARCH methodology; MATHEMATICAL statistics; Marketing Research and Public Opinion Polling,Argument support systems; structured argumentation tools; Toulmin argument formalism,"Some structured argumentation tools employ the Toulmin argument formalism, but no research has been performed testing this formalism's effect on argument evaluation or communication. An experiment was conducted to address this need by assessing: 1) if the process of generating Toulmin structures impacted participant (re)assessment of the soundness of an argument presented in an article and 2) if other participants thought that the structured representations adequately reflected the written argument. Results were mixed. First, generating Toulmin structures did impact the assessment of argument soundness. This was noteworthy given that participants were professionals representing the population of interest and that a weak manipulation and small sample size were used in the experiment. However, the effect was limited to the article where the argument was poorly aligned with the Toulmin formalism, and second, participants reviewing these structures found them to be less sound than the argument presented in the article itself. More generally, participants did not find it easy to generate Toulmin structures. Greater perceived difficulty in structure generation (and not generation time) was significantly correlated with the amount of change in the participants' soundness ratings, suggesting the mediating role of cognitive effort on reassessment. Generated structures varied greatly. Structures that had more total elements were easier to understand and were given better soundness ratings. These findings suggest that one needs to be cautious of the claimed value of the structured argumentation tools employing the Toulmin formalism without additional empirical research, demonstrating whether and how they can be effective cognitive aids. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part A is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24858457&site=ehost-live
1,Confirmation Bias in Complex Analyses.,Leonard Adelman,"IEEE Transactions on Systems, Man & Cybernetics: Part A",10834427,,May2008,38,3,584,9.0,31836666,10.1109/TSMCA.2008.918634,IEEE,Article,"EXPERIMENTAL design; COMPUTER systems; HEURISTIC-systematic model (Communication); HEURISTIC programming; FAILURE time data analysis; ANCHORING effect; COMPUTER software; COMPUTER simulation; ANALYSIS of variance; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Software publishers (except video game publishers); Computer systems design and related services (except video game design and development); Computer Systems Design Services",Analysis of competing hypotheses (ACH); anchoring effect; complex analysis; confirmation bias; heuristics and biases,"Most research works investigating the confirmation bias has used abstract experimental tasks where participants drew inferences from just a few items of evidence. The experiment reported in this paper investigated the confirmation bias in a complex analysis task that is more characteristic of law enforcement investigations, financial analysis, and intelligence analysis. Participants were professionals, half of whom had intelligence analysis experience. The effectiveness of a procedure designed to mitigate the confirmation bias, called analysis of competing hypotheses (ACH), was tested. Results showed a confirmation bias for both experience groups, but ACH significantly reduced bias only for participants without intelligence analysis experience. Confirmation bias manifested as a weighting bias, not as an interpretation bias. Participants tended to agree on the interpretation of evidence (i.e., whose hypothesis was supported by the evidence) but tended to disagree on the importance of the evidence-giving more weight to the evidence that supported their preferred hypothesis and less weight to evidence that disconfirmed it. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part A is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=31836666&site=ehost-live
2,Confirmation Bias in the Analysis of Remote Sensing Data.,Leonard Adelman,"IEEE Transactions on Systems, Man & Cybernetics: Part A",10834427,,Jan2009,39,1,218,9.0,39455810,10.1109/TSMCA.2008.2006372,IEEE,Article,REMOTE sensing; DATA analysis; PREJUDICES; ELECTROMAGNETIC waves; SCIENTIFIC communication; HEURISTIC-systematic model (Communication),Alternative causes approach; confirmation bias; heuristics and biases; remote sensing; technical data analysis,"Analysis of remote sensing data requires a mix of technical data analysis and expert judgment. Although there is considerable empirical evidence that expert judgments reflect substantial biases, the impact of judgment biases in remote sensing and similar types of technical data analyses has not been investigated. In particular, judgment research suggests that experts are prone to a confirmation bias-where focus on a proposed hypothesis leads the expert to seek and overweigh confirming versus disconfirming evidence. In technical data analysis, this predicts a tendency toward false positives in interpretation-concluding that sensor data support a hypothesis when they do not. In this paper, we empirically examine confirmation bias in technical data analysis of remote sensing data, along with an approach to mitigating this bias that systematically promotes consideration of alternative causes in the analysis. Results suggest that analysts do exhibit confirmation bias in their technical data analysis of remote sensing data, and furthermore, that structured consideration of alternative causes mitigates this bias. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part A is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=39455810&site=ehost-live
3,Testing the Effectiveness of Icons for Supporting Distributed Team Decision Making Under Time Pressure.,Leonard Adelman,"IEEE Transactions on Systems, Man & Cybernetics: Part A",10834427,,Mar2004,34,2,179,11.0,12464409,10.1109/TSMCA.2003.819492,IEEE,Article,DECISION making; TIME pressure; PSYCHOLOGICAL stress; PROBLEM solving; TEAMS in the workplace,Distributed decision making; human-computer interface; team decision making; time pressure.,"There has been minimal experimentation testing the effectiveness of icons (or interface features in general) on distributed team decision making. To overcome this deficiency, an experiment tested the effectiveness of a ""send"" icon to remind team members to send information to their teammates, and a ""receive"" icon to tell them when they had received information, for a simulated, military task. As predicted, the ""send"" icon was effective in maintaining information flow, particularly when time pressure was high and simulated teammates sent less information, because it reduced memory burden and supported proactive behavior. The ""receive"" icon was only effective in supporting decision accuracy when time pressure was low. As time pressure increased, participants' with the ""receive"" icon increasingly used a strategy of making decisions before reading the most important information, completely counter to expectations. These results illustrate the subtle, sometimes surprising way task characteristics (e.g., time pressure) can affect participants' strategies and, thereby, nullify the positive effect of displays on performance. The experiment also examined other task characteristics and working memory capacity, and showed how the lens model equation helped explain all effects on decision accuracy. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part A is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=12464409&site=ehost-live
4,Using Brunswikian theory and a longitudinal design to study how hierarchical teams adapt to increasing levels of time pressure,Leonard Adelman,Acta Psychologica,00016918,,Feb2003,112,2,181,26.0,8804129,10.1016/S0001-6918(02)00082-3,Elsevier B.V.,Article,DECISION making; TIME pressure,Adaption; Brunswikian theory; Team decision making; Time pressure,"Brunswikian theory and a longitudinal design were used to study how three-person, hierarchical teams adapted to increasing levels of time pressure and, thereby, try to understand why previous team research has not necessarily found a direct relationship between team processes and performance with increasing time pressure. We obtained four principal findings. First, team members initially adapted to increasing time pressure without showing any performance decrements by accelerating their cognitive processing, increasing the amount of their implicit coordination by sending more information without being asked and, to a lesser extent, filtering (omitting) certain activities. Second, teams began and continued to perform the task differently with increasing time pressure, yet often achieved comparable levels of performance. Third, time pressure did affect performance because there was a level of time pressure beyond which performance could not be maintained, although that level differed for different teams. And, fourth, some adaptation strategies were more effective than others at the highest time pressure level. Taken together, these findings support the Brunswikian perspective that one should not necessarily expect a direct relationship between team processes and performance with increasing time pressure because teams adapt their processes in different, yet often equally effective ways, in an effort to maintain high and stable performance. [Copyright &y& Elsevier] Copyright of Acta Psychologica is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=8804129&site=ehost-live
5,Using inferred probabilities to measure the accuracy of imprecise forecasts.,Leonard Adelman,Judgment & Decision Making,19302975,,Nov2012,7,6,728,13.0,83767746,,Society for Judgment & Decision Making,Article,FORECASTING; METHODOLOGY; CALIBRATION; QUANTITATIVE research; PROBABILITY theory,forecast accuracy; imprecise forecasts; imputed probability; inferred probability; judgment-based forecasting; political forecasting; probability calibration; verbal probability,"Research on forecasting is effectively limited to forecasts that are expressed with clarity; which is to say that the forecasted event must be sufficiently well-defined so that it can be clearly resolved whether or not the event occurred and forecasts certainties are expressed as quantitative probabilities. When forecasts are expressed with clarity, then quantitative measures (scoring rules, calibration, discrimination, etc.) can be used to measure forecast accuracy, which in turn can be used to measure the comparative accuracy of different forecasting methods. Unfortunately most real world forecasts are not expressed clearly. This lack of clarity extends to both the description of the forecast event and to the use of vague language to express forecast certainty. It is thus difficult to assess the accuracy of most real world forecasts, and consequently the accuracy the methods used to generate real world forecasts. This paper addresses this deficiency by presenting an approach to measuring the accuracy of imprecise real world forecasts using the same quantitative metrics routinely used to measure the accuracy of well-defined forecasts. To demonstrate applicability, the inferred probability method is applied to measure the accuracy of forecasts in fourteen documents examining complex political domains. [ABSTRACT FROM AUTHOR] Copyright of Judgment & Decision Making is the property of Society for Judgment & Decision Making and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=83767746&site=ehost-live
6,A Study of Several Classification Algorithms to Predict Students' Learning Performance.,Pouyan Ahmadi,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2019,,,2937,10.0,139581526,,ASEE,Article,,Classification algorithms; Learning Management System; navigational behavior; performance prediction,"Identifying students who need better pedagogical support is an invaluable asset for any academic institution. The main objective of this study is to predict the students' performance and thereby maximize their learning productivity. We focus on the students' past academic performance to predict their future results. This is done by analyzing the various factors of course material and students' online behavior from the Learning Management System (LMS). We also analyze several predictors that contribute to the overall student performance from the data collected. To determine the efficient model that is more accurate and precise, we compare the performance of four well-known machine learning classification algorithms. The 2017 and 2018 academic year data collected consists of user patterns, navigational behavior and the students' daily activities from the LMS, Blackboard (Bb) Learn of the Undergraduate IT program within the Information Sciences and Technology (IST) Department at George Mason University (GMU). This comparison effort will help us confirm the most effective algorithm to identify students' who are at risk of failing a class so that academic advisors/instructors can offer better academic guidance and support. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139581526&site=ehost-live
6,A Study of Several Classification Algorithms to Predict Students' Learning Performance.,Khondkar Islam,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2019,,,2937,10.0,139581526,,ASEE,Article,,Classification algorithms; Learning Management System; navigational behavior; performance prediction,"Identifying students who need better pedagogical support is an invaluable asset for any academic institution. The main objective of this study is to predict the students' performance and thereby maximize their learning productivity. We focus on the students' past academic performance to predict their future results. This is done by analyzing the various factors of course material and students' online behavior from the Learning Management System (LMS). We also analyze several predictors that contribute to the overall student performance from the data collected. To determine the efficient model that is more accurate and precise, we compare the performance of four well-known machine learning classification algorithms. The 2017 and 2018 academic year data collected consists of user patterns, navigational behavior and the students' daily activities from the LMS, Blackboard (Bb) Learn of the Undergraduate IT program within the Information Sciences and Technology (IST) Department at George Mason University (GMU). This comparison effort will help us confirm the most effective algorithm to identify students' who are at risk of failing a class so that academic advisors/instructors can offer better academic guidance and support. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139581526&site=ehost-live
7,Discovering the Top-k Unexplained Sequences in Time-Stamped Observation Data.,Massimiliano Albanese,IEEE Transactions on Knowledge & Data Engineering,10414347,,Mar2014,26,3,577,18.0,94339027,10.1109/TKDE.2013.33,IEEE,Article,MATHEMATICAL sequences; DATA analysis; APPLICATION software; WEBSITES; INTERNET security; ARTIFICIAL intelligence; KNOWLEDGE base; Software Publishers; Software publishers (except video game publishers); Custom Computer Programming Services; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals,artificial intelligence; computing methodologies; knowledge base management; Knowledge representation formalisms and methods,"There are numerous applications where we wish to discover unexpected activities in a sequence of time-stamped observation data--for instance, we may want to detect inexplicable events in transactions at a website or in video of an airport tarmac. In this paper, we start with a known set (\cal A) of activities (both innocuous and dangerous) that we wish to monitor. However, in addition, we wish to identify ""unexplained"" subsequences in an observation sequence that are poorly explained (e.g., because they may contain occurrences of activities that have never been seen or anticipated before, i.e., they are not in (\cal A)). We formally define the probability that a sequence of observations is unexplained (totally or partially) w.r.t. (\cal A). We develop efficient algorithms to identify the top-$(k)$ Totally and partially unexplained sequences w.r.t. (\cal A). These algorithms leverage theorems that enable us to speed up the search for totally/partially unexplained sequences. We describe experiments using real-world video and cyber-security data sets showing that our approach works well in practice in terms of both running time and accuracy. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94339027&site=ehost-live
8,Fast Activity Detection: Indexing for Temporal Stochastic Automaton-Based Activity Models.,Massimiliano Albanese,IEEE Transactions on Knowledge & Data Engineering,10414347,,Feb2013,25,2,360,14.0,84489405,10.1109/TKDE.2011.246,IEEE,Article,"COMPUTER systems; SECURITY systems; STOCHASTIC analysis; PROBABILISTIC automata; MACHINE theory; ASYNCHRONOUS transfer mode; WEB servers; Security Systems Services (except Locksmiths); Computer systems design and related services (except video game design and development); Computer Systems Design Services; Data Processing, Hosting, and Related Services",Activity detection; Automata; Context; Hidden Markov models; indexing; Monitoring; stochastic automata; Stochastic processes; timestamped data,"Today, numerous applications require the ability to monitor a continuous stream of fine-grained data for the occurrence of certain high-level activities. A number of computerized systems—including ATM networks, web servers, and intrusion detection systems—systematically track every atomic action we perform, thus generating massive streams of timestamped observation data, possibly from multiple concurrent activities. In this paper, we address the problem of efficiently detecting occurrences of high-level activities from such interleaved data streams. A solution to this important problem would greatly benefit a broad range of applications, including fraud detection, video surveillance, and cyber security. There has been extensive work in the last few years on modeling activities using probabilistic models. In this paper, we propose a temporal probabilistic graph so that the elapsed time between observations also plays a role in defining whether a sequence of observations constitutes an activity. We first propose a data structure called “temporal multiactivity graph” to store multiple activities that need to be concurrently monitored. We then define an index called Temporal Multiactivity Graph Index Creation (tMAGIC) that, based on this data structure, examines and links observations as they occur. We define algorithms for insertion and bulk insertion into the tMAGIC index and show that this can be efficiently accomplished. We also define algorithms to solve two problems: the “evidence” problem that tries to find all occurrences of an activity (with probability over a threshold) within a given sequence of observations, and the “identification” problem that tries to find the activity that best matches a sequence of observations. We introduce complexity reducing restrictions and pruning strategies to make the problem—which is intrinsically exponential—linear to the number of observations. Our experiments confirm that tMAGIC has time and space complexity linear to the size of the input, and can efficiently retrieve instances of the monitored activities. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=84489405&site=ehost-live
10,Applying data redundancy to differential equation solvers.,Paul Ammann,Annals of Software Engineering,10227091,,1997,4,1,65,13.0,9803764,10.1023/A:1018958526071,Springer Nature,Article,REDUNDANCY in engineering; AUTOMATIC data collection systems; INFORMATION services; SOFTWARE engineering; SOFTWARE productivity; COMPUTER programming; SOFTWARE compatibility; COMPUTER software development; EQUATIONS; Custom Computer Programming Services; Computer systems design and related services (except video game design and development); All Other Information Services; Other Computer Related Services,,"Data redundancy methods evaluate the output of a program on a given input by examining the outputs produced by the same program on additional inputs. This papers explores the use of data redundancy to detect and/or tolerate failures in differential equation solvers. Our first goal is to show that data redundancy techniques are applicable to a wide class of differential equations. Our second task is to identify circumstances in which an independence model of the sort used in program checking can be exploited to build highly reliable solvers from moderately reliable components. We conclude with illustrative examples of applying various data redundancy techniques to a standard differential equation solver. The method has potential for critical systems in which the application's control laws are specified as sets of differential equations. [ABSTRACT FROM AUTHOR] Copyright of Annals of Software Engineering is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=9803764&site=ehost-live
11,Can-Follow Concurrency Control.,Paul Ammann,IEEE Transactions on Computers,00189340,,Oct2007,56,10,1425,6.0,26820551,10.1109/TC.2007.70761,IEEE,Article,DATABASES; ELECTRONIC information resources; INTEGRATED software; INFORMATION science; INFORMATION technology; ANALYSIS of variance; ELECTRICAL engineering; NUMERICAL analysis; MATHEMATICAL analysis; Engineering Services,Can-follow; concurrency control; transaction processing,"Can-follow concurrency control permits a transaction to read (write) an item write-locked (read-locked) by another transaction with almost no delays. By combining the merits of 2PL and 2V2PL, this approach mitigates the lock contention not Only between update and read-only transactions, but also between update and update transactions. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26820551&site=ehost-live
13,TRUSTED RECOVERY.,Paul Ammann,Communications of the ACM,00010782,,Jul99,42,7,71,5.0,12513677,10.1145/306549.306580,Association for Computing Machinery,Article,INFORMATION warfare; INFORMATION technology; COMPUTER security; COMPUTER hackers; COMPUTER networks; COUNTERTERRORISM; CYBERTERRORISM; Computer Systems Design Services,,"This article emphasizes that though prevention and detection get much attention but recovery is also an equally important phase of information warfare defense. Prevention is just one phase of information warfare process and it might be detrimental for a system if it neglects an equally important recovery phase. To protect a system against information warfare, it is of course necessary to take steps to prevent attacks from succeeding. At the same time, however, it is important to recognize that not all attacks can be averted at the outset. The goal of defense is to keep available as many of the critical system elements as possible in the face of information warfare attacks. It is undesirable to use recovery techniques that require halting system operations for repair, for denial of service may be the attacker's objective, especially if it occurs at a critical time. Once a bad system element has been detected, it is essential to proceed quickly with repairs while allowing applications to continue operating even if some of the elements have been damaged by an attack.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=12513677&site=ehost-live
72,The importance of metadata to assess information content in digital reconstructions of neuronal morphology.,Giorgio Ascoli,Cell & Tissue Research,0302766X,,Apr2015,360,1,121,7.0,101805666,10.1007/s00441-014-2103-6,Springer Nature,Article,NEURONS; CELL morphology; NEUROMORPHICS; DATA mining; IMAGE reconstruction; DIGITAL images; QUANTITATIVE research,Completeness; Data standards; Digital reconstruction; Metadata; Neuron morphology,"Digital reconstructions of axonal and dendritic arbors provide a powerful representation of neuronal morphology in formats amenable to quantitative analysis, computational modeling, and data mining. Reconstructed files, however, require adequate metadata to identify the appropriate animal species, developmental stage, brain region, and neuron type. Moreover, experimental details about tissue processing, neurite visualization and microscopic imaging are essential to assess the information content of digital morphologies. Typical morphological reconstructions only partially capture the underlying biological reality. Tracings are often limited to certain domains (e.g., dendrites and not axons), may be incomplete due to tissue sectioning, imperfect staining, and limited imaging resolution, or can disregard aspects irrelevant to their specific scientific focus (such as branch thickness or depth). Gauging these factors is critical in subsequent data reuse and comparison. NeuroMorpho.Org is a central repository of reconstructions from many laboratories and experimental conditions. Here, we introduce substantial additions to the existing metadata annotation aimed to describe the completeness of the reconstructed neurons in NeuroMorpho.Org. These expanded metadata form a suitable basis for effective description of neuromorphological data. [ABSTRACT FROM AUTHOR] Copyright of Cell & Tissue Research is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101805666&site=ehost-live
75,Axonal morphometry of hippocampal pyramidal neurons semi-automatically reconstructed after in vivo labeling in different CA3 locations.,Giorgio Ascoli,Brain Structure & Function,18632653,,Mar2011,216,1,1,15.0,58132822,10.1007/s00429-010-0291-8,Springer Nature,Article,AXONS; MORPHOMETRICS; HIPPOCAMPUS (Brain); BIOLOGICAL neural networks; QUANTITATIVE research; NEURONS; NEUROSCIENCES,Axonal arbors; CA3b; CA3c; Digital morphology; Hippocampus; Principal neuron; Schaffer collateral,"xonal arbors of principal neurons form the backbone of neuronal networks in the mammalian cortex. Three-dimensional reconstructions of complete axonal trees are invaluable for quantitative analysis and modeling. However, digital data are still sparse due to labor intensity of reconstructing these complex structures. We augmented conventional tracing techniques with computational approaches to reconstruct fully labeled axonal morphologies. We digitized the axons of three rat hippocampal pyramidal cells intracellularly filled in vivo from different CA3 sub-regions: two from areas CA3b and CA3c, respectively, toward the septal pole, and one from the posterior/ventral area (CA3pv) near the temporal pole. The reconstruction system was validated by comparing the morphology of the CA3c neuron with that traced from the same cell by a different operator on a standard commercial setup. Morphometric analysis revealed substantial differences among neurons. Total length ranged from 200 (CA3b) to 500 mm (CA3c), and axonal branching complexity peaked between 1 (CA3b and CA3pv) and 2 mm (CA3c) of Euclidean distance from the soma. Length distribution was analyzed among sub-regions (CA3a,b,c and CA1a,b,c), cytoarchitectonic layers, and longitudinal extent within a three-dimensional template of the rat hippocampus. The CA3b axon extended thrice more collaterals within CA3 than into CA1. On the contrary, the CA3c projection was double into CA1 than within CA3. Moreover, the CA3b axon extension was equal between strata oriens and radiatum, while the CA3c axon displayed an oriens/radiatum ratio of 1:6. The axonal distribution of the CA3pv neuron was intermediate between those of the CA3b and CA3c neurons both relative to sub-regions and layers, with uniform collateral presence across CA3/CA1 and moderate preponderance of radiatum over oriens. In contrast with the dramatic sub-region and layer differences, the axon longitudinal spread around the soma was similar for the three neurons. To fully characterize the axonal diversity of CA3 principal neurons will require higher-throughput reconstruction systems beyond the threefold speed-up of the method adopted here. [ABSTRACT FROM AUTHOR] Copyright of Brain Structure & Function is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=58132822&site=ehost-live
86,Covariate Balancing through Naturally Occurring Strata.,Ivan Avramovic,Health Services Research,00179124,,Feb2018,53,1,273,20.0,127564141,10.1111/1475-6773.12628,Wiley-Blackwell,journal article,PROPENSITY score matching; NURSING care facilities; PUBLIC health; MEDICAL care; COMPARATIVE studies; COMPUTER simulation; RESEARCH methodology; MEDICAL cooperation; PROBABILITY theory; RESEARCH; STATISTICS; SYSTEM analysis; COMORBIDITY; LOGISTIC regression analysis; DATA analysis; EVALUATION research; SENIOR housing; UNITED States; UNITED States. Dept. of Veterans Affairs; Nursing Care Facilities (Skilled Nursing Facilities); Community care facilities for the elderly; Health and Welfare Funds; Administration of Veterans' Affairs; Assisted Living Facilities for the Elderly,Balancing databases; causal impact; confounding; prognosis; propensity scoring,"<bold>Objective: </bold>To provide an alternative to propensity scoring (PS) for the common situation where there are interacting covariates.<bold>Setting: </bold>We used 1.3 million assessments of residents of the United States Veterans Affairs nursing homes, collected from January 1, 2000, through October 9, 2012.<bold>Design: </bold>In stratified covariate balancing (SCB), data are divided into naturally occurring strata, where each stratum is an observed combination of the covariates. Within each stratum, cases with, and controls without, the target event are counted; controls are weighted to be as frequent as cases. This weighting procedure guarantees that covariates, or combination of covariates, are balanced, meaning they occur at the same rate among cases and controls. Finally, impact of the target event is calculated in the weighted data. We compare the performance of SCB, logistic regression (LR), and propensity scoring (PS) in simulated and real data. We examined the calibration of SCB and PS in predicting 6-month mortality from inability to eat, controlling for age, gender, and nine other disabilities for 296,051 residents in Veterans Affairs nursing homes. We also performed a simulation study, where outcomes were randomly generated from treatment, 10 covariates, and increasing number of covariate interactions. The accuracy of SCB, PS, and LR in recovering the simulated treatment effect was reported.<bold>Findings: </bold>In simulated environment, as the number of interactions among the covariates increased, SCB and properly specified LR remained accurate but pairwise LR and pairwise PS, the most common applications of these tools, performed poorly. In real data, application of SCB was practical. SCB was better calibrated than linear PS, the most common method of PS.<bold>Conclusions: </bold>In environments where covariates interact, SCB is practical and more accurate than common methods of applying LR and PS. [ABSTRACT FROM AUTHOR] Copyright of Health Services Research is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127564141&site=ehost-live
87,An Incremental Server for Scheduling Overloaded Real-Time Systems.,Hakan Aydin,IEEE Transactions on Computers,00189340,,Oct2003,52,10,1347,15.0,11034849,10.1109/TC.2003.1234531,IEEE,Article,"COMPUTER systems; COMPUTER algorithms; WORKLOAD of computer networks; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Computer Systems Design Services; Computer systems design and related services (except video game design and development)",,Proposes a novel scheduling framework for a real-time environment that experiences dynamic workload changes. Capability of adjusting the system workload in incremental steps under overloaded conditions; Assignment of criticality value to each task; Process of selecting tasks to discard.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=11034849&site=ehost-live
88,Dynamic modulation scaling enabled multi-hop topology control for time critical wireless sensor networks.,Hakan Aydin,Wireless Networks (10220038),10220038,,Feb2020,26,2,1203,24.0,141475264,10.1007/s11276-019-02146-9,Springer Nature,Article,WIRELESS sensor networks; ELECTRIC network topology; SOFTWARE radio; TOPOLOGY; NETWORK performance; POLYNOMIAL time algorithms; SOFTWARE measurement,,"The previous work on connection driven topology control has shown that it has significant potential to reduce energy consumption of Wireless Sensor Networks (WSNs). Dynamic Modulation Scaling (DMS) which is a technique that manages transmission power levels in order to change the number of bits encoded per symbol has a direct impact on connection driven topology control. In this paper we investigate the transmission scheduling of multi-hop real-time WSNs equipped with DMS enabled radio chips while taking the effect of DMS on topology control into account. To our best knowledge, this is the first paper that addresses this issue. The current work on DMS enabled WSN tend to rely on theoretical DMS models to predict network performance metrics. However, there is little, if any, work that is based upon empirically verified network performance outcomes using DMS especially on its effect on connection driven topology control. This paper fills this gap by using GNU Radio and Software Defined Radio hardware to show how to emulate DMS in low power wireless systems and measure the impact of varying Signal-to-Noise levels, distance and elevation on throughput and delivery rates for different DMS control strategies. Next, we present the Mixed Integer Nonlinear Optimization Problem of minimizing energy consumption of DMS enabled connection driven topology control on real-time WSNs. Lastly, we present two polynomial time heuristics and compare their performance against the optimal solution. [ABSTRACT FROM AUTHOR] Copyright of Wireless Networks (10220038) is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141475264&site=ehost-live
89,Evaluation framework for energy-aware multiprocessor scheduling in real-Time systems.,Hakan Aydin,Journal of Systems Architecture,13837621,,Sep2019,98,,388,15.0,138270784,10.1016/j.sysarc.2019.01.018,Elsevier B.V.,Article,VERNACULAR architecture; MULTIPROCESSORS; ENERGY consumption; COMPUTER scheduling; PAPER arts; ARCHITECTURE; Other Construction Material Merchant Wholesalers; Folding Paperboard Box Manufacturing,Energy management; Multicore architectures; Real-time systems; Scheduling,"Multiprocessor and multicore architectures are fast becoming the platform of choice for deploying workloads, as they have higher computing capabilities and energy efficiency than traditional architectures. In addition to time constraints, a number of real-time applications are required to operate in systems working with limited power supplies, which also imposes tight energy constraints on their execution. Therefore, it is desirable for the system to minimize its energy consumption while still achieving a satisfactory performance. Several energy-aware scheduling techniques addressing this issue have been proposed over the past few years. Unfortunately, few aspects of implementation are seldom considered in theoretical work, and only a tiny fraction of these techniques have been implemented in an actual hardware platform and evaluated by analytical methods. The work presented in this paper thus attempts to provide a prototyping and evaluation framework in which energy-aware multiprocessor scheduling algorithms can translate into full-fledged practical realizations, where their power consumption profiles can be properly measured. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138270784&site=ehost-live
90,Exact Fault-Sensitive Feasibility Analysis of Real-Time Tasks.,Hakan Aydin,IEEE Transactions on Computers,00189340,,Oct2007,56,10,1372,15.0,26820547,10.1109/TC.2007.70739,IEEE,Article,ALGORITHMS; LINEAR programming; MATHEMATICAL programming; DYNAMIC programming; INTEGER programming; MATHEMATICAL optimization; SYSTEMS engineering; COMPUTER algorithms; ANALYSIS of variance,deadline-driven systems; fault tolerance; processor demand analysis; Real-time scheduling; real-time systems; recovery blocks,"In this paper, we consider the problem of checking the feasibility of a set of n real-time tasks while provisioning for timely recovery from (at most) k transient faults. We extend the well-known processor demand approach to take into account the extra overhead that may be induced by potential recovery operations under Earliest-Deadline-First scheduling. We develop a necessary and sufficient test using a dynamic programming technique. An improvement upon the previous solutions is to address and efficiently solve the case where the recovery blocks associated with a given task do not necessarily have the same execution time. We also provide an online version of the algorithm that does not require a priori knowledge of release times. The online algorithm runs in O(m ∙ k²) time, where m is the number of ready tasks. We extend the framework to periodic execution settings: We derive a sufficient condition that can be checked efficiently for the feasibility of periodic tasks in the presence of faults. Finally, we analyze the case where the recovery blocks are to be executed nonpreemptively and we formally show that the problem becomes intractable under that assumption. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26820547&site=ehost-live
91,Exploiting primary/backup mechanism for energy efficiency in dependable real-time systems.,Hakan Aydin,Journal of Systems Architecture,13837621,,Aug2017,78,,68,13.0,124270695,10.1016/j.sysarc.2017.06.008,Elsevier B.V.,Article,ENERGY consumption; SAVINGS; ELECTRIC potential; ARRAY processors; ALGORITHMS,DPM; DVFS; Energy management; Fault tolerance; Multiprocessor; Primary/backup; Real-time systems,"Primary/Backup has been well studied as an effective fault-tolerance technique. In this paper, with the objectives of tolerating a single permanent fault and maintaining system reliability with respect to transient faults, we study dynamic-priority based energy-efficient fault-tolerance scheduling algorithms for periodic real-time tasks running on multiprocessor systems by exploiting the primary/backup technique while considering the negative effects of the widely deployed Dynamic Voltage and Frequency Scaling (DVFS) on transient faults. Specifically, by separating primary and backup tasks on their dedicated processors, we first devise two schemes based on the idea of Standby-Sparing (SS) : For Paired-SS , processors are organized as groups of two (i.e., pairs) and the existing SS scheme is applied within each pair of processors after partitioning tasks to the pairs. In Generalized-SS , processors are divided into two groups (of potentially different sizes), which are denoted as primary and secondary processor groups, respectively. The main (backup) tasks are scheduled on the primary (secondary) processor group under the partitioned-EDF ( partitioned-EDL ) with DVFS (DPM) to save energy. Moreover, we propose schemes that allocate primary and backup tasks in a mixed manner to better utilize system slack on all processors for more energy savings. On each processor, the Preference-Oriented Earliest Deadline (POED) scheduler is adopted to run primary tasks at scaled frequencies as soon as possible (ASAP) and backup tasks at the maximum frequency as late as possible (ALAP) to save energy. Our empirical evaluations show that, for systems with a given number of processors, there normally exists a configuration for Generalized-SS with different number of processors in primary and backup groups, which leads to better energy savings when compared to that of the Paired-SS scheme. Moreover, the POED-based schemes normally have more stable performance and can achieve better energy savings. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124270695&site=ehost-live
92,Multicore Mixed-Criticality Systems: Partitioned Scheduling and Utilization Bound.,Hakan Aydin,IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems,02780070,,Jan2018,37,1,21,14.0,126963990,10.1109/TCAD.2017.2697955,IEEE,Article,COMPUTER scheduling; PARALLEL algorithms; MULTICORE processors; LINUX operating systems; EMBEDDED computer systems; Computer and peripheral equipment manufacturing; Electronic Computer Manufacturing; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers,Computer science; Embedded systems; Engines; mixed-criticality (MC); Multicore processing; multicore systems; partitioned scheduling; Partitioning algorithms; Program processors; Scheduling algorithms; utilization bound,"In mixed-criticality (MC) systems, multiple activities with various certification requirements (thus with different criticality levels) can co-exist on shared hardware platforms, where multicore processors have emerged as the de facto computing engines. In this paper, by using the partitioned earliest-deadline-first with virtual deadlines (EDF-VDs) scheduler for a set of periodic MC tasks running on multicore systems, we derive a criticality-aware utilization bound for efficient feasibility tests and then identify its characteristics. Our analysis shows that the bound increases with increasing number of cores and decreasing system criticality level. We show that, since the utilizations of MC tasks at different criticality levels can vary considerably, the utilization contribution of a task on different cores may have large variations and thus can significantly affect the system schedulability under the EDF-VD scheduler. Based on these observations, we propose a novel and efficient criticality-aware task partitioning algorithm (CA-TPA) to compensate for the inherent pessimism of the utilization bound. In order to improve the system schedulability, the task priorities are determined according to their utilization contributions to the system in CA-TPA. Moreover, by analyzing the utilization variations of tasks at different levels, we develop several heuristics to minimize the utilization increment and balance the workload on cores. The simulation results show that the CA-TPA scheme is very effective in achieving higher schedulability ratio and yielding balanced workloads. The actual implementation in Linux operating system further demonstrates the applicability of CA-TPA with lower run-time overhead, compared to the existing partitioning schemes. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126963990&site=ehost-live
93,On Reliability Management of Energy-Aware Real-Time Systems Through Task Replication.,Hakan Aydin,IEEE Transactions on Parallel & Distributed Systems,10459219,,Mar2017,28,3,813,13.0,121301737,10.1109/TPDS.2016.2600595,IEEE,Article,RELIABILITY in engineering; REAL-time computing; MULTICORE processors; POWER aware computing; EMBEDDED computer systems; COMPUTER scheduling; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and peripheral equipment manufacturing; Electronic Computer Manufacturing,Energy consumption; Energy-aware systems; Management; Multicore processing; multicore systems; real-time and embedded systems; Real-time systems; reliability; scheduling; Transient analysis; Voltage control,"On emerging multicore systems, task replication is a powerful way to achieve high reliability targets. In this paper, we consider the problem of achieving a given reliability target for a set of periodic real-time tasks running on a multicore system with minimum energy consumption. Our framework explicitly takes into account the coverage factor of the fault detection techniques and the negative impact of Dynamic Voltage Scaling (DVS) on the rate of transient faults leading to soft errors. We characterize the subtle interplay between the processing frequency, replication level, reliability, fault coverage, and energy consumption on DVS-enabled multicore systems. We first develop static solutions and then propose dynamic adaptation schemes in order to reduce the concurrent execution of the replicas of a given task and to take advantage of early completions. Our simulation results indicate that through our algorithms, a very broad spectrum of reliability targets can be achieved with minimum energy consumption thanks to the judicious task replication and frequency assignment. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121301737&site=ehost-live
94,On the Interplay of Voltage/Frequency Scaling and Device Power Management for Frame-Based Real-Time Embedded Applications.,Hakan Aydin,IEEE Transactions on Computers,00189340,,Jan2012,61,1,31,0.0,67669908,10.1109/TC.2010.248,IEEE,Article,EMBEDDED computer systems; ENERGY management; QUALITY of service; CENTRAL processing units; PERFORMANCE evaluation; ELECTRIC potential; REAL-time programming; Electronic Computer Manufacturing; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and peripheral equipment manufacturing; Facilities Support Services; Nonresidential Property Managers; Other Services to Buildings and Dwellings,Algorithm design and analysis; Benchmark testing; device power management; energy management; Performance evaluation; Power demand; Quality of service; Real-time systems; Security; Throughput; voltage/frequency scaling,"Voltage/Frequency Scaling (VFS) and Device Power Management (DPM) are two popular techniques commonly employed to save energy in real-time embedded systems. VFS policies aim at reducing the CPU energy, while DPM-based solutions involve putting the system components (e.g., memory or I/O devices) to low-power/sleep states at runtime, when sufficiently long idle intervals can be predicted. Despite numerous research papers that tackled the energy minimization problem using VFS or DPM separately, the interactions of these two popular techniques are not yet well understood. In this paper, we undertake an exact analysis of the problem for a real-time embedded application running on a VFS-enabled CPU and using multiple devices. Specifically, by adopting a generalized system-level energy model, we characterize the variations in different components of the system energy as a function of the CPU processing frequency. Then, we propose a provably optimal and efficient algorithm to determine the optimal CPU frequency as well as device state transition decisions to minimize the system-level energy. We also extend our solution to deal with workload variability. The experimental evaluations confirm that substantial energy savings can be obtained through our solution that combines VFS and DPM optimally under the given task and energy models. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=67669908&site=ehost-live
95,Power-Aware Scheduling for Periodic Real-Time Tasks.,Hakan Aydin,IEEE Transactions on Computers,00189340,,May2004,53,5,584,17.0,12966080,10.1109/TC.2004.1275298,IEEE,Article,"POWER resources; ENERGY consumption; ENERGY auditing; ENERGY policy; ALGORITHMS; WORKLOAD of computer networks; WORKLOAD of computers; Regulation and Administration of Communications, Electric, Gas, and Other Utilities",,"In this paper, we address power-aware scheduling of periodic tasks to reduce CPU energy consumption in hard real-time systems through dynamic voltage scaling. Our intertask voltage scheduling solution includes three components: 1) a static (offline) solution to compute the optimal speed, assuming worst-case workload for each arrival, 2) an online speed reduction mechanism to reclaim energy by adapting to the actual workload, and 3) an online, adaptive and speculative speed adjustment mechanism to anticipate early completions of future executions by using the average-case workload information. All these solutions still guarantee that all deadlines are met. Our simulation results show that our reclaiming algorithm alone outperforms other recently proposed intertask voltage scheduling schemes. Our speculative techniques are shown to provide additional gains, approaching the theoretical lower-bound by a margin of 10 percent. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=12966080&site=ehost-live
96,Preference-oriented fixed-priority scheduling for periodic real-time tasks.,Hakan Aydin,Journal of Systems Architecture,13837621,,Sep2016,69,,1,14.0,117895229,10.1016/j.sysarc.2016.07.005,Elsevier B.V.,Article,"REAL-time control; ALGORITHMS; MONOTONIC functions; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); ASAP (Computer software)",Fixed-priority scheduling; Periodic tasks; Preference-oriented executions; Real-time systems,"Traditionally, real-time scheduling algorithms prioritize tasks solely based on their timing parameters and cannot effectively handle tasks that have different execution preferences . In this paper, for a set of periodic real-time tasks running on a single processor, where some tasks are preferably executed as soon as possible (ASAP) and others as late as possible (ALAP) , we investigate Preference-Oriented Fixed-Priority (POFP) scheduling techniques. First, based on Audsley’s Optimal Priority Assignment (OPA) , we study a Preference Priority Assignment (PPA) scheme that attempts to assign ALAP (ASAP) tasks lower (higher) priorities, whenever possible. Then, by considering the non-work-conserving strategy, we exploit the promotion times of ALAP tasks and devise an online dual-queue based POFP scheduling algorithm. Basically, with the objective of fulfilling the execution preferences of all tasks, the POFP scheduler retains ALAP tasks in the delay queue until their promotion times while putting ASAP tasks into the ready queue right after their arrivals. In addition, to further expedite (delay) the executions of ASAP (ALAP) tasks using system slack, runtime techniques based on dummy and wrapper tasks are investigated. The proposed schemes are evaluated through extensive simulations. The results show that, compared to the classical fixed-priority Rate Monotonic Scheduling (RMS) algorithm, the proposed priority assignment scheme and POFP scheduler can achieve significant improvement in terms of fulfilling the execution preferences of both ASAP and ALAP tasks, which can be further enhanced at runtime with the wrapper-task based slack management technique. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117895229&site=ehost-live
97,Preference-oriented partitioning for multiprocessor real-time systems.,Hakan Aydin,Journal of Systems Architecture,13837621,,May2022,126,,N.PAG,1.0,156519722,10.1016/j.sysarc.2022.102468,Elsevier B.V.,Article,PARALLEL algorithms; MULTIPROCESSORS,Fixed-priority; Multiprocessor; Partitioned scheduling; Preference-oriented tasks; Real-time systems,"Recently, real-time application models where tasks may have as early as possible (ASAP) or as late as possible (ALAP) execution preferences, while meeting their deadlines, have been proposed and studied. In this work we consider the preference-oriented (PO) real-time scheduling problem for multiprocessor systems. Specifically, we focus on partitioned scheduling of fixed-priority preference-oriented real-time tasks on multiprocessor platforms. Firstly, we explore the use of the Reverse-Preference Priority Assignment (RPPA) scheme, where ALAP tasks are assigned higher priority compared to ASAP tasks on a given processor. Counter-intuitively, this helps the tasks to better fulfill their execution preferences when deployed in the context of the Preference-Oriented Fixed-Priority (POFP) scheduler. Then, considering the complementary execution preferences of the ASAP and ALAP tasks, we propose a preference-oriented partitioning algorithm to allocate tasks to processors. Finally, we extend the algorithm to exploit the period information about the ALAP and ASAP tasks when making the task allocation decisions. The proposed RPPA and preference-oriented partitioning algorithms are evaluated through extensive simulations. The results show that, when the POFP scheduler is adopted, RPPA can significantly improve the execution preferences of ALAP tasks with only marginal impact on ASAP tasks compared to the state-of-the-art priority assignment schemes. Furthermore, compared to the classical utilization based worst-fit-decreasing (WFD) partitioning scheme, the proposed PO partitioning schemes provide more opportunities for both ASAP and ALAP tasks to better fulfill their execution preferences, especially when combined with RPPA and POFP scheduling on each processor. • A novel Reverse-Preference Priority Assignment scheme for POFP scheduler is proposed • New Preference-Oriented partitioning algorithms are devised and analyzed • The schemes are evaluated via simulations and the results show their effectiveness [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156519722&site=ehost-live
98,Preference-oriented real-time scheduling and its application in fault-tolerant systems.,Hakan Aydin,Journal of Systems Architecture,13837621,,Feb2015,61,2,127,13.0,101342200,10.1016/j.sysarc.2014.12.001,Elsevier B.V.,Article,PRODUCTION scheduling; FAULT tolerance (Engineering); REAL-time computing; ALGORITHMS; COMPUTER simulation,Fault-tolerant systems; Periodic real-time tasks; Preference-oriented execution; Scheduling algorithms,"In this paper, we consider a set of real-time periodic tasks where some tasks are preferably executed as soon as possible (ASAP) and others as late as possible (ALAP) while still meeting their deadlines. After introducing the idea of preference-oriented (PO) execution , we formally define the concept of PO-optimality . For fully-loaded systems (with 100% utilization), we first propose a PO-optimal scheduler, namely ASAP-Ensured Earliest Deadline (SEED) , by focusing on ASAP tasks where the optimality of ALAP tasks’ preference is achieved implicitly due to the harmonicity of the PO-optimal schedules for such systems. Then, for under-utilized systems (with less than 100% utilization), we show the discrepancies between different PO-optimal schedules. By extending SEED, we propose a generalized Preference-Oriented Earliest Deadline (POED) scheduler that can obtain a PO-optimal schedule for any schedulable task set. The application of the POED scheduler in a dual-processor fault-tolerant system is further illustrated. We evaluate the proposed PO-optimal schedulers through extensive simulations. The results show that, comparing to that of the well-known EDF scheduler, the scheduling overheads of SEED and POED are higher (but still manageable) due to the additional consideration of tasks’ preferences. However, SEED and POED can achieve the preference-oriented execution objectives in a more successful way than EDF. [ABSTRACT FROM AUTHOR] Copyright of Journal of Systems Architecture is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101342200&site=ehost-live
99,A study on the least squares estimator of multivariate isotonic regression function.,Pramita Bagchi,Scandinavian Journal of Statistics,03036898,,Dec2020,47,4,1192,30.0,147049916,10.1111/sjos.12459,Wiley-Blackwell,Article,ISOTONIC regression; LEAST squares; ASYMPTOTIC distribution; NONPARAMETRIC estimation; REGRESSION analysis; CONVEX functions,consistency; convex function; cumulative sum diagram; nonstandard asymptotic distribution; rate of convergence,"Consider the problem of pointwise estimation of f in a multivariate isotonic regression model Z=f(X1,...,Xd)+ϵ, where Z is the response variable, f is an unknown nonparametric regression function, which is isotonic with respect to each component, and ϵ is the error term. In this article, we investigate the behavior of the least squares estimator of f. We generalize the greatest convex minorant characterization of isotonic regression estimator for the multivariate case and use it to establish the asymptotic distribution of properly normalized version of the estimator. Moreover, we test whether the multivariate isotonic regression function at a fixed point is larger (or smaller) than a specified value or not based on this estimator, and the consistency of the test is established. The practicability of the estimator and the test are shown on simulated and real data as well. [ABSTRACT FROM AUTHOR] Copyright of Scandinavian Journal of Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147049916&site=ehost-live
100,Circulating microRNAs in cellular and antibody-mediated heart transplant rejection.,Pramita Bagchi,Journal of Heart & Lung Transplantation,10532498,,Oct2022,41,10,1401,13.0,159361101,10.1016/j.healun.2022.06.019,Elsevier B.V.,Article,HEART transplantation; GRAFT rejection; HEART transplant recipients; RECEIVER operating characteristic curves; MICRORNA; CD30 antigen; STANFORD University,allograft rejection; biomarker; heart transplantation; microRNA,"Noninvasive monitoring of heart allograft health is important to improve clinical outcomes. MicroRNAs (miRs) are promising biomarkers of cardiovascular disease and limited studies suggest they can be used to noninvasively diagnose acute heart transplant rejection. The Genomic Research Alliance for Transplantation (GRAfT) is a multicenter prospective cohort study that phenotyped heart transplant patients from 5 mid-Atlantic centers. Patients who had no history of rejection after transplant were compared to patients with acute cellular rejection (ACR) or antibody-mediated rejection (AMR). Small RNA sequencing was performed on plasma samples collected at the time of an endomyocardial biopsy. Differential miR expression was performed with adjustment for clinical covariates. Regression was used to develop miR panels with high diagnostic accuracy for ACR and AMR. These panels were then validated in independent samples from GRAfT and Stanford University. Receiver operating characteristic curves were generated and area under the curve (AUC) statistics calculated. Distinct ACR and AMR clinical scores were developed to translate miR expression data for clinical use. The GRAfT cohort had a median age of 52 years, with 35% females and 45% Black patients. Between GRAfT and Stanford, we included 157 heart transplant patients: 108 controls and 49 with rejection (50 ACR and 38 AMR episodes). After differential miR expression and regression analysis, we identified 12 miRs that accurately discriminate ACR and 17 miRs in AMR. Independent validation of the miR panels within GRAfT led to an ACR AUC 0.92 (95% confidence interval [CI]: 0.86-0.98) and AMR AUC 0.82 (95% CI: 0.74-0.90). The externally validated ACR AUC was 0.72 (95% CI: 0.59-0.82). We developed distinct ACR and AMR miR clinical scores (range 0-100), a score ≥ 65, identified ACR with 86% sensitivity, 76% specificity, and 98% negative predictive value, for AMR score performance was 82%, 84% and 97%, respectively. We identified novel miRs that had excellent performance to noninvasively diagnose acute rejection after heart transplantation. Once rigorously validated, the unique clinical ACR and AMR scores usher in an era whereby genomic biomarkers can be used to screen and diagnose the subtype of rejection. These novel biomarkers may potentially alleviate the need for an endomyocardial biopsy while facilitating the initiation of targeted therapy based on the noninvasive diagnosis of ACR or AMR. [ABSTRACT FROM AUTHOR] Copyright of Journal of Heart & Lung Transplantation is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159361101&site=ehost-live
101,Guest Editorial Special Issue on Blockchain and Economic Knowledge Automation.,Angelos Stavrou,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Jan2020,50,1,2,7.0,141082994,10.1109/TSMC.2019.2959929,IEEE,Article,CRYPTOCURRENCIES; BLOCKCHAINS; TRADITIONAL knowledge; AUTOMATION; HOSPITALITY; BITCOIN,,"Blockchain, as an emerging decentralized architecture and distributed computing paradigm underlying Bitcoin and other cryptocurrencies, has attracted intensive attention in both research and applications recently. Blockchain, especially powered by chain-coded smart contracts, has the full potential of revolutionizing increasingly centralized cyber-physical-social systems (CPSSs) for constructions and applications, and reshaping traditional knowledge automation workflows. The key advantage of blockchain technology lies in the fact that it can enable the establishment of secured, trusted, and decentralized autonomous ecosystems for various scenarios, especially for better usage of the legacy devices, infrastructure, and resources. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141082994&site=ehost-live
101,Guest Editorial Special Issue on Blockchain and Economic Knowledge Automation.,Foteini Baldimtsi,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Jan2020,50,1,2,7.0,141082994,10.1109/TSMC.2019.2959929,IEEE,Article,CRYPTOCURRENCIES; BLOCKCHAINS; TRADITIONAL knowledge; AUTOMATION; HOSPITALITY; BITCOIN,,"Blockchain, as an emerging decentralized architecture and distributed computing paradigm underlying Bitcoin and other cryptocurrencies, has attracted intensive attention in both research and applications recently. Blockchain, especially powered by chain-coded smart contracts, has the full potential of revolutionizing increasingly centralized cyber-physical-social systems (CPSSs) for constructions and applications, and reshaping traditional knowledge automation workflows. The key advantage of blockchain technology lies in the fact that it can enable the establishment of secured, trusted, and decentralized autonomous ecosystems for various scenarios, especially for better usage of the legacy devices, infrastructure, and resources. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141082994&site=ehost-live
102,16S rRNA metagenome clustering and diversity estimation using locality sensitive hashing.,Huzefa Rangwala,BMC Systems Biology,17520509,,2013 Suppl 4,7,,1,13.0,131728572,10.1186/1752-0509-7-S4-S11,BioMed Central,Article,,,"Background: Advances in biotechnology have changed the manner of characterizing large populations of microbial communities that are ubiquitous across several environments.""Metagenome” sequencing involves decoding the DNA of organisms co-existing within ecosystems ranging from ocean, soil and human body. Several researchers are interested in metagenomics because it provides an insight into the complex biodiversity across several environments. Clinicians are using metagenomics to determine the role played by collection of microbial organisms within human body with respect to human health wellness and disease. Results: We have developed an efficient and scalable, species richness estimation algorithm that uses locality sensitive hashing (LSH). Our algorithm achieves efficiency by approximating the pairwise sequence comparison operations using hashing and also incorporates matching of fixed-length, gapless subsequences criterion to improve the quality of sequence comparisons. We use LSH-based similarity function to cluster similar sequences and make individual groups, called operational taxonomic units (OTUs). We also compute different species diversity/richness metrics by utilizing OTU assignment results to further extend our analysis. Conclusion: The algorithm is evaluated on synthetic samples and eight targeted 16S rRNA metagenome samples taken from seawater. We compare the performance of our algorithm with several competing diversity estimation algorithms. We show the benefits of our approach with respect to computational runtime and meaningful OTU assignments. We also demonstrate practical significance of the developed algorithm by comparing bacterial diversity and structure across different skin locations. [ABSTRACT FROM AUTHOR] Copyright of BMC Systems Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131728572&site=ehost-live
102,16S rRNA metagenome clustering and diversity estimation using locality sensitive hashing.,Daniel Barbará,BMC Systems Biology,17520509,,2013 Suppl 4,7,,1,13.0,131728572,10.1186/1752-0509-7-S4-S11,BioMed Central,Article,,,"Background: Advances in biotechnology have changed the manner of characterizing large populations of microbial communities that are ubiquitous across several environments.""Metagenome” sequencing involves decoding the DNA of organisms co-existing within ecosystems ranging from ocean, soil and human body. Several researchers are interested in metagenomics because it provides an insight into the complex biodiversity across several environments. Clinicians are using metagenomics to determine the role played by collection of microbial organisms within human body with respect to human health wellness and disease. Results: We have developed an efficient and scalable, species richness estimation algorithm that uses locality sensitive hashing (LSH). Our algorithm achieves efficiency by approximating the pairwise sequence comparison operations using hashing and also incorporates matching of fixed-length, gapless subsequences criterion to improve the quality of sequence comparisons. We use LSH-based similarity function to cluster similar sequences and make individual groups, called operational taxonomic units (OTUs). We also compute different species diversity/richness metrics by utilizing OTU assignment results to further extend our analysis. Conclusion: The algorithm is evaluated on synthetic samples and eight targeted 16S rRNA metagenome samples taken from seawater. We compare the performance of our algorithm with several competing diversity estimation algorithms. We show the benefits of our approach with respect to computational runtime and meaningful OTU assignments. We also demonstrate practical significance of the developed algorithm by comparing bacterial diversity and structure across different skin locations. [ABSTRACT FROM AUTHOR] Copyright of BMC Systems Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131728572&site=ehost-live
103,Improving the recognition of grips and movements of the hand using myoelectric signals.,Gene Shuman,BMC Medical Informatics & Decision Making,14726947,,7/21/2016,16,,65,19.0,116978119,10.1186/s12911-016-0308-1,BioMed Central,journal article,GRIPS (Persons); HUMAN mechanics; ELECTROMYOGRAPHY; PROSTHETICS; COMPUTER interfaces; ACTIVITIES of daily living; HAND physiology; HAND; INFORMATION science; SIGNAL processing; BODY movement; All Other Health and Personal Care Stores,ADLs; Classification; Dynamic time warping; Electromyograms; Machine learning; Prehensile patterns; SAX,"<bold>Background: </bold>People want to live independently, but too often disabilities or advanced age robs them of the ability to do the necessary activities of daily living (ADLs). Finding relationships between electromyograms measured in the arm and movements of the hand and wrist needed to perform ADLs can help address performance deficits and be exploited in designing myoelectrical control systems for prosthetics and computer interfaces.<bold>Methods: </bold>This paper reports on several machine learning techniques employed to discover the electromyogram patterns present when performing 24 typical fine motor functional activities of the hand and the rest position used to accomplish ADLs. Accelerometer data is collected from the hand as an aid in identifying the start and end of movements and to help in labeling the signal data. Techniques employed include classification of 100 ms individual signal instances, using a symbolic representation to approximate signal streams, and the use of nearest neighbor in two specific situations: creation of an affinity matrix to model learning instances and classify based on multiple adjacent signal values, and using Dynamic Time Warping (DTW) as a distance measure to classify entire activity segments.<bold>Results: </bold>Results show the patterns can be learned to an accuracy of 76.64 % for a 25 class problem when classifying 100 ms instances, 83.63 % with the affinity matrix approach with symbolic representation, and 85.22 % with Dynamic Time Warping. Classification errors are, with a few exceptions, concentrated within particular grip action groups.<bold>Conclusion: </bold>The findings reported here support the view that grips and movements of the hand can be distinguished by combining electrical and mechanical properties of the task to an accuracy of 85.22 % for a 25 class problem. Converting the signals to a symbolic representation and classifying based on larger portions of the signal stream improve classification accuracy. This is both clinically useful and opens the way for an approach to help simulate hand functional activities. With improvements it may also prove useful in real time control applications. [ABSTRACT FROM AUTHOR] Copyright of BMC Medical Informatics & Decision Making is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116978119&site=ehost-live
103,Improving the recognition of grips and movements of the hand using myoelectric signals.,Daniel Barbará,BMC Medical Informatics & Decision Making,14726947,,7/21/2016,16,,65,19.0,116978119,10.1186/s12911-016-0308-1,BioMed Central,journal article,GRIPS (Persons); HUMAN mechanics; ELECTROMYOGRAPHY; PROSTHETICS; COMPUTER interfaces; ACTIVITIES of daily living; HAND physiology; HAND; INFORMATION science; SIGNAL processing; BODY movement; All Other Health and Personal Care Stores,ADLs; Classification; Dynamic time warping; Electromyograms; Machine learning; Prehensile patterns; SAX,"<bold>Background: </bold>People want to live independently, but too often disabilities or advanced age robs them of the ability to do the necessary activities of daily living (ADLs). Finding relationships between electromyograms measured in the arm and movements of the hand and wrist needed to perform ADLs can help address performance deficits and be exploited in designing myoelectrical control systems for prosthetics and computer interfaces.<bold>Methods: </bold>This paper reports on several machine learning techniques employed to discover the electromyogram patterns present when performing 24 typical fine motor functional activities of the hand and the rest position used to accomplish ADLs. Accelerometer data is collected from the hand as an aid in identifying the start and end of movements and to help in labeling the signal data. Techniques employed include classification of 100 ms individual signal instances, using a symbolic representation to approximate signal streams, and the use of nearest neighbor in two specific situations: creation of an affinity matrix to model learning instances and classify based on multiple adjacent signal values, and using Dynamic Time Warping (DTW) as a distance measure to classify entire activity segments.<bold>Results: </bold>Results show the patterns can be learned to an accuracy of 76.64 % for a 25 class problem when classifying 100 ms instances, 83.63 % with the affinity matrix approach with symbolic representation, and 85.22 % with Dynamic Time Warping. Classification errors are, with a few exceptions, concentrated within particular grip action groups.<bold>Conclusion: </bold>The findings reported here support the view that grips and movements of the hand can be distinguished by combining electrical and mechanical properties of the task to an accuracy of 85.22 % for a 25 class problem. Converting the signals to a symbolic representation and classifying based on larger portions of the signal stream improve classification accuracy. This is both clinically useful and opens the way for an approach to help simulate hand functional activities. With improvements it may also prove useful in real time control applications. [ABSTRACT FROM AUTHOR] Copyright of BMC Medical Informatics & Decision Making is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116978119&site=ehost-live
103,Improving the recognition of grips and movements of the hand using myoelectric signals.,Jessica Lin,BMC Medical Informatics & Decision Making,14726947,,7/21/2016,16,,65,19.0,116978119,10.1186/s12911-016-0308-1,BioMed Central,journal article,GRIPS (Persons); HUMAN mechanics; ELECTROMYOGRAPHY; PROSTHETICS; COMPUTER interfaces; ACTIVITIES of daily living; HAND physiology; HAND; INFORMATION science; SIGNAL processing; BODY movement; All Other Health and Personal Care Stores,ADLs; Classification; Dynamic time warping; Electromyograms; Machine learning; Prehensile patterns; SAX,"<bold>Background: </bold>People want to live independently, but too often disabilities or advanced age robs them of the ability to do the necessary activities of daily living (ADLs). Finding relationships between electromyograms measured in the arm and movements of the hand and wrist needed to perform ADLs can help address performance deficits and be exploited in designing myoelectrical control systems for prosthetics and computer interfaces.<bold>Methods: </bold>This paper reports on several machine learning techniques employed to discover the electromyogram patterns present when performing 24 typical fine motor functional activities of the hand and the rest position used to accomplish ADLs. Accelerometer data is collected from the hand as an aid in identifying the start and end of movements and to help in labeling the signal data. Techniques employed include classification of 100 ms individual signal instances, using a symbolic representation to approximate signal streams, and the use of nearest neighbor in two specific situations: creation of an affinity matrix to model learning instances and classify based on multiple adjacent signal values, and using Dynamic Time Warping (DTW) as a distance measure to classify entire activity segments.<bold>Results: </bold>Results show the patterns can be learned to an accuracy of 76.64 % for a 25 class problem when classifying 100 ms instances, 83.63 % with the affinity matrix approach with symbolic representation, and 85.22 % with Dynamic Time Warping. Classification errors are, with a few exceptions, concentrated within particular grip action groups.<bold>Conclusion: </bold>The findings reported here support the view that grips and movements of the hand can be distinguished by combining electrical and mechanical properties of the task to an accuracy of 85.22 % for a 25 class problem. Converting the signals to a symbolic representation and classifying based on larger portions of the signal stream improve classification accuracy. This is both clinically useful and opens the way for an approach to help simulate hand functional activities. With improvements it may also prove useful in real time control applications. [ABSTRACT FROM AUTHOR] Copyright of BMC Medical Informatics & Decision Making is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116978119&site=ehost-live
104,An investigation into the mechanistic origin of thermal stability in thermal-microstructural-engineered additively manufactured Inconel 718.,Ali Beheshti,Vacuum,0042207X,,May2022,199,,N.PAG,1.0,155725035,10.1016/j.vacuum.2022.110971,Elsevier B.V.,Article,THERMAL stability; LASER peening; INCONEL; RESIDUAL stresses; HEAT treatment; LASER annealing; Metal Heat Treating,Laser peening; Microstructure engineering; Nickel-based superalloys; Thermal stability,"An obstacle hindering the applicability of surface modification techniques such as laser peening (LP) in high temperature systems stems from thermally-driven degradation of desirable, strain-induced material modifications. Illustrated in this work is a novel LP scheme termed laser peening plus thermal microstructure engineering (LP + TME) comprised of cyclic LP and the addition of intermittent 600 °C (0.55T m) heat treatments designed to impart thermally-stable microstructural modifications in additively manufactured (AM) Inconel 718 (In718). Instrumented microindentation uncovered significant surface and sub-surface hardness enhancements exceeding 600 HV following LP + TME, a 20% increase over the as-built material. High magnitude compressive residual stresses exceeding −310 MPa were also measured following a 350-h 600 °C thermal exposure; a 25% increase compared to the material subjected to only a single laser shot. Thermal stabilization and overall material enhancement were determined to be the result of the formation of thermally-stable subgrains, subgrain and grain growth regulation through pinning effects, and dislocation-precipitate interactions. • Enhanced thermal stability of microstructural enhancements induced by cyclic laser peening and annealing (LP + TME). • Increased microhardness of 20% over the non-treated material. • LP + TME treated material retained 25% greater residual stresses than the traditionally laser peened material. • Greater stability of the LP + TME is attributed to subgrain formation and pinning effects of precipitates. [ABSTRACT FROM AUTHOR] Copyright of Vacuum is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155725035&site=ehost-live
105,Elevated temperature contact creep and friction of nickel-based superalloys using machine learning assisted finite element analysis.,Ali Beheshti,Mechanics of Materials,01676636,,Aug2022,171,,N.PAG,1.0,157418515,10.1016/j.mechmat.2022.104346,Elsevier B.V.,Article,FINITE element method; HIGH temperatures; HEAT resistant alloys; MACHINE learning; FRICTION; Iron and Steel Mills and Ferroalloy Manufacturing,Artificial neural network; Elastic-plastic-viscoplastic contact; High temperatures; Machine learning; Static friction; Superalloys,"Nickel-based superalloys with superior thermochemical, mechanical, and tribological properties are highly utilized for critical components in several high temperature applications such as gas turbines and nuclear reactors. Inconel 617, in particular, is considered as one of the main candidate superalloys for tribo-components in very-high-temperature gas-cooled nuclear reactors. Recent findings indicate that this alloy grows unique surface oxide especially in a high-temperature helium environment with distinctive wear, friction, and contact properties. This study investigates the high temperature contact area evolution and frictional behavior of Inconel 617 using finite element simulation and provides predictive models for the contact and friction performance at different normal loads, dwell times, and temperatures. High temperature helium-aged Inconel 617 top surface properties (up to 600 °C) are utilized along with a single asperity-based deformable elastic-plastic contact model under combined normal and tangential loading. Machine learning is used to assist the finite element results and to predict friction coefficient as well as contact area evolution. While a small difference is observed in the instantaneous friction coefficient (no dwell time) for all temperatures, friction coefficient increases considerably with dwell time. This shows that the effect of contact creep for longer dwell times significantly dominants the effect of high temperature variation in basic mechanical parameters such as modulus and yield strength. It is found that increasing temperature and dwell times lead to the friction coefficient increase, yet the dominance of dwell time effects decreases at higher temperatures and loads. While the analysis is presented for Inconel 617, the methodology is easy to be generalized and can be applied to other HT alloys. • High temperature contact area and frictional behavior of Inconel 617 is studied. • A single asperity-based elastic-plastic finite element simulation is developed. • Machine learning is used to assist simulations and predict friction/contact area. • Friction coefficient and contact area increase considerably with dwell time. • Method is easy to be generalized and can be used for other high temperature alloys. [ABSTRACT FROM AUTHOR] Copyright of Mechanics of Materials is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157418515&site=ehost-live
106,Elevated temperature mechanical properties of Inconel 617 surface oxide using nanoindentation.,Ali Beheshti,Materials Science & Engineering: A,09215093,,Jun2020,788,,N.PAG,1.0,143780433,10.1016/j.msea.2020.139539,Elsevier B.V.,Article,"NANOINDENTATION; HIGH temperatures; GAS cooled reactors; INCONEL; BULK solids; FINITE element method; Dry bulk materials trucking, local; Dry bulk materials trucking, long distance",Creep; Elevated temperature; Finite element analysis; Inconel 617; Nanoindentation,"Inconel 617 is a principal candidate material for helium gas cooled very-high-temperature reactors with outlet temperatures of 700–950 °C. Recent findings showed that this alloy develops unique surface oxide layers especially at high temperature (HT) helium environment with distinctive wear, friction and contact properties. This study investigates the elevated temperature mechanical properties of Inconel 617 top surface layers aged in HT helium environment. Nanoindentation technique is used to obtain load-displacement graphs of the alloy top surface oxide in temperatures ranging from 25 °C up to 600 °C. In addition, using finite element analysis along with an iterative regression technique, a semi-numerical method is developed to further measure and quantify the material parameters and, in particular, time-independent and creep characteristics of the oxide. While Young's modulus of the oxide is found to be relatively close to the bulk for the tested temperatures, the yield strength and hardness, in comparison to the bulk material, increase significantly as the material is oxidized after aging. The oxide exhibits significant softening as the temperature increases to 600 °C. Unlike the bulk material, diffusion through the grains is found to be the dominant creep mechanism for the oxide. Considerable difference between the mechanical properties of the oxide and the bulk material shows the need for accurate measurements of near surface mechanical properties, if reliable predictive contact and tribological models are sought at elevated temperatures. [ABSTRACT FROM AUTHOR] Copyright of Materials Science & Engineering: A is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143780433&site=ehost-live
107,Elevated temperature nanoscratch of Inconel 617 Superalloy.,Ali Beheshti,Mechanics Research Communications,00936413,,Apr2022,121,,N.PAG,1.0,155846532,10.1016/j.mechrescom.2022.103875,Elsevier B.V.,Article,HIGH temperatures; INCONEL; HEAT resistant alloys; FINITE element method; TEMPERATURE effect; Iron and Steel Mills and Ferroalloy Manufacturing,Adhesion; High temperature; Inconel 617; Nicke-based Superalloys; Plowing; Scratch friction,"• High temperature nanoscratch and finite element analysis to extract friction. • Friction coefficient shows no dependence on sliding velocity for the range investigated. • Elastic recovery after nanoscratch decreases with increasing load and temperature. • Adhesion component of friction significantly increases with temperature rise. • Plowing contribution to the overall friction is noticeable above 400 °C. Inconel 617 superalloy is a main candidate to be used for mechanical and tribo-components in high temperature helium-cooled reactors. Recent findings show that it grows a unique surface oxide, especially under high temperature helium with distinct wear, friction, and contact properties. This study reports the elevated temperature nanoscratch behavior of Inconel 617 and further utilizes it to understand the effect of temperature on contact friction constituent contributors, adhesion and plowing at small scales. Inconel 617 is aged in high temperature helium, and consequently, the total kinetic friction coefficient of the alloy surface oxide is obtained in temperatures ranging from 25 °C to 400 °C. A finite element model is developed and validated based on the experimental results. The model is then utilized along with previously established techniques to determine the adhesion and plowing components of the friction coefficient. At small scale, the experimental results show that with increasing temperature the friction coefficient increases. It was inferred that this increase is mainly due to the increased contribution of plowing friction at high levels of deformation. [Display omitted] [ABSTRACT FROM AUTHOR] Copyright of Mechanics Research Communications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155846532&site=ehost-live
108,Elevated temperature tribology of Ni alloys under helium environment for nuclear reactor applications.,Ali Beheshti,Tribology International,0301679X,,Jul2018,123,,372,13.0,128945093,10.1016/j.triboint.2018.03.021,Elsevier B.V.,Article,"TRIBOLOGY; NICKEL alloys; NUCLEAR reactors; HELIUM; HIGH temperature physics; Industrial Gas Manufacturing; Non-ferrous metal (except copper and aluminum) rolling, drawing, extruding and alloying; Power Boiler and Heat Exchanger Manufacturing",Alloy 800HT; Elevated temperature helium tribology; Inconel 617; Nuclear reactor,"The current study investigates the friction and wear behavior of two primary candidate materials, Inconel 617 and alloy 800HT for high-temperature gas cooled nuclear reactors/very-high-temperature reactors. Using a custom-built high temperature tribometer, helium cooled reactor environment was simulated at room and 800 °C temperatures. Microscopy and chemical analyses were carried out to explain the tribological performance of the alloys. At elevated temperatures, both alloys show higher friction in helium, compared to air environment. Both alloys exhibit high wear resistance in all experimental conditions, except at high temperature helium environment. The formation of glazed and mechanically mixed layers of oxides were found to be important causes for the lower friction and wear in high temperature air atmosphere. [ABSTRACT FROM AUTHOR] Copyright of Tribology International is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128945093&site=ehost-live
109,Helium Tribology of Inconel 617 at Elevated Temperatures up to 950°C: Parametric Study.,Ali Beheshti,Nuclear Science & Engineering,00295639,,Sep2019,193,9,998,15.0,138051627,10.1080/00295639.2019.1582315,Taylor & Francis Ltd,Article,HELIUM; NUCLEAR reactors; HIGH temperatures; Industrial Gas Manufacturing; Power Boiler and Heat Exchanger Manufacturing,friction; high temperature; Inconel 617; Nuclear reactor; wear,"This study investigates the friction and wear behavior of Inconel 617, one of the primary candidate materials for high-temperature gas-cooled nuclear reactors. Using a custom-built, high-temperature tribometer, a helium (He)-cooled reactor environment was simulated up to 950°C. To obtain a comprehensive understanding of the Inconel 617 tribological response, the effects of contact load, temperature, air and He environments, sliding speed, and sliding distance were studied. From the conditions investigated, the coefficient of friction and wear values are the highest in a high-temperature He atmosphere. Scanning electron microscopy, energy dispersive spectroscopy, and X-ray diffraction techniques were used to analyze the Inconel 617 oxide layer. Analysis of the samples tested in the He atmosphere showed the presence of Cr-rich oxide with a lower presence of Co-Ni-Mo compared to the samples tested in air. Characterization also revealed the existence of a very hard protective glaze layer in air while such layer was not observed in the He environment, which was associated with higher wear/friction values. [ABSTRACT FROM AUTHOR] Copyright of Nuclear Science & Engineering is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138051627&site=ehost-live
110,Helium tribology of Inconel 617 subjected to laser peening for high temperature nuclear reactor applications.,Ali Beheshti,Applied Surface Science,01694332,,Mar2022,577,,N.PAG,1.0,154297827,10.1016/j.apsusc.2021.151961,Elsevier B.V.,Abstract,LASER peening; TRIBOLOGY; NUCLEAR reactors; INCONEL; GAS cooled reactors; HEAT resistant materials; Power Boiler and Heat Exchanger Manufacturing,Helium thermal aging; High temperature tribology; Inconel 617; Laser peening; Thermal microstructure engineering,"[Display omitted] • Laser peening enhances the microhardness of Inconel 617 by 62%. • Laser peening sustains structural integrity on Inconel 617 after thermal exposure. • Significant wear and friction reduction in helium due to peening and aging processes. Inconel 617 is among the best candidates for utilization in high temperature gas cooled reactor tribo-components. However, the combined effects of sliding contact, along with intermittent idle times and very high temperature material degradation, deteriorates the alloy tribological performance, especially under a helium atmosphere. Laser peening is a surface treatment technique which can enhance the properties at the surface and subsurface by generating deep residual stresses and enhanced microstructure. Herein, we report the tribological behavior of regular laser peened as well as thermally-engineered laser peened Inconel 617 under helium and air atmospheres at 800 °C. In addition to friction and wear studies, the specimens are characterized by different analytical techniques to further understand the mechanisms involved in the peening process and sliding contact. Regardless of the peening process and post-process treatment types, it is observed that laser peening improves the tribological characteristics of Inconel 617. Interestingly, laser peening followed by helium thermal aging shows highly enhanced tribological behavior. This is attributed to the strengthening effect of the laser peening on the surface oxides providing an excellent and lasting protective and lubricating film under helium exposure. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154297827&site=ehost-live
111,Microfabricated Biomimetic placoid Scale-Inspired surfaces for antifouling applications.,Ali Beheshti,Applied Surface Science,01694332,,Sep2018,453,,166,7.0,130073164,10.1016/j.apsusc.2018.05.030,Elsevier B.V.,Article,ANTIFOULING paint; ELASTOMERS; POLYDIMETHYLSILOXANE; ESCHERICHIA coli; DURABILITY; MICROFABRICATION; CHEMICAL molding; Resin and synthetic rubber manufacturing; Plastics Material and Resin Manufacturing; Synthetic Rubber Manufacturing,Antifouling; Biomimetic; Microfabrication; Microtopographies; Surface durability,"Marine biological fouling or ‘biofouling’, the unwanted aggregation of aquatic organisms such as algae, barnacles, and marine microorganisms has been a detriment to maritime industries. Previous antifouling strategies included the use of toxic, biocide-containing paints; however, as humanity strives to lessen its environmental impact, a shift towards ecological deterrents which draw inspiration from nature are starting to be developed. In this work, the manipulation of the surface topography of nontoxic polydimethylsiloxane elastomer (PDMSe) was performed in order to mimic natural-occurring antifouling surfaces like those found on shark skin by means of a micro-molding technique using etched silicon molds. Previous polymer-based antifouling patterns which draw influence from shark skin have been successful in mitigating microorganism settlement; however, they lack a degree of biological accuracy as the features are of constant height. Our novel designs utilize various microfabrication techniques and micro-molding to generate placoid scale patterns with an engineered height gradient to deter organism settlement. Surface durability studies showed that the patterns can effectively keep their integrity under external sliding motion. Significant decreases in Escherichia coli ( E. coli ) settlement up to 75% were observed when measuring the effectiveness of pristine patterns, and up to 56% when patterns underwent extreme mechanical wear. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=130073164&site=ehost-live
112,Performance of additively manufactured polylactic acid (PLA) in prolonged marine environments.,Ali Beheshti,Polymer Degradation & Stability,01413910,,May2022,199,,N.PAG,1.0,156520445,10.1016/j.polymdegradstab.2022.109903,Elsevier B.V.,Article,HOPKINSON bars (Testing); ARTIFICIAL seawater; POLYLACTIC acid; NANOINDENTATION; STRAIN rate; OCEAN temperature; TENSILE tests,Additive manufacturing; Impact; Marine environment; Mechanical properties; Nanoindentation; PLA,"• An accelerated aging model was developed to predict life of additively manufactured PLA over years with only weeks or days of exposure for the first time. • A wide range of mechanical tests such as quasi-static tensile, nanoindentation, and direct impact Hopkinson pressure bar tests were conducted to deduce different mechanical properties after aging. • High strain rate behavior of the aged material was studied for the first time. • Using the results gathered from tensile testing, data was extrapolated to predict that PLA will lose 0.5% ductility at around 5166 h (30.75 weeks) and will suffer a 10 MPa drop in yield strength at around 2084 h (12.4 weeks) while submerged in 17 °C seawater. Additive manufacturing has seen rapid growth in a variety of sectors due to its advantages in cost and lead-time reductions. However, there is still skepticism regarding the ability for additively manufactured materials to withstand extreme conditions. In this report, AM polylactic acid (PLA) was submerged in seawater at several temperatures to deduce long-term effects of marine environments for these materials. An accelerated aging model was developed to predict life of PLA over years with only weeks or days of exposure. Samples were submerged in artificial seawater between one and ten weeks with temperatures ranging from 22 °C to 60 °C. Several mechanical tests were then conducted on the submerged samples, such as quasi-static tensile, nanoindentation, and direct impact Hopkinson pressure bar tests to deduce different mechanical properties after aging. For the seawater samples, diffusion data was collected to measure water absorbed by the specimens. An Arrhenius relationship was then studied, and a model for accelerated aging was developed with resulting acceleration factors. Results indicated clear aging in PLA over the timeframe studied, and it was deduced that the acceleration factors of aging between 22 °C and 40 °C, 22 °C and 50 °C, and 22 °C and 60 °C, were 1.91, 3.56, and 5.57, respectively. Results demonstrated a clear aging in PLA; it can be predicted that PLA will lose 0.5% ductility at around 5166 h (30.75 weeks) and will suffer a 10 MPa drop in yield strength at around 2084 h (12.4 weeks) while submerged in 17 °C seawater. [ABSTRACT FROM AUTHOR] Copyright of Polymer Degradation & Stability is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156520445&site=ehost-live
113,Substrate-regulated nanoscale friction of graphene.,Ali Beheshti,Materials Letters,0167577X,,Jun2018,221,,54,3.0,128852560,10.1016/j.matlet.2018.03.078,Elsevier B.V.,Article,GRAPHENE; NANOCHEMISTRY; ATOMIC force microscopy; NANOELECTROMECHANICAL systems; NANOELECTRONICS,Atomic force microscopy; Friction; Graphene; Substrate-regulation,"In the present study, nanotribological measurements were performed via atomic force microscopy on Si/SiO 2 -supported graphene monolayers with varying oxide layer thicknesses. The observations uncovered significant discrepancies in resulting friction forces between each graphene sample. Nanoscale interfacial friction forces were observed to increase from ∼0.49 nN to ∼1.00 nN when the oxide layer thickness was increased from 90 nm to 300 nm. The findings were determined to be the result of increased phonon scattering which is responsible for the removal of the vibrational reduction of nanoscale friction. Such discrepancies in friction forces points toward the potential tunability of nanoscale friction in supported graphene. [ABSTRACT FROM AUTHOR] Copyright of Materials Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128852560&site=ehost-live
114,Tribology of incoloy 800HT for nuclear reactors under helium environment at elevated temperatures.,Ali Beheshti,Wear,00431648,,Oct2019,436,,N.PAG,1.0,138726399,10.1016/j.wear.2019.203022,Elsevier B.V.,Article,NUCLEAR reactors; GAS cooled reactors; HIGH temperatures; HELIUM; SCANNING electron microscopy; Industrial Gas Manufacturing; Power Boiler and Heat Exchanger Manufacturing,Elevated temperature; Friction and wear; Helium tribology; Incoloy 800HT; Nuclear reactor,"Nickel-based alloy 800HT is considered one of the main candidate alloys for nuclear reactors with gas cooled high-temperature environment and, therefore, it is necessary to have a thorough understanding of the alloy tribological response for obtaining optimum operating and loading conditions. The current study investigated the wear and friction behavior of alloy 800HT using a customized high temperature tribometer to simulate the environment of helium cooled reactor up to 750 °C. The effect of temperature, contact load, environment, sliding distance and sliding speed on the alloy friction and wear were studied. To generalize the study for other applications, an investigation is also performed in air environment. The friction and wear coefficients have the highest values at high temperature helium atmosphere where the formation and stability of the oxide scales play an important role. Optical and contact profiling, scanning electron microscopy, as well as energy dispersive spectroscopy techniques were utilized to study the surface oxide. The analysis showed the presence of Fe-Cr-Ni rich oxide both in air and helium. The protective glazed layer did not form in helium in any condition, whereas in air and under specific conditions a stable protective oxide layer was observed. • Tribological experiments of Incoloy 800HT in He environment up to 750 °C. • At 500 °C and 750 °C He, friction and wear stay high for all conditions. • Glazed oxides reduce friction and wear in high temperature air, depending on the contact pressure. [ABSTRACT FROM AUTHOR] Copyright of Wear is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138726399&site=ehost-live
115,Wear in superelastic shape memory alloys: A thermomechanical analysis.,Ali Beheshti,Wear,00431648,,Jan2022,488,,N.PAG,1.0,153848026,10.1016/j.wear.2021.204139,Elsevier B.V.,Article,ALLOY analysis; SHAPE memory alloys; SLIDING wear; MATERIAL plasticity; WEAR resistance; PHASE transitions,Indentation; Shape memory alloy; Thermomechanical behavior; Wear,"Shape Memory Alloys (SMAs) have shown superior mechanical properties including improved wear resistance compared to their conventional counterparts of the same surface hardness. Available wear models cannot estimate a wear coefficient for SMAs, since they do not consider the effects of phase transformation and the related phenomena. This study presents a methodology considering thermomechanical behavior of shape memory alloys in sliding wear. Wear analysis is carried out for a superelastic SMA under different loads and temperatures, and the results are validated by experimental findings. The proposed approach introduces a constant wear coefficient at different loads and temperatures supporting its reliability for quantitative description of wear in an SMA. • A novel thermomechanical model is presented for sliding wear in shape memory alloys. • Phase transformation and plastic deformations are quantified to predict wear mass. • A unified wear coefficient for any arbitrary load and temperature is achieved. • Theoretical results are validated by empirical findings for NiTi shape memory alloy. [ABSTRACT FROM AUTHOR] Copyright of Wear is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153848026&site=ehost-live
145,Temporal sensitivity of protein kinase A activation in a stochastic reaction-diffusion model of late phase long term potentiation.,Kim Blackwell,BMC Neuroscience,14712202,,2009 Supplement 1,10,,1,1.0,44114604,10.1186/1471-2202-10-S1-P202,BioMed Central,Article,CHRONOBIOLOGY; PROTEIN kinases; STOCHASTIC models; NEURAL physiology; HIPPOCAMPUS (Brain); CELLULAR signal transduction; NEUROPLASTICITY,,"Introduction The ability of neurons within the hippocampus to differentially respond to specific temporal and spatial patterns of stimulation underlies the storage of memory and information in neural circuits. Signal transduction pathways are critical for information storage and alterations in key signaling molecules, such as the cAMP-dependent protein kinase (PKA) signaling pathway, modify both hippocampus-dependent learning and a form of synaptic plasticity known as late-phase long-term potentiation (L-LTP). The induction of late phase LTP (L-LTP) in the CA1 region of the hippocampus requires several kinases, including CaMKII and PKA, which are activated by calcium-dependent signaling processes and other intracellular signaling pathways. Many of the biochemical reactions leading to activation of these critical kinases are localized to dendritic spines. The small size of theses spines implies that small numbers of molecules are involved; the presence of anchoring proteins and the morphology of neurons implies that molecules are inhomogeneously distributed. Therefore, to accurately model these cellular signaling events requires software for stochastic reaction-diffusion systems. Methods We developed a spatial, stochastic, computational model of CA1 signaling pathways to investigate the sensitivity of PKA to spatial and temporal patterns of stimulation. The model is implemented using NeuroRD, novel software for efficient computational modeling of stochastic reaction-diffusion systems. The model describes the interactions of calcium and cAMP signaling pathways and is based on published biochemical measurements of two key synaptic signaling molecules, PKA and CaMKII. The model is stimulated using four 100 Hz tetani separated by 3 sec (massed) or 5 min (spaced), identical to experimental LLTP induction protocols. Results The cAMP concentration is larger in response to massed, as compared to spaced stimulation, similar to the results observed for a deterministic model. Though cAMP directly activates PKA, the ability to differentiate the effect of temporal stimulation pattern on PKA activation depends on morphological factors such as the size of the spine head, and whether PKA is anchored in the spine. In very small spines without anchoring, only a few molecules of PKA are activated; thus the effect of stimulation, much less temporal pattern, is not apparent. In contrast, in large spines temporal stimulation pattern influences PKA activation, and spaced stimulation produces a larger cumulative activity than massed stimulation. This leads to enhanced phosphorylation of Inhibitor-1, and inhibition of protein phosphotase 1. Additional simulations further explore the effect of anchoring and protein co-localization on PKA activation in response to LTP stimulation. [ABSTRACT FROM AUTHOR] Copyright of BMC Neuroscience is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=44114604&site=ehost-live
151,Crowdsourcing cyber experts to determine relevant topics during cyber curriculum development efforts.,Mihai Boicu,Innovations in Education & Teaching International,14703297,,Apr2022,,,1,11.0,156289792,10.1080/14703297.2022.2063924,Taylor & Francis Ltd,Article,,curriculum development; cyber security education; engineering education; Expert crowdsourcing; needs assessment; requirements determination,"The cyber security environment, its threats, and its defence strategies are constantly changing. Educational programmes and their curriculum are known to be slow changing and at times out-of-date, resulting in content that may not be as relevant to their students and the industry. This research paper will 1 – present an overview of the curriculum development (CDev) process when using committees and their hindrance, 2 – describe the concept of crowdsourcing and its benefits when using domain experts, 3 – propose a Curriculum Development using Crowdsourcing Framework (CDC-F) to integrate expert crowdsourcing into parts of the CDev process (specifically the identification of industry-relevant topics and sub-topics for further curriculum content development), and 4 – present the process and results of ang experiment utilising the CDC-F. [ABSTRACT FROM AUTHOR] Copyright of Innovations in Education & Teaching International is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156289792&site=ehost-live
152,THE DISCIPLE–RKF LEARNING AND REASONING AGENT.,Gheorghe Tecuci,Computational Intelligence,08247935,,Nov2005,21,4,462,18.0,18417787,10.1111/j.1467-8640.2005.00282.x,Wiley-Blackwell,Article,MACHINE learning; ARTIFICIAL intelligence; PROBLEM solving; LEARNING strategies; REASONING,military center of gravity analysis; mixed-initiative reasoning; multistrategy apprenticeship learning; ontology; plausible version spaces; rule learning; task reduction,"Over the years we have developed the Disciple theory, methodology, and family of tools for building knowledge-based agents. This approach consists of developing an agent shell that can be taught directly by a subject matter expert in a way that resembles how the expert would teach a human apprentice when solving problems in cooperation. This paper presents the most recent version of the Disciple approach and its implementation in the Disciple–RKF (rapid knowledge formation) system. Disciple–RKF is based on mixed-initiative problem solving, where the expert solves the more creative parts of the problem and the agent solves the more routine ones, integrated teaching and learning, where the agent helps the expert to teach it, by asking relevant questions, and the expert helps the agent to learn, by providing examples, hints, and explanations, and multistrategy learning, where the agent integrates multiple learning strategies, such as learning from examples, learning from explanations, and learning by analogy, to learn from the expert how to solve problems. Disciple–RKF has been applied to build learning and reasoning agents for military center of gravity analysis, which are used in several courses at the US Army War College. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18417787&site=ehost-live
152,THE DISCIPLE–RKF LEARNING AND REASONING AGENT.,Mihai Boicu,Computational Intelligence,08247935,,Nov2005,21,4,462,18.0,18417787,10.1111/j.1467-8640.2005.00282.x,Wiley-Blackwell,Article,MACHINE learning; ARTIFICIAL intelligence; PROBLEM solving; LEARNING strategies; REASONING,military center of gravity analysis; mixed-initiative reasoning; multistrategy apprenticeship learning; ontology; plausible version spaces; rule learning; task reduction,"Over the years we have developed the Disciple theory, methodology, and family of tools for building knowledge-based agents. This approach consists of developing an agent shell that can be taught directly by a subject matter expert in a way that resembles how the expert would teach a human apprentice when solving problems in cooperation. This paper presents the most recent version of the Disciple approach and its implementation in the Disciple–RKF (rapid knowledge formation) system. Disciple–RKF is based on mixed-initiative problem solving, where the expert solves the more creative parts of the problem and the agent solves the more routine ones, integrated teaching and learning, where the agent helps the expert to teach it, by asking relevant questions, and the expert helps the agent to learn, by providing examples, hints, and explanations, and multistrategy learning, where the agent integrates multiple learning strategies, such as learning from examples, learning from explanations, and learning by analogy, to learn from the expert how to solve problems. Disciple–RKF has been applied to build learning and reasoning agents for military center of gravity analysis, which are used in several courses at the US Army War College. [ABSTRACT FROM AUTHOR] Copyright of Computational Intelligence is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18417787&site=ehost-live
153,Quasi-static responses and associated failure mechanisms of cold-formed steel roof trusses.,Doaa Bondok,Engineering Structures,01410296,,Mar2021,231,,N.PAG,1.0,148502985,10.1016/j.engstruct.2020.111741,Elsevier B.V.,Article,COLD-formed steel; TRUSSES; BLAST effect; FINITE element method; ROOFS; ALUMINUM foam; PEARLITIC steel; Roofing Contractors; Truss Manufacturing; Framing Contractors,"Cold-formed steel trusses; Finite element simulation; Large deformations, quasi-static tests; Roof trusses","• The static resistance and the associated failure mechanisms of cold-formed steel roof trusses were determined experimentally. • Advanced three-dimensional numerical models predicted the resistance and the ultimate failure mode. • The truss layout and the shape of loading significantly affect the performance of the truss and the failure mechanism. • For moderate damage levels, strengthening compression web members improve the absorbed energy. The use of cold-formed steel trusses in roof framing has significantly increased recently. Cold-formed steel roof trusses are ideal and efficient systems for a variety of applications due to their design flexibility and ease of construction. Past research explored the behavior of these truss systems up to the ultimate capacity point; however, the inelastic behavior to the failure was not fully captured. Information about the response beyond the buckling point and the energy absorption capacities are missing and need to be investigated. In this paper, small-scale cold-formed steel roof truss specimens were tested to failure under quasi-static loading. The static resistance of these systems and the associated failure mechanisms were identified. Such information is key input when analyzing these roof systems under blast loads using the Single Degree of Freedom simplified technique. Experimental results and absorbed energy comparisons show that the truss layout and the shape of loading significantly affect the performance of the truss and the failure mechanism. Three-dimensional finite element models were developed and verified against the experimental results. The advanced models predicted the static resistance with a high level of accuracy. Experimental and finite element analyses have shown that the energy absorbed is improved significantly when the web members susceptible to buckling are strengthened. [ABSTRACT FROM AUTHOR] Copyright of Engineering Structures is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148502985&site=ehost-live
153,Quasi-static responses and associated failure mechanisms of cold-formed steel roof trusses.,Girum Urgessa,Engineering Structures,01410296,,Mar2021,231,,N.PAG,1.0,148502985,10.1016/j.engstruct.2020.111741,Elsevier B.V.,Article,COLD-formed steel; TRUSSES; BLAST effect; FINITE element method; ROOFS; ALUMINUM foam; PEARLITIC steel; Roofing Contractors; Truss Manufacturing; Framing Contractors,"Cold-formed steel trusses; Finite element simulation; Large deformations, quasi-static tests; Roof trusses","• The static resistance and the associated failure mechanisms of cold-formed steel roof trusses were determined experimentally. • Advanced three-dimensional numerical models predicted the resistance and the ultimate failure mode. • The truss layout and the shape of loading significantly affect the performance of the truss and the failure mechanism. • For moderate damage levels, strengthening compression web members improve the absorbed energy. The use of cold-formed steel trusses in roof framing has significantly increased recently. Cold-formed steel roof trusses are ideal and efficient systems for a variety of applications due to their design flexibility and ease of construction. Past research explored the behavior of these truss systems up to the ultimate capacity point; however, the inelastic behavior to the failure was not fully captured. Information about the response beyond the buckling point and the energy absorption capacities are missing and need to be investigated. In this paper, small-scale cold-formed steel roof truss specimens were tested to failure under quasi-static loading. The static resistance of these systems and the associated failure mechanisms were identified. Such information is key input when analyzing these roof systems under blast loads using the Single Degree of Freedom simplified technique. Experimental results and absorbed energy comparisons show that the truss layout and the shape of loading significantly affect the performance of the truss and the failure mechanism. Three-dimensional finite element models were developed and verified against the experimental results. The advanced models predicted the static resistance with a high level of accuracy. Experimental and finite element analyses have shown that the energy absorbed is improved significantly when the web members susceptible to buckling are strengthened. [ABSTRACT FROM AUTHOR] Copyright of Engineering Structures is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148502985&site=ehost-live
154,Analysis and optimization based on reusable knowledge base of process performance models.,Alexander Brodsky,International Journal of Advanced Manufacturing Technology,02683768,,Jan2017,88,4-Jan,337,21.0,120738418,10.1007/s00170-016-8761-7,Springer Nature,Article,ARCHITECTURAL design; SOFTWARE frameworks; KNOWLEDGE base; COMPOSITE materials; DECISION support systems; USER interfaces,Data analytics; Domain specific user interface; Optimization; Process performance models; Reusable knowledge base; Smart manufacturing,"In this paper, we propose an architectural design and software framework for fast development of descriptive, diagnostic, predictive, and prescriptive analytics solutions for dynamic production processes. The proposed architecture and framework will support the storage of modular, extensible, and reusable knowledge base (KB) of process performance models. The approach requires developing automated methods that can translate the high-level models in the reusable KB into low-level specialized models required by a variety of underlying analysis tools, including data manipulation, optimization, statistical learning, estimation, and simulation. We also propose an organization and key structure for the reusable KB, composed of atomic and composite process performance models and domain-specific dashboards. Furthermore, we illustrate the use of the proposed architecture and framework by prototyping a decision support system for process engineers. The decision support system allows users to hierarchically compose and optimize dynamic production processes via a graphical user interface. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Advanced Manufacturing Technology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=120738418&site=ehost-live
154,Analysis and optimization based on reusable knowledge base of process performance models.,Daniel Menascé,International Journal of Advanced Manufacturing Technology,02683768,,Jan2017,88,4-Jan,337,21.0,120738418,10.1007/s00170-016-8761-7,Springer Nature,Article,ARCHITECTURAL design; SOFTWARE frameworks; KNOWLEDGE base; COMPOSITE materials; DECISION support systems; USER interfaces,Data analytics; Domain specific user interface; Optimization; Process performance models; Reusable knowledge base; Smart manufacturing,"In this paper, we propose an architectural design and software framework for fast development of descriptive, diagnostic, predictive, and prescriptive analytics solutions for dynamic production processes. The proposed architecture and framework will support the storage of modular, extensible, and reusable knowledge base (KB) of process performance models. The approach requires developing automated methods that can translate the high-level models in the reusable KB into low-level specialized models required by a variety of underlying analysis tools, including data manipulation, optimization, statistical learning, estimation, and simulation. We also propose an organization and key structure for the reusable KB, composed of atomic and composite process performance models and domain-specific dashboards. Furthermore, we illustrate the use of the proposed architecture and framework by prototyping a decision support system for process engineers. The decision support system allows users to hierarchically compose and optimize dynamic production processes via a graphical user interface. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Advanced Manufacturing Technology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=120738418&site=ehost-live
155,Desalination supply chain decision analysis and optimization.,Alexander Brodsky,Desalination,00119164,,Aug2014,347,,144,14.0,96786583,10.1016/j.desal.2014.05.037,Elsevier B.V.,Article,SALINE water conversion; SUPPLY chains; DECISION making; RENEWABLE energy sources; STRATEGIC planning; SAUDI Arabia,Decision analysis; Desalination; Optimization modeling; Strategic planning; Supply chain,"Abstract: The desalination industry has been growing progressively in the last few decades. A large number of new plants are contracted every year. Strategic decisions related to plant locations and capacity, the selection of the desalination technology, and many other technical decisions related to the plant design and operation are very critical to these strategic investments. Viewing the desalination industry network as a supply chain provides a holistic view allowing decision makers to perform optimization of water desalination operations end to end. The methodology we propose provides the decision makers with (1) a set of investment alternatives comprising combinations of the different desalination locations, capacities, technologies, and energy sources, and (2) a decision graph showing the performance of each decision alternative in terms of quantitative and qualitative performance metrics chosen by the decision maker. The case study of Saudi Arabia, the world leader in desalination, shows how the methodology can present strategic planners with an optimal configuration of the desalination supply chain. [Copyright &y& Elsevier] Copyright of Desalination is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=96786583&site=ehost-live
156,Optimal splitting for rare-event simulation.,Chun-Hung Chen,IIE Transactions,0740817X,,May2012,44,5,352,16.0,72338848,10.1080/0740817X.2011.596507,Taylor & Francis Ltd,Article,SIMULATION methods & models; STOCHASTIC analysis; PROBABILITY theory; COMPARATIVE studies; NUMERICAL analysis; MATHEMATICAL optimization,comparison of designs; Rare-event simulation; splitting,"Simulation is a popular tool for analyzing large, complex, stochastic engineering systems. When estimating rare-event probabilities, efficiency is a big concern, since a huge number of simulation replications may be needed in order to obtain a reasonable estimate of the rare-event probability. The idea of splitting has emerged as a promising variance reduction technique. The basic idea is to create separate copies (splits) of the simulation whenever it gets close to the rare event. Some splitting methods use an equal number of splits at all levels. This can compromise the efficiency and can even increase the estimation variance. This article formulates the problem of determining the number of splits as an optimization problem that minimizes the variance of an estimator subject to a constraint on the total computing budget. An optimal solution for a certain class of problems is derived that is then extended to the problem of choosing the better of two designs, where each design is evaluated via rare-event simulation. Theoretical results for the improvements that are achievable using the methods are provided. Numerical experiments indicate that the proposed approaches are efficient and robust. [ABSTRACT FROM AUTHOR] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=72338848&site=ehost-live
156,Optimal splitting for rare-event simulation.,Alexander Brodsky,IIE Transactions,0740817X,,May2012,44,5,352,16.0,72338848,10.1080/0740817X.2011.596507,Taylor & Francis Ltd,Article,SIMULATION methods & models; STOCHASTIC analysis; PROBABILITY theory; COMPARATIVE studies; NUMERICAL analysis; MATHEMATICAL optimization,comparison of designs; Rare-event simulation; splitting,"Simulation is a popular tool for analyzing large, complex, stochastic engineering systems. When estimating rare-event probabilities, efficiency is a big concern, since a huge number of simulation replications may be needed in order to obtain a reasonable estimate of the rare-event probability. The idea of splitting has emerged as a promising variance reduction technique. The basic idea is to create separate copies (splits) of the simulation whenever it gets close to the rare event. Some splitting methods use an equal number of splits at all levels. This can compromise the efficiency and can even increase the estimation variance. This article formulates the problem of determining the number of splits as an optimization problem that minimizes the variance of an estimator subject to a constraint on the total computing budget. An optimal solution for a certain class of problems is derived that is then extended to the problem of choosing the better of two designs, where each design is evaluated via rare-event simulation. Theoretical results for the improvements that are achievable using the methods are provided. Numerical experiments indicate that the proposed approaches are efficient and robust. [ABSTRACT FROM AUTHOR] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=72338848&site=ehost-live
157,Unauthorized inferences in semistructured databases,Alexander Brodsky,Information Sciences,00200255,,Nov2006,176,22,3269,31.0,22134565,10.1016/j.ins.2006.01.004,Elsevier B.V.,Article,ALGORITHMS; DATABASES; HORN clauses; LOGIC programming,Completeness; Entailment; Inference problem; Privacy; Semistructured data; Soundness,"Abstract: In this paper we study the problem of providing controlled access to confidential data stored in semistructured databases. More specifically, we focus on privacy violations via data inferences that occur when domain knowledge is combined with non-private data. We propose a formal model, called Privacy Information Flow Model, to represent the information flow and the privacy requirements. These privacy requirements are enforced by the Privacy Mediator. Privacy Mediator guarantees that users are not be able to logically entail information that violates the privacy requirements. We present an inference algorithm that is sound and complete. The inference algorithm is developed for a tree-like, semistructured data model, selection–projection queries, and domain knowledge, represented as Horn-clause constraints. [Copyright &y& Elsevier] Copyright of Information Sciences is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22134565&site=ehost-live
158,A note on coding and standardization of categorical variables in (sparse) group lasso regression.,Juan Cebral,Journal of Statistical Planning & Inference,03783758,,May2020,206,,1,11.0,140090765,10.1016/j.jspi.2019.08.003,Elsevier B.V.,Article,STANDARDIZATION; COMPUTATIONAL complexity; REGRESSION analysis,Group Lasso; Regularization; Standardization; Variable Selection,"Categorical regressor variables are usually handled by introducing a set of indicator variables, and imposing a linear constraint to ensure identifiability in the presence of an intercept, or equivalently, using one of various coding schemes. As proposed in Yuan and Lin (2006), the group lasso is a natural and computationally convenient approach to perform variable selection in settings with categorical covariates. As pointed out by Simon and Tibshirani (2012), ""standardization"" by means of block-wise orthonormalization of column submatrices each corresponding to one group of variables can substantially boost performance. In this note, we study the aspect of standardization for the special case of categorical predictors in detail. The main result is that orthonormalization is not required; column-wise scaling of the design matrix followed by re-scaling and centering of the coefficients is shown to have exactly the same effect. Similar reductions can be achieved in the case of interactions. The extension to the so-called sparse group lasso, which additionally promotes within-group sparsity, is considered as well. The importance of proper standardization is illustrated via simulations and a case study. • Fitting regularized regression models with categorical predictors depends on the encoding scheme. • The proposed standardization schemes yield significant reductions in terms of computational complexity. • The extension to interaction terms is discussed as well. • The effect of proper standardization is demonstrated on real and synthetic data sets. [ABSTRACT FROM AUTHOR] Copyright of Journal of Statistical Planning & Inference is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140090765&site=ehost-live
158,A note on coding and standardization of categorical variables in (sparse) group lasso regression.,Martin Slawski,Journal of Statistical Planning & Inference,03783758,,May2020,206,,1,11.0,140090765,10.1016/j.jspi.2019.08.003,Elsevier B.V.,Article,STANDARDIZATION; COMPUTATIONAL complexity; REGRESSION analysis,Group Lasso; Regularization; Standardization; Variable Selection,"Categorical regressor variables are usually handled by introducing a set of indicator variables, and imposing a linear constraint to ensure identifiability in the presence of an intercept, or equivalently, using one of various coding schemes. As proposed in Yuan and Lin (2006), the group lasso is a natural and computationally convenient approach to perform variable selection in settings with categorical covariates. As pointed out by Simon and Tibshirani (2012), ""standardization"" by means of block-wise orthonormalization of column submatrices each corresponding to one group of variables can substantially boost performance. In this note, we study the aspect of standardization for the special case of categorical predictors in detail. The main result is that orthonormalization is not required; column-wise scaling of the design matrix followed by re-scaling and centering of the coefficients is shown to have exactly the same effect. Similar reductions can be achieved in the case of interactions. The extension to the so-called sparse group lasso, which additionally promotes within-group sparsity, is considered as well. The importance of proper standardization is illustrated via simulations and a case study. • Fitting regularized regression models with categorical predictors depends on the encoding scheme. • The proposed standardization schemes yield significant reductions in terms of computational complexity. • The extension to interaction terms is discussed as well. • The effect of proper standardization is demonstrated on real and synthetic data sets. [ABSTRACT FROM AUTHOR] Copyright of Journal of Statistical Planning & Inference is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140090765&site=ehost-live
168,Improving the speed and accuracy of projection-type incompressible flow solvers,Juan Cebral,Computer Methods in Applied Mechanics & Engineering,00457825,,Apr2006,195,23/24,3087,23.0,20254209,10.1016/j.cma.2004.07.058,Elsevier B.V.,Article,GALERKIN methods; POISSON'S equation; COMPRESSIBILITY; PRESSURE,CFD; FEM; Incompressible flow solvers; Linelet preconditioning; LU-SGS; Multistage Runge–Kutta; Projection schemes,"Abstract: Superseding so-called first-generation incompressible flow solvers of the projection type (based on Taylor–Galerkin advection, second-order pressure damping and element-based data structures), the current, second-generation solvers (based on high-order upwind advection, fourth-order pressure damping and edge-based data structures) have now been in use for half a decade and have proven remarkably robust and efficient for many large-scale problems. In order to achieve higher accuracy and speed, these solvers have recently been enhanced in a variety of ways: (a) substepping for advection, (b) implicit treatment of advective terms via SGS and GMRES-LU-SGS iterative solvers, (c) fully implicit, time-accurate advancement of pressure and velocities, and (d) linelet preconditioning for the pressure-Poisson equation. The combined effect of these third-generation improvements leads to speedups of the order of O(1:5−1:10), with similar or even better temporal accuracy, as demonstrated on a variety of academic and industrial problems. [Copyright &y& Elsevier] Copyright of Computer Methods in Applied Mechanics & Engineering is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20254209&site=ehost-live
169,Incorporating variability of patient inflow conditions into statistical models for aneurysm rupture assessment.,Martin Slawski,Acta Neurochirurgica,00016268,,Mar2020,162,3,553,14.0,141985830,10.1007/s00701-020-04234-8,Springer Nature,Article,STATISTICAL models; COMPUTATIONAL fluid dynamics; RECEIVER operating characteristic curves; INTRACRANIAL aneurysms; CEREBRAL circulation,Cerebral aneurysm; Computational fluid dynamics; Hemodynamics; Prediction; Risk factors; Rupture,"Background: Hemodynamic patterns have been associated with cerebral aneurysm instability. For patient-specific computational fluid dynamics (CFD) simulations, the inflow rates of a patient are typically not known. The aim of this study was to analyze the influence of inter- and intra-patient variations of cerebral blood flow on the computed hemodynamics through CFD simulations and to incorporate these variations into statistical models for aneurysm rupture prediction. Methods: Image data of 1820 aneurysms were used for patient-specific steady CFD simulations with nine different inflow rates per case, capturing inter- and intra-patient flow variations. Based on the computed flow fields, 17 hemodynamic parameters were calculated and compared for the different flow conditions. Next, statistical models for aneurysm rupture were trained in 1571 of the aneurysms including hemodynamic parameters capturing the flow variations either by defining hemodynamic ""response variables"" (model A) or repeatedly randomly selecting flow conditions by patients (model B) as well as morphological and patient-specific variables. Both models were evaluated in the remaining 249 cases. Results: All hemodynamic parameters were significantly different for the varying flow conditions (p < 0.001). Both the flow-independent ""response"" model A and the flow-dependent model B performed well with areas under the receiver operating characteristic curve of 0.8182 and 0.8174 ± 0.0045, respectively. Conclusions: The influence of inter- and intra-patient flow variations on computed hemodynamics can be taken into account in multivariate aneurysm rupture prediction models achieving a good predictive performance. Such models can be applied to CFD data independent of the specific inflow boundary conditions. [ABSTRACT FROM AUTHOR] Copyright of Acta Neurochirurgica is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141985830&site=ehost-live
170,Mechanisms Involved in the Formation of BiocompatibleLipid Polymeric Hollow Patchy Particles.,Juan Cebral,Langmuir,07437463,,Jun2015,31,24,6639,10.0,103545644,10.1021/acs.langmuir.5b01551,American Chemical Society,Article,BIOMEDICAL materials; LIPID synthesis; ANISOTROPIC crystals; CROSS-sectional method; SHEARING force; SURFACE morphology,,"Patchypolymeric particles have anisotropic surface domains thatcan be remarkably useful in diverse medical and industrial fieldsbecause of their ability to simultaneously present two different surfacechemistries on the same construct. In this article, we report themechanisms involved in the formation of novel lipid–polymerichollow patchy particles during their synthesis. By cross-sectioningthe patchy particles, we found that a phase segregation phenomenonoccurs between the core, shell, and patch. Importantly, we found thatthe shear stress that the polymer blend undergoes during the particlesynthesis is the most important parameter for the formation of thesepatchy particles. In addition, we found that the interplay of solvent–solvent,polymer–solvent, and polymer–polymer–solventinteractions generates particles with different surface morphologies.Understanding the mechanisms involved in the formation of patchy particlesallows us to have a better control on their physicochemical properties.Therefore, these fundamental studies are critical to achieve batchcontrol and scalability, which are essential aspects that must beaddressed in any type of particle synthesis to be safely used in medicine. [ABSTRACT FROM AUTHOR] Copyright of Langmuir is the property of American Chemical Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103545644&site=ehost-live
171,Parabolic recovery of boundary gradients.,Juan Cebral,Communications in Numerical Methods in Engineering,10698299,,Dec2008,24,12,1611,5.0,35485663,10.1002/cnm.1054,Wiley-Blackwell,Article,STRAINS & stresses (Mechanics); STRUCTURAL analysis (Engineering); PARABOLIC differential equations; HEAT flux; HEAT transfer,CFD; higher order gradients; stress recovery,"A parabolic recovery procedure suited for shear stress and heat flux recovery on surfaces from linear element data is proposed. The information required consists of the usual unknowns at points, as well as gradients recovered at the points that are one layer away from the wall. The procedure has been in use for some time and has consistently delivered superior results as compared with the usual wall shear stress and heat flux obtained from linear finite element method shape functions. Copyright © 2007 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of Communications in Numerical Methods in Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=35485663&site=ehost-live
176,Unsteady wall shear stress analysis from image-based computational fluid dynamic aneurysm models under Newtonian and Casson rheological models.,Juan Cebral,Medical & Biological Engineering & Computing,01400118,,Oct2014,52,10,827,13.0,98286355,10.1007/s11517-014-1189-z,Springer Nature,Article,UNSTEADY flow; SHEARING force; COMPUTATIONAL fluid dynamics; INTRACRANIAL aneurysms; NEWTONIAN fluids; RHEOLOGY (Biology),Angiography; Casson flow; Cerebral aneurysms; Computational fluid dynamics; Wall shear stress,"The aim of this work was to determine whether or not Newtonian rheology assumption in image-based patient-specific computational fluid dynamics (CFD) cerebrovascular models harboring cerebral aneurysms may affect the hemodynamics characteristics, which have been previously associated with aneurysm progression and rupture. Ten patients with cerebral aneurysms with lobulations were considered. CFD models were reconstructed from 3DRA and 4DCTA images by means of region growing, deformable models, and an advancing front technique. Patient-specific FEM blood flow simulations were performed under Newtonian and Casson rheological models. Wall shear stress (WSS) maps were created and distributions were compared at the end diastole. Regions of lower WSS (lobulation) and higher WSS (neck) were identified. WSS changes in time were analyzed. Maximum, minimum and time-averaged values were calculated and statistically compared. WSS characterization remained unchanged. At high WSS regions, Casson rheology systematically produced higher WSS minimum, maximum and time-averaged values. However, those differences were not statistically significant. At low WSS regions, when averaging over all cases, the Casson model produced higher stresses, although in some cases the Newtonian model did. However, those differences were not significant either. There is no evidence that Newtonian model overestimates WSS. Differences are not statistically significant. [ABSTRACT FROM AUTHOR] Copyright of Medical & Biological Engineering & Computing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98286355&site=ehost-live
180,Connecting curves in higher dimensions.,Juan Cebral,Journal of Physics: A Mathematical & Theoretical,17518113,,5/30/2014,47,21,1,1.0,96340255,10.1088/1751-8113/47/21/215101,IOP Publishing,Article,CURVES; CURVILINEAR motion; PARAMETRIC equations; CURVES on surfaces; DYNAMICAL systems; APPLIED mathematics,,"Connecting curves have been shown to organize the rotational structure of strange attractors in three-dimensional dynamical systems. We extend the description of connecting curves and their properties to higher dimensions within a special class of differential dynamical systems. The general properties of connecting curves are derived and selection rules stated. Examples are presented to illustrate these properties for dynamical systems of dimension n = 3, 4, 5. [ABSTRACT FROM AUTHOR] Copyright of Journal of Physics: A Mathematical & Theoretical is the property of IOP Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=96340255&site=ehost-live
181,Development of a statistical model for discrimination of rupture status in posterior communicating artery aneurysms.,Martin Slawski,Acta Neurochirurgica,00016268,,Aug2018,160,8,1643,10.0,130694515,10.1007/s00701-018-3595-8,Springer Nature,Article,INTRACRANIAL aneurysm ruptures; INTRACRANIAL aneurysms; COMPUTATIONAL fluid dynamics; INTERNAL carotid artery; HEMODYNAMICS; SHEARING force; DISEASE risk factors,Cerebral aneurysm; Hemodynamics; Morphology; Posterior communicating artery; Prediction; Rupture,"Background: Intracranial aneurysms at the posterior communicating artery (PCOM) are known to have high rupture rates compared to other locations. We developed and internally validated a statistical model discriminating between ruptured and unruptured PCOM aneurysms based on hemodynamic and geometric parameters, angio-architectures, and patient age with the objective of its future use for aneurysm risk assessment.Methods: A total of 289 PCOM aneurysms in 272 patients modeled with image-based computational fluid dynamics (CFD) were used to construct statistical models using logistic group lasso regression. These models were evaluated with respect to discrimination power and goodness of fit using tenfold nested cross-validation and a split-sample approach to mimic external validation.Results: The final model retained maximum and minimum wall shear stress (WSS), mean parent artery WSS, maximum and minimum oscillatory shear index, shear concentration index, and aneurysm peak flow velocity, along with aneurysm height and width, bulge location, non-sphericity index, mean Gaussian curvature, angio-architecture type, and patient age. The corresponding area under the curve (AUC) was 0.8359. When omitting data from each of the three largest contributing hospitals in turn, and applying the corresponding model on the left-out data, the AUCs were 0.7507, 0.7081, and 0.5842, respectively.Conclusions: Statistical models based on a combination of patient age, angio-architecture, hemodynamics, and geometric characteristics can discriminate between ruptured and unruptured PCOM aneurysms with an AUC of 84%. It is important to include data from different hospitals to create models of aneurysm rupture that are valid across hospital populations. [ABSTRACT FROM AUTHOR] Copyright of Acta Neurochirurgica is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=130694515&site=ehost-live
185,Gene expression comparison of flow diversion and coiling in an experimental aneurysm model.,Juan Cebral,Journal of NeuroInterventional Surgery,17598478,,Dec2015,7,12,926,5.0,111309662,10.1136/neurintsurg-2014-011452,BMJ Publishing Group,Article,"INTRACRANIAL aneurysm surgery; AGAR; ANIMAL experimentation; CEREBRAL circulation; CHEMOKINES; COLLAGEN; ELECTROPHORESIS; FIBRONECTINS; FRACTURE fixation; GENE expression; MUSCLE proteins; NEUROSURGERY; OXIDOREDUCTASES; RABBITS; SURGICAL instruments; TUMOR necrosis factors; MATRIX metalloproteinases; Medicinal and Botanical Manufacturing; Pharmaceutical and medicine manufacturing; Poultry Processing; Fur-Bearing Animal and Rabbit Production; Medical, Dental, and Hospital Equipment and Supplies Merchant Wholesalers; Surgical Appliance and Supplies Manufacturing; Surgical and Medical Instrument Manufacturing; Other Electronic and Precision Equipment Repair and Maintenance",,"Background and purpose Mechanisms of both healing and complications, including spontaneous aneurysm rupture, remain unclear following flow diverter treatment. The aim of this study was to compare gene expression of various key molecules involved in the healing of aneurysms, between aneurysms treated with microcoils and flow diverters. Methods Saccular aneurysms were created in rabbits. Aneurysms were treated with coils (n=6) or flow diverters (n=6). Aneurysms were harvested at 4 weeks following treatment and used for gene expression and zymography experiments. Genes with a fold change of 1.2 or more were considered upregulated whereas those with a fold change of 0.8 or less were considered downregulated. Results All coil embolized aneurysms were completely occluded at follow-up. Two aneurysms were occluded and the remaining four samples were incompletely occluded in the flow diverter treated group. The following genes were expressed at lower levels in the flow diverter group compared with the coiled aneurysm group: proteinases (matrix metalloproteinases 2 and 9), cellular markers (endothelial nitric oxide synthase and smooth muscle actin), and structural proteins (collagens and fibronectin). Genes related to inflammation (tumor necrosis factor α and monocyte chemoattractant protein 1) were upregulated in flow diverter treated aneurysms compared with coil embolized aneurysms. Notably, the enzymatic activity of active matrix metalloproteinase 9 was high in aneurysms treated with flow diverters. Conclusions Our findings may provide improved understanding of rupture risk and healing following aneurysm treatment and inform development of therapies aimed at lowering rupture risk and accelerating healing. [ABSTRACT FROM AUTHOR] Copyright of Journal of NeuroInterventional Surgery is the property of BMJ Publishing Group and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=111309662&site=ehost-live
189,Hemodynamics in two tandem aneurysms treated with flow diverters.,Juan Cebral,International Journal for Numerical Methods in Biomedical Engineering,20407939,,Apr2014,30,4,517,8.0,95322584,10.1002/cnm.2614,Wiley-Blackwell,Article,HEMODYNAMICS; ANEURYSMS; TANDEM mass spectrometers; FUSION reactor divertors; ANGIOGRAPHY,cerebral aneurysms; flow diverters; hemodynamics,"SUMMARY The purpose of this study was to investigate whether the occlusion time of cerebral aneurysms treated with flow diverters depends on the hemodynamic conditions created immediately after treatment. A case study of a pair of tandem intracranial aneurysms that were treated with flow-diverting devices and occluded at different times was carried out. A patient-specific computational fluid dynamics model was constructed from 3D rotational angiography images. Blood flow simulations were carried out under pulsatile physiologic conditions, and hemodynamic variables before and after deployment of the flow-diverting devices were quantified and compared. The flow-diverting devices reduced aneurysm inflow rates, intra-aneurysmal flow velocities, shear rates, and wall shear stresses. The flow patterns after flow modulation by the flow diverters were smoother and with less swirling. The reductions in hemodynamic quantities depended on the aneurysm and parent artery and were larger in the aneurysm that occluded faster. The results of this case study suggest that the larger the reduction in the hemodynamic variables considered, the shorter the time it takes for the aneurysm to thrombose. This result can help us better define the goal of these interventions. Copyright © 2013 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of International Journal for Numerical Methods in Biomedical Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95322584&site=ehost-live
191,Patient-specific flow analysis of brain aneurysms at a single location: comparison of hemodynamic characteristics in small aneurysms.,Juan Cebral,Medical & Biological Engineering & Computing,01400118,,Nov2008,46,11,1113,8.0,35261178,10.1007/s11517-008-0400-5,Springer Nature,Article,ANEURYSMS; BRAIN diseases; HEMODYNAMICS; BLOOD circulation; VASCULAR diseases,Brain aneurysm; Flow analysis; Hemodynamics,"The purpose of this study is to examine and compare the hemodynamic characteristics of small aneurysms at the same anatomical location. Six internal carotid artery-ophthalmic artery aneurysms smaller than 10 mm were selected. Image-based computational fluid dynamics (CFD) techniques were used to simulate aneurysm hemodynamics. Flow velocity and wall shear stress (WSS) were also quantitatively compared, both in absolute value and relative value using the parent artery as a baseline. We found that flow properties were similar in ruptured and unruptured small aneurysms. However, the WSS was lower at the aneurysm site in unruptured aneurysms and higher in ruptured aneurysms ( P < 0.05). Hemodynamic analyses at a single location with similar size enabled us to directly compare the hemodynamics and clinical presentation of brain aneurysms. The results suggest that the WSS in an aneurysm sac can be an important hemodynamic parameter related to the mechanism of brain aneurysm growth and rupture. [ABSTRACT FROM AUTHOR] Copyright of Medical & Biological Engineering & Computing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=35261178&site=ehost-live
192,Patient-specific hemodynamic analysis of small internal carotid artery-ophthalmic artery aneurysms,Juan Cebral,Surgical Neurology,00903019,,Nov2009,72,5,444,7.0,45638735,10.1016/j.surneu.2008.12.013,Elsevier B.V.,Article,INTRACRANIAL aneurysm ruptures; BRAIN disease treatment; VASCULAR diseases; CAROTID artery diseases; HEMODYNAMICS; STANDARD deviations; COMPUTATIONAL fluid dynamics,Cerebral aneurysm; computational fluid dynamics ( CFD ); Flow analysis; Hemodynamics; internal carotid artery-ophthalmic artery ( ICA-Oph ); standard deviation ( SD ); Wall shear stress; wall shear stress ( WSS ),"Abstract: Background: Prophylactic treatment of unruptured small brain aneurysms is still controversial due to the low risk of rupture. Distinguishing which small aneurysms are at risk for rupture has become important for treatment. Previous studies have indicated a variety of hemodynamic properties that may influence aneurysm rupture. This study uses hemodynamic principles to evaluate these in the context of ruptured and unruptured small aneurysms in a single location. Methods: Eight small internal carotid artery-ophthalmic artery (ICA-Oph) aneurysms (<10 mm) were selected from the University of California, Los Angeles, database. We analyzed rupture-related hemodynamic characteristics including flow patterns, wall shear stress (WSS), and flow impingement using previously developed patient-specific computational fluid dynamics software. Results: Most ruptured aneurysms had complicated flow patterns in the aneurysm domes, but all of the unruptured cases showed a simple vortex. A reduction in flow velocity between the parent artery and the aneurysm sac was found in all the cases. Inside the aneurysms, the highest flow velocities were found either at the apex or neck. We also observed a trend of higher and more inhomogeneous WSS distribution within ruptured aneurysms (10.66 ± 5.99 Pa) in comparison with the unruptured ones (6.31 ± 6.47 Pa) (P < .01). Conclusion: A comparison of hemodynamic properties between ruptured and unruptured small ICA-Oph aneurysms found that some hemodynamic properties vary between small aneurysms although they are similar in size and share the same anatomical location. In particular, WSS may be a useful hemodynamic factor for studying small aneurysm rupture. [Copyright &y& Elsevier] Copyright of Surgical Neurology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=45638735&site=ehost-live
194,Relationship between aneurysm occlusion and flow diverting device oversizing in a rabbit model.,Juan Cebral,Journal of NeuroInterventional Surgery,17598478,,Jan2016,8,1,94,5.0,112059232,10.1136/neurintsurg-2014-011487,BMJ Publishing Group,Article,ANEURYSM surgery; ANEURYSMS; ANGIOGRAPHY; ANIMAL experimentation; RABBITS; SURGICAL stents; TREATMENT effectiveness; Fur-Bearing Animal and Rabbit Production; Poultry Processing,,"Background and purpose Implanted, actual flow diverter pore density is thought to be strongly influenced by proper matching between the device size and parent artery diameter. The objective of this study was to characterize the correlation between device sizing, metal coverage, and the resultant occlusion of aneurysms following flow diverter treatment in a rabbit model. Methods Rabbit saccular aneurysms were treated with flow diverters (iso-sized to proximal parent artery, 0.5 mm oversized, or 1.0 mm oversized, respectively, n=6 for each group). Eight weeks after implantation, the angiographic degree of aneurysm occlusion was graded (complete, near-complete, or incomplete). The ostium of the explanted aneurysm covered with the flow diverter struts was photographed. Based on gross anatomic findings, the metal coverage and pore density at the ostium of the aneurysm were calculated and correlated with the degree of aneurysm occlusion. Results Angiographic results showed there were no statistically significant differences in aneurysm geometry and occlusion among groups. The mean parent artery diameter to flow diverter diameter ratio was higher in the 1.0 mm oversized group than in the other groups. Neither the percentage metal coverage nor the pore density showed statistically significant differences among groups. Aneurysm occlusion was inversely correlated with the ostium diameter, irrespective of the size of the device implanted. Conclusions Device sizing alone does not predict resultant pore density or metal coverage following flow diverter implantation in the rabbit aneurysm model. Aneurysm occlusion was not impacted by either metal coverage or pore density, but was inversely correlated with the diameter of the ostium. [ABSTRACT FROM AUTHOR] Copyright of Journal of NeuroInterventional Surgery is the property of BMJ Publishing Group and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=112059232&site=ehost-live
196,The effect of aneurysm geometry on the intra-aneurysmal flow condition.,Juan Cebral,Neuroradiology,00283940,,Dec2010,52,12,1135,7.0,55315026,10.1007/s00234-010-0687-4,Springer Nature,Article,BLOOD flow measurement; BRAIN; CEREBRAL angiography; COMPUTER simulation; STATISTICAL correlation; HEMODYNAMICS; INTRACRANIAL aneurysms; QUANTITATIVE research; EVALUATION; PATHOLOGY; RADIOGRAPHY; DONOR blood supply,Aneurysm shape; Aneurysm size; Computational fluid dynamics simulation; Hemodynamics; Intracranial aneurysm,"Introduction: Various anatomical parameters affect on intra-aneurysmal hemodynamics. Nevertheless, how the shapes of real patient aneurysms affect on their intra-aneurysmal hemodynamics remains unanswered. Methods: Quantitative computational fluid dynamics simulation was conducted using eight patients' angiograms of internal carotid artery-ophthalmic artery aneurysms. The mean size of the intracranial aneurysms was 11.5 mm (range 5.8 to 19.9 mm). Intra-aneurysmal blood flow velocity and wall shear stress (WSS) were collected from three measurement planes in each aneurysm dome. The correlation coefficients ( r) were obtained between hemodynamic values (flow velocity and WSS) and the following anatomical parameters: averaged dimension of aneurysm dome, the largest aneurysm dome dimension, aspect ratio, and dome-neck ratio. Results: Negative linear correlations were observed between the averaged dimension of aneurysm dome and intra-aneurysmal flow velocity ( r = −0.735) and also WSS ( r = −0.736). The largest dome diameter showed a negative correlation with intra-aneurysmal flow velocity ( r = −0.731) and WSS ( r = −0.496). The aspect ratio demonstrated a weak negative correlation with the intra-aneurysmal flow velocity ( r = −0.381) and WSS ( r = −0.501). A clear negative correlation was seen between the intra-aneurysmal flow velocity and the dome-neck ratio ( r = −0.708). A weak negative correlation is observed between the intra-aneurysmal WSS and the dome-neck ratio ( r = −0.392). Conclusion: The aneurysm dome size showed a negative linear correlation with intra-aneurysmal flow velocity and WSS. Wide-necked aneurysm geometry was associated with faster intra-aneurysmal flow velocity. [ABSTRACT FROM AUTHOR] Copyright of Neuroradiology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=55315026&site=ehost-live
197,Computational modelling of blood flow in side arterial branches after stenting of cerebral aneurysms.,Juan Cebral,International Journal of Computational Fluid Dynamics,10618562,,Dec2008,22,10,669,8.0,35348388,10.1080/10618560802495255,Taylor & Francis Ltd,Article,SURGICAL stents; INTRACRANIAL aneurysms; HEMODYNAMIC monitoring; BLOOD flow; CEREBRAL ischemia; ARTERIAL occlusions; PHYSIOLOGY; THERAPEUTICS; All Other Miscellaneous Ambulatory Health Care Services; Medical equipment and supplies manufacturing; Surgical and Medical Instrument Manufacturing; All other ambulatory health care services,cerebral aneurysms; haemodynamics; image-based modelling; perforators; stenting,"The major concern with the use of stents as flow diverters for the treatment of intracranial aneurysms is the potential occlusion of a perforating artery or other side branches which can cause ischemic strokes. This article presents image-based patient-specific models of stented cerebral aneurysms in which a small side artery has been jailed by the stent mesh. The results indicate that, because of the large resistances of the distal vascular beds which dominate the flow divisions among the different arterial branches, the flow reduction in jailed side branches is quite small even when a large percentage of the inlet area of these branches has been blocked. This suggests that unless the side branch is completely occluded, it will likely maintain its normal blood flow. Although this conclusion eases the concern of stenting cerebral aneurysms, a complete occlusion can still be caused depending on the conformability characteristics of the stents. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Computational Fluid Dynamics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=35348388&site=ehost-live
199,Simulation of intracranial aneurysm stenting: Techniques and challenges,Juan Cebral,Computer Methods in Applied Mechanics & Engineering,00457825,,Sep2009,198,45/46,3567,16.0,44259967,10.1016/j.cma.2009.01.017,Elsevier B.V.,Article,SIMULATION methods & models; INTRACRANIAL aneurysms; BLOOD flow; COMPUTATIONAL fluid dynamics; HEMODYNAMICS; MEDICAL imaging systems,Adaptive embedded unstructured grids; Cerebral aneurysms; Computational fluid dynamics; Hemodynamics; Stenting,"Abstract: Recently, there has been considerable interest in the use of stents as endovascular flow diverters for the treatment of intracranial aneurysms. Simulating this novel method of treatment is essential for understanding the intra-aneurysmal hemodynamics in order to design better stents and to personalize and optimize the endovascular stenting procedures. This paper describes a methodology based on unstructured embedded grids for patient-specific modeling of stented cerebral aneurysms, demonstrates how the methodology can be used to address specific clinical questions, and discusses remaining technical issues. In particular, simulations are presented on a number of patient-specific models constructed from medical images and using different stent designs and treatment alternatives. Preliminary sensitivity analyses with respect to stent positioning and truncation of the stent model are presented. The results show that these simulations provide useful and valuable information that can be used during the planning phase of endovascular stenting interventions for the treatment of intracranial aneurysms. [Copyright &y& Elsevier] Copyright of Computer Methods in Applied Mechanics & Engineering is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=44259967&site=ehost-live
200,A Sequential Budget Allocation Framework for Simulation Optimization.,Chun-Hung Chen,IEEE Transactions on Automation Science & Engineering,15455955,,Apr2017,14,2,1185,10.0,122420359,10.1109/TASE.2015.2501386,IEEE,Article,MANUFACTURING industries; RESOURCE management; SIMULATION methods & models; ITERATIVE methods (Mathematics); PROBLEM solving,Computational modeling; Exploration and exploitation; Indexes; Numerical models; Optimization; Resource management; sequential procedure; simulation budget allocation; simulation optimization; Space exploration,"Many problems in automation and manufacturing are most suitable to be modeled as simulation optimization problems. Solving these problems typically involves two efforts: one is to explore the solution space, and the other is to exploit the performance values of the sampled solutions. When the amount of computing budget is limited, we need to know how to balance these two efforts in order to obtain the best result. In this study, we derive two measures to quantify the marginal contribution of exploring the search space and exploiting the performance values. A sequential budget allocation framework is designed by keeping the two measures approximately the same at each iteration. Numerical experiments on both continuous and discrete simulation optimization problems demonstrate that our new approach can significantly enhance the computing efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automation Science & Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122420359&site=ehost-live
201,An efficient simulation budget allocation method incorporating regression for partitioned domains.,Chun-Hung Chen,Automatica,00051098,,May2014,50,5,1391,10.0,95813698,10.1016/j.automatica.2014.03.011,Elsevier B.V.,Article,SIMULATION methods & models; BUDGET; ALLOCATION (Accounting); PARTITIONS (Mathematics); REGRESSION analysis; MATHEMATICAL domains; DECISION making; Public Finance Activities,Budget allocation; Regression; Simulation,"Abstract: Simulation can be a very powerful tool to help decision making in many applications but exploring multiple courses of actions can be time consuming. Numerous ranking and selection (R&S) procedures have been developed to enhance the simulation efficiency of finding the best design. To further improve efficiency, one approach is to incorporate information from across the domain into a regression equation. However, the use of a regression metamodel also inherits some typical assumptions from most regression approaches, such as the assumption of an underlying quadratic function and the simulation noise is homogeneous across the domain of interest. To extend the limitation while retaining the efficiency benefit, we propose to partition the domain of interest such that in each partition the mean of the underlying function is approximately quadratic. Our new method provides approximately optimal rules for between and within partitions that determine the number of samples allocated to each design location. The goal is to maximize the probability of correctly selecting the best design. Numerical experiments demonstrate that our new approach can dramatically enhance efficiency over existing efficient R&S methods. [Copyright &y& Elsevier] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95813698&site=ehost-live
201,An efficient simulation budget allocation method incorporating regression for partitioned domains.,Jie Xu,Automatica,00051098,,May2014,50,5,1391,10.0,95813698,10.1016/j.automatica.2014.03.011,Elsevier B.V.,Article,SIMULATION methods & models; BUDGET; ALLOCATION (Accounting); PARTITIONS (Mathematics); REGRESSION analysis; MATHEMATICAL domains; DECISION making; Public Finance Activities,Budget allocation; Regression; Simulation,"Abstract: Simulation can be a very powerful tool to help decision making in many applications but exploring multiple courses of actions can be time consuming. Numerous ranking and selection (R&S) procedures have been developed to enhance the simulation efficiency of finding the best design. To further improve efficiency, one approach is to incorporate information from across the domain into a regression equation. However, the use of a regression metamodel also inherits some typical assumptions from most regression approaches, such as the assumption of an underlying quadratic function and the simulation noise is homogeneous across the domain of interest. To extend the limitation while retaining the efficiency benefit, we propose to partition the domain of interest such that in each partition the mean of the underlying function is approximately quadratic. Our new method provides approximately optimal rules for between and within partitions that determine the number of samples allocated to each design location. The goal is to maximize the probability of correctly selecting the best design. Numerical experiments demonstrate that our new approach can dramatically enhance efficiency over existing efficient R&S methods. [Copyright &y& Elsevier] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95813698&site=ehost-live
202,An Optimal Sample Allocation Strategy for Partition-Based Random Search.,Chun-Hung Chen,IEEE Transactions on Automation Science & Engineering,15455955,,Jan2014,11,1,177,10.0,93570384,10.1109/TASE.2013.2251881,IEEE,Article,OPTIMAL control theory; SEARCH algorithms; GLOBAL optimization; ITERATIVE methods (Mathematics); NUMERICAL analysis; RANDOM variables,Global optimization; optimal sample allocation; partition-based random search,"Partition-based random search (PRS) provides a class of effective algorithms for global optimization. In each iteration of a PRS algorithm, the solution space is partitioned into subsets which are randomly sampled and evaluated. One subset is then determined to be the promising subset for further partitioning. In this paper, we propose the problem of allocating samples to each subset so that the samples are utilized most efficiently. Two types of sample allocation problems are discussed, with objectives of maximizing the probability of correctly selecting the promising subset (P\CSPS\) given a sample budget and minimizing the required sample size to achieve a satisfied level of P\CSPS\, respectively. An extreme value-based prospectiveness criterion is introduced and an asymptotically optimal solution to the two types of sample allocation problems is developed. The resulting optimal sample allocation strategy (OSAS) is an effective procedure for the existing PRS algorithms by intelligently utilizing the limited computing resources. Numerical tests confirm that OSAS is capable of increasing the P\CSPS\ in each iteration and subsequently improving the performance of PRS algorithms. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Automation Science & Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=93570384&site=ehost-live
203,An optimal -statistics quantile estimator for a set of location–scale populations,Chun-Hung Chen,Statistics & Probability Letters,01677152,,Oct2012,82,10,1853,6.0,77961656,10.1016/j.spl.2012.05.015,Elsevier B.V.,Article,OPTIMAL designs (Statistics); REGRESSION quantiles; VECTOR calculus; CONSTRAINED optimization; MEAN square algorithms; ERROR analysis in mathematics; PARAMETERS (Statistics),-statistics; Location equivariance; Location–scale distributions; Quantile estimator,"Abstract: This paper presents an -statistics quantile estimator for estimating the th quantile of a population which belongs to a set of location–scale distributions. The design of the weight vector of the estimator is formulated as a constrained optimization problem. The objective of the optimization problem is to minimize the mean square error. The optimization problem is subject to a unitary constraint on the weight vector of the -statistics quantile estimation. We solve the optimization problem and obtain an optimal solution, which is the weight vector of the proposed estimator. [Copyright &y& Elsevier] Copyright of Statistics & Probability Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=77961656&site=ehost-live
204,Balancing Search and Estimation in Random Search Based Stochastic Simulation Optimization.,Jie Xu,IEEE Transactions on Automatic Control,00189286,,Nov2016,61,11,3593,6.0,119139493,10.1109/TAC.2016.2522094,IEEE,Article,STOCHASTIC analysis; MATHEMATICAL optimization; SIMULATION methods & models; AUTOMATIC control systems; SEARCH algorithms,Algorithm design and analysis; Computational modeling; Estimation; Optimal number of replications; optimal sampling set size; Optimization; Partitioning algorithms; simulation optimization; Stochastic processes,"Stochastic simulation optimization involves two fundamental steps: 1) searching the solution space to generate candidate solutions for comparison and 2) estimating the performance of each candidate solution via multiple simulations and selecting a solution as the best solution found. Comparisons of solutions via simulation estimation are subject to error due to the stochastic noise in simulation output. While estimation errors can be reduced by increasing the number of simulation replications, it would in turn limit the number of candidate solutions that can be generated for comparison in a fixed computation budget. Under a random search framework, we derive an analytical formula to (approximately) optimally determine the number of candidate solutions generated in the search step and simulation replications in the estimation step to maximize the quality of the solution selected as the best by the random search algorithm. We then propose a practical method based on this formula and test the method on several common benchmark problems. Experiment results show that our method is quite effective and leads to significant improvement in the quality of the best solution found. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119139493&site=ehost-live
204,Balancing Search and Estimation in Random Search Based Stochastic Simulation Optimization.,Chun-Hung Chen,IEEE Transactions on Automatic Control,00189286,,Nov2016,61,11,3593,6.0,119139493,10.1109/TAC.2016.2522094,IEEE,Article,STOCHASTIC analysis; MATHEMATICAL optimization; SIMULATION methods & models; AUTOMATIC control systems; SEARCH algorithms,Algorithm design and analysis; Computational modeling; Estimation; Optimal number of replications; optimal sampling set size; Optimization; Partitioning algorithms; simulation optimization; Stochastic processes,"Stochastic simulation optimization involves two fundamental steps: 1) searching the solution space to generate candidate solutions for comparison and 2) estimating the performance of each candidate solution via multiple simulations and selecting a solution as the best solution found. Comparisons of solutions via simulation estimation are subject to error due to the stochastic noise in simulation output. While estimation errors can be reduced by increasing the number of simulation replications, it would in turn limit the number of candidate solutions that can be generated for comparison in a fixed computation budget. Under a random search framework, we derive an analytical formula to (approximately) optimally determine the number of candidate solutions generated in the search step and simulation replications in the estimation step to maximize the quality of the solution selected as the best by the random search algorithm. We then propose a practical method based on this formula and test the method on several common benchmark problems. Experiment results show that our method is quite effective and leads to significant improvement in the quality of the best solution found. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119139493&site=ehost-live
205,Dynamic Sampling Allocation Under Finite Simulation Budget for Feasibility Determination.,Chun-Hung Chen,INFORMS Journal on Computing,10919856,,Winter2022,34,1,557,12.0,155827904,10.1287/ijoc.2020.1057,INFORMS: Institute for Operations Research,Article,"MONTE Carlo method; STOCHASTIC systems; FINITE, The; RESOURCE allocation; CALL centers; Telephone call centres; Telemarketing Bureaus and Other Contact Centers",asymptotic optimality; dynamic sampling; feasibility determination; finite simulation budget,"Monte Carlo simulation is a commonly used tool for evaluating the performance of complex stochastic systems. In practice, simulation can be expensive, especially when comparing a large number of alternatives, thus motivating the need to intelligently allocate simulation replications. Given a finite set of alternatives whose means are estimated via simulation, we consider the problem of determining the subset of alternatives that have means smaller than a fixed threshold. A dynamic sampling procedure that possesses not only asymptotic optimality, but also desirable finite-sample properties is proposed. Theoretical results show that there is a significant difference between finite-sample optimality and asymptotic optimality. Numerical experiments substantiate the effectiveness of the new method. Summary of Contribution: Simulation is an important tool to estimate the performance of complex stochastic systems. We consider a feasibility determination problem of identifying all those among a finite set of alternatives with mean smaller than a given threshold, in which the means are unknown but can be estimated by sampling replications via stochastic simulation. This problem appears widely in many applications, including call center design and hospital resource allocation. Our work considers how to intelligently allocate simulation replications to different alternatives for efficiently finding the feasible alternatives. Previous work focuses on the asymptotic properties of the sampling allocation procedures, whereas our contribution lies in developing a finite-budget allocation rule that possesses both asymptotic optimality and desirable finite-budget properties. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155827904&site=ehost-live
206,Efficient Learning for Selecting Important Nodes in Random Network.,Chun-Hung Chen,IEEE Transactions on Automatic Control,00189286,,Mar2021,66,3,1321,8.0,148970687,10.1109/TAC.2020.2989753,IEEE,Article,MARKOV processes; TAYLOR'S series; STOCHASTIC processes; PROBABILITY theory; LEARNING,Adaptive systems; Backstepping; Bayesian learning; dynamic sampling; Lyapunov methods; Markov chain; network; Nonlinear systems; ranking and selection (R&S); Stability analysis; Stochastic processes; Uncertainty,"In this article, we consider the problem of selecting important nodes in a random network, where the nodes connect to each other randomly with certain transition probabilities. The node importance is characterized by the stationary probabilities of the corresponding nodes in a Markov chain defined over the network, as in Google's PageRank. Unlike a deterministic network, the transition probabilities in a random network are unknown but can be estimated by sampling. Under a Bayesian learning framework, we apply the first-order Taylor expansion and normal approximation to provide a computationally efficient posterior approximation of the stationary probabilities. In order to maximize the probability of correct selection, we propose a dynamic sampling procedure, which uses not only posterior means and variances of certain interaction parameters between different nodes, but also the sensitivities of the stationary probabilities with respect to each interaction parameter. Numerical experiment results demonstrate the superiority of the proposed sampling procedure. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148970687&site=ehost-live
207,Efficient Sampling Allocation Procedures for Optimal Quantile Selection.,Chun-Hung Chen,INFORMS Journal on Computing,10919856,,Winter2021,33,1,230,16.0,148514740,10.1287/ijoc.2019.0946,INFORMS: Institute for Operations Research,Article,SAMPLE size (Statistics); INFINITY (Mathematics),Bayesian framework; dynamic sampling allocation; quantile optimization; ranking and selection,"We propose a dynamic sampling allocation and selection paradigm for finding the alternative with the optimal quantile in a Bayesian framework. Myopic allocation policies (MAPs), analogous to existing methods in classic ranking and selection for selecting the alternative with the optimal mean, and computationally efficient selection policies are derived for selecting the alternative with the optimal quantile. Under certain conditions, we prove that the proposed MAPs and selection procedures are consistent, which means that the best quantile would be eventually correctly selected as the sample size goes to infinity. Numerical experiments demonstrate that the proposed schemes can significantly improve the performance. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148514740&site=ehost-live
208,Efficient simulation budget allocation with regression.,Chun-Hung Chen,IIE Transactions,0740817X,,Mar2013,45,3,291,18.0,84423218,10.1080/0740817X.2012.712238,Taylor & Francis Ltd,Article,SIMULATION methods & models; REGRESSION analysis; DECISION making; RANKING (Statistics); APPROXIMATION theory; EXPERIMENTAL design,optimal computing budget allocation; optimization using regression; Simulation optimization; stochastic optimization; stochastic simulation,"Simulation can be a very powerful tool to help decision making in many applications; however, exploring multiple courses of actions can be time consuming. Numerous Ranking & Selection (R&S) procedures have been developed to enhance the simulation efficiency of finding the best design. This article explores the potential of further enhancing R&S efficiency by incorporating simulation information from across the domain into a regression metamodel. This article assumes that the underlying function to be optimized is one-dimensional as well as approximately quadratic or piecewise quadratic. Under some common conditions in most regression-based approaches, the proposed method provides approximations of the optimal rules that determine the design locations to conduct simulation runs and the number of samples allocated to each design location. Numerical experiments demonstrate that the proposed approach can dramatically enhance efficiency over existing efficient R&S methods and can obtain significant savings over regression-based methods. In addition to utilizing concepts from the Design Of Experiments (DOE) literature, it introduces the probability of correct selection optimality criterion that underpins our new R&S method to the DOE literature. [ABSTRACT FROM PUBLISHER] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=84423218&site=ehost-live
209,Efficient Simulation Resource Sharing and Allocation for Selecting the Best.,Chun-Hung Chen,IEEE Transactions on Automatic Control,00189286,,Apr2013,58,4,1017,7.0,86391443,10.1109/TAC.2012.2215533,IEEE,Article,RESOURCE allocation; RANDOM numbers; RESOURCE management; STATISTICAL correlation; APPROXIMATION algorithms; NICKEL; MATHEMATICAL models; COMPUTER simulation; Metal service centres,Algorithm design and analysis; Approximation algorithms; Approximation methods; Computational modeling; Computing budget sharing; Correlation; multiple-comparison procedures; Nickel; optimal sampling schemes; Resource management; simulation budget allocation,"Common random numbers and the standard clock method are examples of effective variance reduction techniques that also share information and simulation resources when generating realizations of different simulated systems whose performances are being compared. This sharing of computing resources and the potentially widely different computational requirements for different simulation models are important considerations in allocating simulation replications among the candidate designs with the objective of maximizing the probability of selecting the best design, and we formulate the optimal computing budget allocation problem under this scenario. The resulting formulation leads to an optimization problem that can be viewed as a generalization of a correlated version considered in earlier work. An approximation to the problem is introduced to allow a tractable solution, for which a heuristic two-stage sequential allocation algorithm is proposed, and several numerical examples are used to illustrate the potential improvements that can be gained. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=86391443&site=ehost-live
210,Efficient Simulation Sampling Allocation Using Multifidelity Models.,Jie Xu,IEEE Transactions on Automatic Control,00189286,,Aug2019,64,8,3156,14.0,137857844,10.1109/TAC.2018.2886165,IEEE,Article,GAUSSIAN mixture models; INFORMATION modeling,Analytical models; Bayes methods; Clustering analysis; Computational modeling; Gaussian mixture model; multifidelity models; Optimization; ranking and selection; Resource management; sequential sampling; simulation optimization,"Simulation is often used to estimate the performance of alternative system designs for selecting the best. For a complex system, high-fidelity simulation is usually time-consuming and expensive. In this paper, we provide a new framework that integrates information from the multifidelity models to increase efficiency for selecting the best. A Gaussian mixture model is introduced to capture performance clustering information in the multifidelity models. Posterior information obtained by a clustering analysis incorporates both cluster-wise information and idiosyncratic information for each design. We propose a new budget allocation method to efficiently allocate high-fidelity simulation replications, utilizing posterior information. Numerical experiments show that the proposed multifidelity framework achieves a significant boost in efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137857844&site=ehost-live
210,Efficient Simulation Sampling Allocation Using Multifidelity Models.,Chun-Hung Chen,IEEE Transactions on Automatic Control,00189286,,Aug2019,64,8,3156,14.0,137857844,10.1109/TAC.2018.2886165,IEEE,Article,GAUSSIAN mixture models; INFORMATION modeling,Analytical models; Bayes methods; Clustering analysis; Computational modeling; Gaussian mixture model; multifidelity models; Optimization; ranking and selection; Resource management; sequential sampling; simulation optimization,"Simulation is often used to estimate the performance of alternative system designs for selecting the best. For a complex system, high-fidelity simulation is usually time-consuming and expensive. In this paper, we provide a new framework that integrates information from the multifidelity models to increase efficiency for selecting the best. A Gaussian mixture model is introduced to capture performance clustering information in the multifidelity models. Posterior information obtained by a clustering analysis incorporates both cluster-wise information and idiosyncratic information for each design. We propose a new budget allocation method to efficiently allocate high-fidelity simulation replications, utilizing posterior information. Numerical experiments show that the proposed multifidelity framework achieves a significant boost in efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137857844&site=ehost-live
211,Equipment Utilization Enhancement in Photolithography Area Through a Dynamic System Control Using Multi-Fidelity Simulation Optimization With Big Data Technique.,Edward Huang,IEEE Transactions on Semiconductor Manufacturing,08946507,,May2017,30,2,166,10.0,122903896,10.1109/TSM.2017.2693259,IEEE,Article,EQUIPMENT utilization; PHOTOLITHOGRAPHY equipment; BIG data; LITHOGRAPHY; MATHEMATICAL optimization; Other printing,Big Data; Control systems; Decision making; Dispatching; Industries; Industry 4.0; Lithography; multi-fidelity simulation optimization; Optimization; Photolithographic process,"Photolithographic (Photo) plays a key role in semiconductor manufacturing because of its importance to advanced process shrinking. Even with a small improvement in its operational efficiency, the cost competitiveness in production can be enhanced as a result of the huge amount of share capital cost. However, it is difficult to stabilize the throughput rhythm of Fabs, while keeping a high equipment utilization for Photo. In the light of Industry 4.0 and big data, a huge potential of maintaining a desired system performance by (near) real-time dynamic system control is highly anticipated. But it also poses challenges to intelligently handling mass data acquisition and allocating computing resources. This research aims to maximize the equipment utilization in Photo by an efficient multi-model simulation optimization approach with big data techniques in the era of Industry 4.0. dynamic Photo configurator and abnormality detector are the two critical units in our proposed system framework; the former can make a quick decision to optimize the system configuration while receiving the adjustment request from the latter. The results from an empirical study show the practical viability of proposed approach that the capacity loss in Photo has been effectively improved. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Semiconductor Manufacturing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122903896&site=ehost-live
211,Equipment Utilization Enhancement in Photolithography Area Through a Dynamic System Control Using Multi-Fidelity Simulation Optimization With Big Data Technique.,Chun-Hung Chen,IEEE Transactions on Semiconductor Manufacturing,08946507,,May2017,30,2,166,10.0,122903896,10.1109/TSM.2017.2693259,IEEE,Article,EQUIPMENT utilization; PHOTOLITHOGRAPHY equipment; BIG data; LITHOGRAPHY; MATHEMATICAL optimization; Other printing,Big Data; Control systems; Decision making; Dispatching; Industries; Industry 4.0; Lithography; multi-fidelity simulation optimization; Optimization; Photolithographic process,"Photolithographic (Photo) plays a key role in semiconductor manufacturing because of its importance to advanced process shrinking. Even with a small improvement in its operational efficiency, the cost competitiveness in production can be enhanced as a result of the huge amount of share capital cost. However, it is difficult to stabilize the throughput rhythm of Fabs, while keeping a high equipment utilization for Photo. In the light of Industry 4.0 and big data, a huge potential of maintaining a desired system performance by (near) real-time dynamic system control is highly anticipated. But it also poses challenges to intelligently handling mass data acquisition and allocating computing resources. This research aims to maximize the equipment utilization in Photo by an efficient multi-model simulation optimization approach with big data techniques in the era of Industry 4.0. dynamic Photo configurator and abnormality detector are the two critical units in our proposed system framework; the former can make a quick decision to optimize the system configuration while receiving the adjustment request from the latter. The results from an empirical study show the practical viability of proposed approach that the capacity loss in Photo has been effectively improved. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Semiconductor Manufacturing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122903896&site=ehost-live
212,On unbiased optimal -statistics quantile estimators,Chun-Hung Chen,Statistics & Probability Letters,01677152,,Nov2012,82,11,1891,7.0,79110145,10.1016/j.spl.2012.05.027,Elsevier B.V.,Article,OPTIMAL designs (Statistics); QUANTILES; ESTIMATION theory; LINEAR statistical models; POPULATION; ANALYSIS of variance,-statistics; Best linear unbiased estimator; Location-scale distributions; Unbiased quantile estimator,"Abstract: Recently, have presented two biased Optimal -statistics Quantile Estimators (OLQEs). In this work, we present two unbiased versions of the two biased OLQEs. Similar to the biased OLQEs, the proposed unbiased OLQEs are able to accommodate a set of scaled populations and a set of location-scale populations, respectively. Furthermore, we compare the proposed unbiased OLQEs with two state-of-the-art efficient unbiased estimators, called Best Linear Unbiased Estimators (BLUEs). Although OLQEs and BLUEs have different aims and models, we point out that the two proposed unbiased OLQEs are closely related to the two BLUEs, respectively. The differences between the unbiased OLQEs and the BLUEs are also provided. We conduct an experimental study to demonstrate that, for a set of location-scale populations and extreme quantiles, if the main concern is large biases, then a proposed unbiased location equivariance OLQE is more appealing. [Copyright &y& Elsevier] Copyright of Statistics & Probability Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=79110145&site=ehost-live
213,Optimal budget allocation for discrete-event simulation experiments.,Chun-Hung Chen,IIE Transactions,0740817X,,Jan2010,42,1,60,11.0,49144617,10.1080/07408170903116360,Taylor & Francis Ltd,Article,"SIMULATION methods & models; OPERATIONS research; NUMERICAL analysis; HEURISTIC algorithms; MATHEMATICAL analysis; Process, Physical Distribution, and Logistics Consulting Services",Discrete-event simulation; simulation optimization; simulation uncertainty,"Simulation plays a vital role in analyzing discrete-event systems, particularly in comparing alternative system designs with a view to optimizing system performance. Using simulation to analyze complex systems, however, can be both prohibitively expensive and time-consuming. Effective algorithms to allocate intelligently a computing budget for discrete-event simulation experiments are presented in this paper. These algorithms dynamically determine the simulation lengths for all simulation experiments and thus significantly improve simulation efficiency under the constraint of a given computing budget. Numerical illustrations are provided and the algorithms are compared with traditional two-stage ranking-and-selection procedures through numerical experiments. Although the proposed approach is based on heuristics, the numerical results indicate that it is much more efficient than the compared procedures. [ABSTRACT FROM AUTHOR] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=49144617&site=ehost-live
214,Optimal Budget Allocation Rule for Simulation Optimization Using Quadratic Regression in Partitioned Domains.,Chun-Hung Chen,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Jul2015,45,7,1047,16.0,103264621,10.1109/TSMC.2014.2383997,IEEE,Article,LARGE deviation theory; PROBABILITY theory; QUADRATIC programming; GEOMETRIC function theory; INDUSTRIAL electronics; CYBERNETICS; MATHEMATICAL models; Relay and Industrial Control Manufacturing,Budget allocation; Convergence; large deviation theory; Mathematical model; Modeling; Noise; Optimization; quadratic regression; ranking and selection; Resource management; simulation; Vectors,"Ranking and selection procedures have been successfully applied to enhance the efficiency of simulation in recent years. To further improve the efficiency, one approach is to incorporate the simulation output from across the domain into some response surfaces. In this paper, the domain of interest is divided into adjacent partitions and a quadratic regression function is assumed for the mean of the underlying function in each partition. Using the large deviation theory, an asymptotically optimal allocation rule is proposed with the objective of maximizing the probability of correctly selecting the best design point. The proposed simulation budget allocation rule is implemented in a heuristic sequential allocation algorithm and compared with some existing allocation rules. Numerical results illustrate the effectiveness of the proposed simulation budget allocation rule. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103264621&site=ehost-live
215,Optimal computing budget allocation for Monte Carlo simulation with application to product design,Chun-Hung Chen,Simulation Modelling Practice & Theory,1569190X,,Mar2003,11,1,57,18.0,9230467,10.1016/S1569-190X(02)00095-3,Elsevier B.V.,Article,MONTE Carlo method; MATHEMATICAL optimization; PRODUCT design; Industrial Design Services,Computing budget allocation; Intelligent simulation; Manufacturing design; Monte Carlo simulation; Yield analysis,"Ordinal optimization has emerged as an efficient technique for simulation and optimization, converging exponentially in many cases. In this paper, we present a new computing budget allocation approach that further enhances the efficiency of ordinal optimization. Our approach intelligently determines the best allocation of simulation trials or samples necessary to maximize the probability of identifying the optimal ordinal solution. We illustrate the approach’s benefits and ease of use by applying it to two electronic circuit design problems. Numerical results indicate the approach yields significant savings in computation time above and beyond the use of ordinal optimization. [Copyright &y& Elsevier] Copyright of Simulation Modelling Practice & Theory is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=9230467&site=ehost-live
216,Optimal Computing Budget Allocation for Stochastic N–$k$ Problem in the Power Grid System.,Chun-Hung Chen,IEEE Transactions on Reliability,00189529,,Sep2019,68,3,778,12.0,138433577,10.1109/TR.2019.2913741,IEEE,Article,GRIDS (Cartography); SYSTEM failures; STOCHASTIC processes; ELECTRIC lines; ASSIGNMENT problems (Programming); BUDGET; MONTE Carlo method; Public Finance Activities; Power and Communication Line and Related Structures Construction; Site Preparation Contractors,Computational modeling; Numerical models; Optimal computing budget allocation (OCBA); Power grids; Reliability; Resource management; stochastic $N$ − 1; Stochastic processes,"The $N$ − $k$ problem is very well known in the power industry and it tries to answer the question whether there exists a set of $k$ lines in a power network with $N$ elements whose removal would cause the failure of the system. In practice, it is common to evaluate a system according to an $N$ −1 criterion, i.e., $k = 1$. While this problem has traditionally been considered in a deterministic setting, stochastic behavior within the system is important especially in the context of extreme events. A number of stochastic Monte Carlo models have been proposed to estimate the probability of cascading failures. In this paper, we deal with simulation budget allocation of the stochastic $N$ –1 problem. More specifically, we assume that a simulation model is able to provide us an estimate of the system failure rate when any line is tripped. It is not difficult to see how simulation of all configurations to some certain accuracy can become computationally expensive with the growth of $N$. Under such a setting, we transform the $N$ –1 problem into a stochastic selection process with optimal computing budget allocation (OCBA): given $N$ configurations, we would like to sequentially allocate a certain number of simulation replications in order to answer the question whether the system is reliable or not. We show through theoretical analysis and numerical experiments that the probability of correctly identifying the system reliability state can be increased by applying OCBA allocation rules in the simulation budget allocation process. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Reliability is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138433577&site=ehost-live
217,Ranking and selection for terminating simulation under sequential sampling.,Chun-Hung Chen,IISE Transactions,24725854,,Jul2021,53,7,735,16.0,149844005,10.1080/24725854.2020.1785647,Taylor & Francis Ltd,Article,ASYMPTOTIC efficiencies; HEURISTIC,design of experiments; optimal computing budget allocation; ranking and selection; sequential sampling; Simulation,"This research develops an efficient ranking and selection procedure to select the best design for terminating simulation under sequential sampling. This approach enables us to obtain an accurate estimate of the mean performance at a particular point using regression in the case of a terminating simulation. The sequential sampling constraint is imposed to fully utilize the information along the simulation replication. The asymptotically optimal simulation budget allocation among all designs is derived concurrently with the optimal simulation run length and optimal number of simulation groups for each design. To implement the simulation budget allocation rule with a fixed finite simulation budget, a heuristic sequential simulation procedure is suggested with the objective of maximizing the probability of correct selection. Numerical experiments confirm the efficiency of the procedure relative to extant approaches. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149844005&site=ehost-live
218,Robust Sampling Budget Allocation Under Deep Uncertainty.,Jie Xu,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Oct2022,52,10,6339,9.0,159210427,10.1109/TSMC.2022.3144363,IEEE,Article,BUDGET; TECHNOLOGICAL innovations; UNCERTAINTY (Information theory); ENGINEERING design; Public Finance Activities; Engineering Services,Complex system design; Computational modeling; Linear programming; minimax regret; Optimization; Resource management; response surface methodologies; Response surface methodology; sampling budget allocation; Standards; Uncertainty,"A novel methodology is introduced for optimally allocating a sampling budget. Sampling budget allocation problems arise frequently in various settings. For example, in the design of complex engineering systems, given both the complexity of these systems and the imperfect information on new technologies, designers often face deep uncertainty as to system performance. Consequently, designers need to sample multiple alternative designs under a limited budget. This article proposes a minimax regret approach to allocate the sampling budget in the presence of deep uncertainty pertaining to system performance. The objective is to maximize the probability of selecting the design with the minimum–maximum regret under a limited sampling budget and imperfect information. To effectively solve the minimax regret problem, an approximation methodology that provides good solutions with quantifiable uncertainty is developed. The essence of the methodology, which has the added benefit of being generally applicable to any multilevel optimization, is that all but the first level of multilevel optimization can be eliminated via a response surface. By sampling many values of a higher level decision’s variables, solving the next lower level optimization given those samples values, and calibrating a response surface to the objective function value eliminate one required optimization. Doing this repeatedly reduces the complexity of the multilevel optimization to a standard optimization. Regardless of the number of levels in the optimization, repeating this process ultimately leaves one with a single optimization whose objective function can be directly computed, given the highest level variables. Numerical experiments with two sampling allocation examples demonstrate both the benefit of the robust sampling budget allocation versus nonrobust formulations and the effectiveness of the proposed solution approach. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159210427&site=ehost-live
218,Robust Sampling Budget Allocation Under Deep Uncertainty.,Edward Huang,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Oct2022,52,10,6339,9.0,159210427,10.1109/TSMC.2022.3144363,IEEE,Article,BUDGET; TECHNOLOGICAL innovations; UNCERTAINTY (Information theory); ENGINEERING design; Public Finance Activities; Engineering Services,Complex system design; Computational modeling; Linear programming; minimax regret; Optimization; Resource management; response surface methodologies; Response surface methodology; sampling budget allocation; Standards; Uncertainty,"A novel methodology is introduced for optimally allocating a sampling budget. Sampling budget allocation problems arise frequently in various settings. For example, in the design of complex engineering systems, given both the complexity of these systems and the imperfect information on new technologies, designers often face deep uncertainty as to system performance. Consequently, designers need to sample multiple alternative designs under a limited budget. This article proposes a minimax regret approach to allocate the sampling budget in the presence of deep uncertainty pertaining to system performance. The objective is to maximize the probability of selecting the design with the minimum–maximum regret under a limited sampling budget and imperfect information. To effectively solve the minimax regret problem, an approximation methodology that provides good solutions with quantifiable uncertainty is developed. The essence of the methodology, which has the added benefit of being generally applicable to any multilevel optimization, is that all but the first level of multilevel optimization can be eliminated via a response surface. By sampling many values of a higher level decision’s variables, solving the next lower level optimization given those samples values, and calibrating a response surface to the objective function value eliminate one required optimization. Doing this repeatedly reduces the complexity of the multilevel optimization to a standard optimization. Regardless of the number of levels in the optimization, repeating this process ultimately leaves one with a single optimization whose objective function can be directly computed, given the highest level variables. Numerical experiments with two sampling allocation examples demonstrate both the benefit of the robust sampling budget allocation versus nonrobust formulations and the effectiveness of the proposed solution approach. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159210427&site=ehost-live
218,Robust Sampling Budget Allocation Under Deep Uncertainty.,Chun-Hung Chen,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Oct2022,52,10,6339,9.0,159210427,10.1109/TSMC.2022.3144363,IEEE,Article,BUDGET; TECHNOLOGICAL innovations; UNCERTAINTY (Information theory); ENGINEERING design; Public Finance Activities; Engineering Services,Complex system design; Computational modeling; Linear programming; minimax regret; Optimization; Resource management; response surface methodologies; Response surface methodology; sampling budget allocation; Standards; Uncertainty,"A novel methodology is introduced for optimally allocating a sampling budget. Sampling budget allocation problems arise frequently in various settings. For example, in the design of complex engineering systems, given both the complexity of these systems and the imperfect information on new technologies, designers often face deep uncertainty as to system performance. Consequently, designers need to sample multiple alternative designs under a limited budget. This article proposes a minimax regret approach to allocate the sampling budget in the presence of deep uncertainty pertaining to system performance. The objective is to maximize the probability of selecting the design with the minimum–maximum regret under a limited sampling budget and imperfect information. To effectively solve the minimax regret problem, an approximation methodology that provides good solutions with quantifiable uncertainty is developed. The essence of the methodology, which has the added benefit of being generally applicable to any multilevel optimization, is that all but the first level of multilevel optimization can be eliminated via a response surface. By sampling many values of a higher level decision’s variables, solving the next lower level optimization given those samples values, and calibrating a response surface to the objective function value eliminate one required optimization. Doing this repeatedly reduces the complexity of the multilevel optimization to a standard optimization. Regardless of the number of levels in the optimization, repeating this process ultimately leaves one with a single optimization whose objective function can be directly computed, given the highest level variables. Numerical experiments with two sampling allocation examples demonstrate both the benefit of the robust sampling budget allocation versus nonrobust formulations and the effectiveness of the proposed solution approach. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159210427&site=ehost-live
219,A Coordinate Optimization Approach for Concurrent Design.,Edward Huang,IEEE Transactions on Automatic Control,00189286,,Jul2019,64,7,2913,8.0,137234542,10.1109/TAC.2018.2872196,IEEE,Article,COORDINATES; CHARGE coupled devices; NEW product development; INFORMATION design; INFORMATION sharing; Marketing Consulting Services,Charge coupled devices; Concurrent design (CCD); Convergence; Elevators; Information management; Linear programming; nonlinear programing; Optimization; Task analysis,"In concurrent design (CCD), multiple design teams execute their tasks simultaneously and then exchange information to update their designs. The process then iterates until the termination criterion is met. When properly controlled and executed, CCD can be an effective method to shorten the time in product development for complex and large-scale projects thanks to its parallel nature. In this note, we propose a coordinate optimization framework to model and control team coordination through information sharing in CCD. It can be shown that under a certain convexity assumption, CCD converges to a globally optimal design if the information sharing intensity is smaller than a certain threshold. Numerical experiments substantiate the theoretical results. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137234542&site=ehost-live
219,A Coordinate Optimization Approach for Concurrent Design.,Jie Xu,IEEE Transactions on Automatic Control,00189286,,Jul2019,64,7,2913,8.0,137234542,10.1109/TAC.2018.2872196,IEEE,Article,COORDINATES; CHARGE coupled devices; NEW product development; INFORMATION design; INFORMATION sharing; Marketing Consulting Services,Charge coupled devices; Concurrent design (CCD); Convergence; Elevators; Information management; Linear programming; nonlinear programing; Optimization; Task analysis,"In concurrent design (CCD), multiple design teams execute their tasks simultaneously and then exchange information to update their designs. The process then iterates until the termination criterion is met. When properly controlled and executed, CCD can be an effective method to shorten the time in product development for complex and large-scale projects thanks to its parallel nature. In this note, we propose a coordinate optimization framework to model and control team coordination through information sharing in CCD. It can be shown that under a certain convexity assumption, CCD converges to a globally optimal design if the information sharing intensity is smaller than a certain threshold. Numerical experiments substantiate the theoretical results. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137234542&site=ehost-live
219,A Coordinate Optimization Approach for Concurrent Design.,Chun-Hung Chen,IEEE Transactions on Automatic Control,00189286,,Jul2019,64,7,2913,8.0,137234542,10.1109/TAC.2018.2872196,IEEE,Article,COORDINATES; CHARGE coupled devices; NEW product development; INFORMATION design; INFORMATION sharing; Marketing Consulting Services,Charge coupled devices; Concurrent design (CCD); Convergence; Elevators; Information management; Linear programming; nonlinear programing; Optimization; Task analysis,"In concurrent design (CCD), multiple design teams execute their tasks simultaneously and then exchange information to update their designs. The process then iterates until the termination criterion is met. When properly controlled and executed, CCD can be an effective method to shorten the time in product development for complex and large-scale projects thanks to its parallel nature. In this note, we propose a coordinate optimization framework to model and control team coordination through information sharing in CCD. It can be shown that under a certain convexity assumption, CCD converges to a globally optimal design if the information sharing intensity is smaller than a certain threshold. Numerical experiments substantiate the theoretical results. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137234542&site=ehost-live
220,Advances in simulation optimization and its applications.,Chun-Hung Chen,IIE Transactions,0740817X,,Jul2013,45,7,683,2.0,86887348,10.1080/0740817X.2013.778709,Taylor & Francis Ltd,Article,SIMULATION methods & models; MATHEMATICAL optimization; MATHEMATICAL variables; PARALLEL algorithms; MATHEMATICAL bounds; PROBLEM solving,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=86887348&site=ehost-live
221,Efficient Splitting Simulation for Blackout Analysis.,Chun-Hung Chen,IEEE Transactions on Power Systems,08858950,,Jul2015,30,4,1775,9.0,103304575,10.1109/TPWRS.2014.2359920,IEEE,Article,ELECTRIC lines; ELECTRIC power failures; SIMULATION methods & models; ELECTRIC power distribution grids; ELECTRIC networks; Power and Communication Line and Related Structures Construction; Site Preparation Contractors,Analytical models; Blackout; cascading failure; Computational modeling; Numerical models; Power grids; Power system faults; Power system protection; rare-event probability; Resource management; stochastic simulation,"The analysis of severe blackouts has become an essential part of transmission grid planning and operation. This may include evaluation of rare-event probabilities, which can be difficult to estimate. While simulation offers flexibility to model large complex systems, efficiency remains a big concern when estimating very small probabilities. This paper presents an effective simulation technique to evaluate rare-event probabilities associated with cascading blackouts in an electric grid. We test our technique on an IEEE 118-bus electric network and show that it can dramatically improve simulation efficiency. We also demonstrate that the proposed technique can effectively locate vulnerable links. These are links whose failures lead to the highest probabilities of a blackout event. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Power Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103304575&site=ehost-live
221,Efficient Splitting Simulation for Blackout Analysis.,John Shortle,IEEE Transactions on Power Systems,08858950,,Jul2015,30,4,1775,9.0,103304575,10.1109/TPWRS.2014.2359920,IEEE,Article,ELECTRIC lines; ELECTRIC power failures; SIMULATION methods & models; ELECTRIC power distribution grids; ELECTRIC networks; Power and Communication Line and Related Structures Construction; Site Preparation Contractors,Analytical models; Blackout; cascading failure; Computational modeling; Numerical models; Power grids; Power system faults; Power system protection; rare-event probability; Resource management; stochastic simulation,"The analysis of severe blackouts has become an essential part of transmission grid planning and operation. This may include evaluation of rare-event probabilities, which can be difficult to estimate. While simulation offers flexibility to model large complex systems, efficiency remains a big concern when estimating very small probabilities. This paper presents an effective simulation technique to evaluate rare-event probabilities associated with cascading blackouts in an electric grid. We test our technique on an IEEE 118-bus electric network and show that it can dramatically improve simulation efficiency. We also demonstrate that the proposed technique can effectively locate vulnerable links. These are links whose failures lead to the highest probabilities of a blackout event. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Power Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103304575&site=ehost-live
222,GO-POLARS: A Steerable Stochastic Search on the Strength of Hyperspherical Coordinates.,Chun-Hung Chen,IEEE Transactions on Automatic Control,00189286,,Dec2017,62,12,6458,8.0,126586035,10.1109/TAC.2017.2657330,IEEE,Article,STOCHASTIC analysis; SEARCH algorithms; HYPERSPHERICAL method; FINITE element method; SIMULATED annealing,Algorithm design and analysis; Approximation algorithms; Convergence; GO-POLARS; gradient; hyperspherical coordinate; Jacobian matrices; Partitioning algorithms; polar coordinate; Probability density function; random search; Stochastic processes,"Search algorithms for optimizing a complex problem are mainly categorized as gradient-driven and stochastic search, each with its advantages and shortcomings. A newly developed algorithm, GO-POLARS, is proposed with a hyperspherical coordinate framework, which could perturb a given direction with well-controlled variation. It designs a steerable stochastic search algorithm that explores toward a promising direction, such as the gradient, at any desired levels. In this note, we provide an analytical study on the hyperspherical coordinates and the corresponding random distributions and, thus, prove the local convergence property of the GO-POLARS. Extensive numerical experiments are illustrated to show its advantages compared to conventional search algorithms. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126586035&site=ehost-live
223,Improving Analytic Hierarchy Process Expert Allocation Using Optimal Computing Budget Allocation.,Edward Huang,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Aug2016,46,8,1140,8.0,116872521,10.1109/TSMC.2015.2478754,IEEE,Article,ANALYTIC hierarchy process; DECISION making; NUMERICAL analysis,Aircraft; Analytic hierarchy process; Analytic hierarchy process (AHP); Analytical models; Convergence; multicriteria decision making; optimal computing budget allocation (OCBA); Reliability; Resource management,"The analytic hierarchy process (AHP) has been widely applied to multicriteria decision making problems. The AHP aids decision makers to determine the priorities of multiple criteria, and make reasonable decisions. In the AHP, evaluating candidate alternatives requires multiple experts’ evaluations to avoid personal subjectivity. Although having more experts can improve selection quality, inviting more experts also results in higher recruitment cost and longer evaluation time. In this paper, we introduce the idea of optimal computing budget allocation (OCBA) and propose a method, named the AHP_OCBA method, to improve the efficiency of expert allocation. The proposed method optimizes the allocation of experts to maximize the probability of correctly selecting the best alternative in the AHP. This method also can minimize the required number of experts to meet the probability of correct selection. An illustrative example is provided to indicate the implementation of the AHP_OCBA method. We show the improvement of the proposed method compared to proportional and equal allocation rules in the numerical result. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116872521&site=ehost-live
223,Improving Analytic Hierarchy Process Expert Allocation Using Optimal Computing Budget Allocation.,Chun-Hung Chen,"IEEE Transactions on Systems, Man & Cybernetics. Systems",21682216,,Aug2016,46,8,1140,8.0,116872521,10.1109/TSMC.2015.2478754,IEEE,Article,ANALYTIC hierarchy process; DECISION making; NUMERICAL analysis,Aircraft; Analytic hierarchy process; Analytic hierarchy process (AHP); Analytical models; Convergence; multicriteria decision making; optimal computing budget allocation (OCBA); Reliability; Resource management,"The analytic hierarchy process (AHP) has been widely applied to multicriteria decision making problems. The AHP aids decision makers to determine the priorities of multiple criteria, and make reasonable decisions. In the AHP, evaluating candidate alternatives requires multiple experts’ evaluations to avoid personal subjectivity. Although having more experts can improve selection quality, inviting more experts also results in higher recruitment cost and longer evaluation time. In this paper, we introduce the idea of optimal computing budget allocation (OCBA) and propose a method, named the AHP_OCBA method, to improve the efficiency of expert allocation. The proposed method optimizes the allocation of experts to maximize the probability of correctly selecting the best alternative in the AHP. This method also can minimize the required number of experts to meet the probability of correct selection. An illustrative example is provided to indicate the implementation of the AHP_OCBA method. We show the improvement of the proposed method compared to proportional and equal allocation rules in the numerical result. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics. Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116872521&site=ehost-live
224,Optimal Computing Budget Allocation for regression with gradient information.,Jie Xu,Automatica,00051098,,Dec2021,134,,N.PAG,1.0,153175918,10.1016/j.automatica.2021.109927,Elsevier B.V.,Article,STOCHASTIC systems; REGRESSION analysis,Computing budget allocation; Gradient information; Quadratic model; Simulation optimization; Stochastic systems,"We consider the problem of optimizing the performance of a stochastic system, e.g., a discrete-event system, where the system performance is evaluated using stochastic simulations. Our objective is to allocate simulation budget to maximize the probability of correct selection (PCS) of the best design, where both system performance and gradient information can be obtained simultaneously via simulation. The objective function is assumed to be quadratic, or can be approximated by a quadratic regression model. The main contribution of our work is to utilize gradient information to enhance the efficiency of traditional Optimal Computing Budget Allocation (OCBA). We develop near-optimal rules that determine design points where simulations should be run and the number of runs allocated to each point. Our numerical experiments demonstrate that the proposed approach performs much better than other existing ranking and selection methods, even in cases where derivative information is very noisy and its simulation cost is high. [ABSTRACT FROM AUTHOR] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153175918&site=ehost-live
224,Optimal Computing Budget Allocation for regression with gradient information.,Chun-Hung Chen,Automatica,00051098,,Dec2021,134,,N.PAG,1.0,153175918,10.1016/j.automatica.2021.109927,Elsevier B.V.,Article,STOCHASTIC systems; REGRESSION analysis,Computing budget allocation; Gradient information; Quadratic model; Simulation optimization; Stochastic systems,"We consider the problem of optimizing the performance of a stochastic system, e.g., a discrete-event system, where the system performance is evaluated using stochastic simulations. Our objective is to allocate simulation budget to maximize the probability of correct selection (PCS) of the best design, where both system performance and gradient information can be obtained simultaneously via simulation. The objective function is assumed to be quadratic, or can be approximated by a quadratic regression model. The main contribution of our work is to utilize gradient information to enhance the efficiency of traditional Optimal Computing Budget Allocation (OCBA). We develop near-optimal rules that determine design points where simulations should be run and the number of runs allocated to each point. Our numerical experiments demonstrate that the proposed approach performs much better than other existing ranking and selection methods, even in cases where derivative information is very noisy and its simulation cost is high. [ABSTRACT FROM AUTHOR] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153175918&site=ehost-live
226,A Model for Understanding How Virtual Reality Aids Complex Conceptual Learning.,Jim Chen,PRESENCE: Teleoperators & Virtual Environments,10547460,,Jun99,8,3,293,24.0,2025604,10.1162/105474699566242,MIT Press,Article,VIRTUAL reality; LEARNING; TECHNOLOGICAL innovations,,"Designers and evaluators of immersive virtual reality systems have many ideas concerning how virtual reality can facilitate learning. However, we have little information concerning which of virtual reality's features provide the most leverage for enhancing understanding or how to customize those affordances for different learning environments. In part, this reflects the truly complex nature of learning. Features of a learning environment do not act in isolation; other factors such as the concepts or skills to be learned, individual characteristics, the learning experience, and the interaction experience all play a role in shaping the learning process and its outcomes. Through Project ScienceSpace, we have been trying to identify, use, and evaluate immersive virtual reality's affordances as a means to facilitate the mastery of complex, abstract concepts. In doing so, we are beginning to understand the interplay between virtual reality's features and other important factors in shaping the learning process and learning outcomes for this type of material. In this paper, we present a general model that describes how we think these factors work together and discuss some of the lessons we are learning about virtual reality's affordances in the context of this model for complex conceptual learning. [ABSTRACT FROM AUTHOR] Copyright of PRESENCE: Teleoperators & Virtual Environments is the property of MIT Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=2025604&site=ehost-live
230,Low-rank representation with graph regularization for subspace clustering.,Jim Chen,"Soft Computing - A Fusion of Foundations, Methodologies & Applications",14327643,,Mar2017,21,6,1569,13.0,121658578,10.1007/s00500-015-1869-0,Springer Nature,Article,SUBSPACES (Mathematics); MATHEMATICAL regularization; ROBUST control; LAPLACIAN matrices; ALGORITHMS,Graph regularization; Low-dimensional subspace; Low-rank representation; Matrix completion; Matrix recovery; Subspace clustering,"In this paper, we propose a low-rank representation method that incorporates graph regularization for robust subspace clustering. We make the assumption that high-dimensional data can be approximated as the union of low-dimensional subspaces of unknown dimension. The proposed method extends the low-rank representation algorithm by incorporating graph regularization with a discriminative dictionary. Existing low-rank representation methods for subspace clustering use noisy data as the dictionary. The proposed technique, however, takes advantage of the discriminative dictionary to seek the lowest-rank representation by virtue of matrix recovery and completion techniques. Moreover, the discriminative dictionary is further used to construct a graph Laplacian to separate the low-rank representation of high-dimensional data. The proposed algorithm can recover the low-dimensional subspace structure from high-dimensional observations (which are often corrupted by gross errors). Simultaneously, the samples are clustered into their corresponding underlying subspaces. Extensive experimental results on benchmark databases demonstrate the efficiency and effectiveness of the proposed algorithm for subspace clustering. [ABSTRACT FROM AUTHOR] Copyright of Soft Computing - A Fusion of Foundations, Methodologies & Applications is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121658578&site=ehost-live
233,Robust object tracking using enhanced random ferns.,Jim Chen,Visual Computer,01782789,,Apr2014,30,4,351,8.0,94942098,10.1007/s00371-013-0860-y,Springer Nature,Article,OBJECT tracking (Computer vision); COMPUTER vision; KERNEL operating systems; KERNEL functions; COMPLEX variables,Enhanced random ferns; Hidden classes; Object tracking; On-line clustering,"This paper presents a method to address the problem of long-term robust object tracking in unconstrained environments. An enhanced random fern is proposed and integrated into our tracking framework as the object detector, whose main idea is to exploit the potential distribution properties of feature vectors which are here called hidden classes by on-line clustering of feature space for each leaf-node of ferns. The kernel density estimation technique is then used to evaluate unlabeled samples based on the hidden classes which are set as the data points of the kernel function. Experimental results on challenging real-world video sequences demonstrate the effectiveness and robustness of our approach. Comparisons with several state-of-the-art approaches are provided. [ABSTRACT FROM AUTHOR] Copyright of Visual Computer is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94942098&site=ehost-live
235,"ACM/Springer Mobile Networks and Applications (MONET) Special Issue on 'Collaborative Computing: Networking, Applications and Worksharing'.",Songqing Chen,Mobile Networks & Applications,1383469X,,Aug2012,17,4,506,2.0,77569103,10.1007/s11036-012-0369-z,Springer Nature,Editorial,WIRELESS communications -- Congresses; ROUTING (Computer network management); COMPUTER network resources; SENSOR networks; NETWORK routing protocols; CONFERENCES & conventions; Convention and Trade Show Organizers; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),,"Information about the 6th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom) is presented. One paper described two broadcast authentication schemes such as key pool scheme and key chain scheme. Another paper introduced an analytical framework needed in the assessment of cooperative transmissions in sensor networks. Third paper focused on the development of utility-based routing scheme (UDR).",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=77569103&site=ehost-live
236,An Application-Level Data Transparent Authentication Scheme without Communication Overhead.,Songqing Chen,IEEE Transactions on Computers,00189340,,Jul2010,59,7,943,12.0,51197325,10.1109/TC.2010.80,IEEE,Article,BANDWIDTHS; AUTHENTICATION (Law); BROADBAND communication systems; WIRELESS communications; PROTOTYPES; Wired Telecommunications Carriers; Wireless Telecommunications Carriers (except Satellite); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,authentication; covert channel; DaTA; data transparent; timing correlation,"With abundant aggregate network bandwidth, continuous data streams are commonly used in scientific and commercial applications. Correspondingly, there is an increasing demand of authenticating these data streams. Existing strategies explore data stream authentication by using message authentication codes (MACs) on a certain number of data packets (a data block) to generate a message digest, then either embedding the digest into the original data, or sending the digest out-of-band to the receiver. Embedding approaches inevitably change the original data, which is not acceptable under some circumstances (e.g., when sensitive information is included in the data). Sending the digest out-of-band incurs additional communication overhead, which consumes more critical resources (e.g., power in wireless devices for receiving information) besides network bandwidth. In this paper, we propose a novel strategy, DaTA, which effectively authenticates data streams by selectively adjusting some interpacket delay. This authentication scheme requires no change to the original data and no additional communication overhead. Modeling-based analysis and experiments conducted on an implemented prototype system in an LAN and over the Internet show that our proposed scheme is efficient and practical. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=51197325&site=ehost-live
236,An Application-Level Data Transparent Authentication Scheme without Communication Overhead.,Xinyuan Wang,IEEE Transactions on Computers,00189340,,Jul2010,59,7,943,12.0,51197325,10.1109/TC.2010.80,IEEE,Article,BANDWIDTHS; AUTHENTICATION (Law); BROADBAND communication systems; WIRELESS communications; PROTOTYPES; Wired Telecommunications Carriers; Wireless Telecommunications Carriers (except Satellite); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,authentication; covert channel; DaTA; data transparent; timing correlation,"With abundant aggregate network bandwidth, continuous data streams are commonly used in scientific and commercial applications. Correspondingly, there is an increasing demand of authenticating these data streams. Existing strategies explore data stream authentication by using message authentication codes (MACs) on a certain number of data packets (a data block) to generate a message digest, then either embedding the digest into the original data, or sending the digest out-of-band to the receiver. Embedding approaches inevitably change the original data, which is not acceptable under some circumstances (e.g., when sensitive information is included in the data). Sending the digest out-of-band incurs additional communication overhead, which consumes more critical resources (e.g., power in wireless devices for receiving information) besides network bandwidth. In this paper, we propose a novel strategy, DaTA, which effectively authenticates data streams by selectively adjusting some interpacket delay. This authentication scheme requires no change to the original data and no additional communication overhead. Modeling-based analysis and experiments conducted on an implemented prototype system in an LAN and over the Internet show that our proposed scheme is efficient and practical. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=51197325&site=ehost-live
237,Developing Subdomain Allocation Algorithms Based on Spatial and Communicational Constraints to Accelerate Dust Storm Simulation.,Songqing Chen,PLoS ONE,19326203,,4/4/2016,11,4,1,33.0,114208716,10.1371/journal.pone.0152250,Public Library of Science,Article,DUST storms; ALGORITHMS; COMPUTER simulation; PREDICTION models; HIGH performance computing; LINEAR programming,Algebra; Algorithms; Applied mathematics; Computer and information sciences; Computing methods; Data visualization; Dust; Graphs; Infographics; Linear programming; Materials by structure; Materials science; Mathematical and statistical techniques; Mathematical functions; Mathematics; Optimization; Physical sciences; Polynomials; Research and analysis methods; Research Article; Simulation and modeling,"Dust storm has serious disastrous impacts on environment, human health, and assets. The developments and applications of dust storm models have contributed significantly to better understand and predict the distribution, intensity and structure of dust storms. However, dust storm simulation is a data and computing intensive process. To improve the computing performance, high performance computing has been widely adopted by dividing the entire study area into multiple subdomains and allocating each subdomain on different computing nodes in a parallel fashion. Inappropriate allocation may introduce imbalanced task loads and unnecessary communications among computing nodes. Therefore, allocation is a key factor that may impact the efficiency of parallel process. An allocation algorithm is expected to consider the computing cost and communication cost for each computing node to minimize total execution time and reduce overall communication cost for the entire simulation. This research introduces three algorithms to optimize the allocation by considering the spatial and communicational constraints: 1) an Integer Linear Programming (ILP) based algorithm from combinational optimization perspective; 2) a K-Means and Kernighan-Lin combined heuristic algorithm (K&K) integrating geometric and coordinate-free methods by merging local and global partitioning; 3) an automatic seeded region growing based geometric and local partitioning algorithm (ASRG). The performance and effectiveness of the three algorithms are compared based on different factors. Further, we adopt the K&K algorithm as the demonstrated algorithm for the experiment of dust model simulation with the non-hydrostatic mesoscale model (NMM-dust) and compared the performance with the MPI default sequential allocation. The results demonstrate that K&K method significantly improves the simulation performance with better subdomain allocation. This method can also be adopted for other relevant atmospheric and numerical modeling. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=114208716&site=ehost-live
238,Measurement and Analysis of an Internet Streaming Service to Mobile Devices.,Fei Li,IEEE Transactions on Parallel & Distributed Systems,10459219,,Nov2013,24,11,2240,11.0,90678082,10.1109/TPDS.2012.324,IEEE,Article,"STREAMING video & television; STREAMING audio; CLOUD computing; MOBILE communication systems software; CACHE memory; TRANSCODING; STREAMING technology; Wireless Telecommunications Carriers (except Satellite); Data Processing, Hosting, and Related Services; Internet Publishing and Broadcasting and Web Search Portals",Cloud computing; heterogeneity; Internet mobile streaming; Mobile communication; Mobile handsets; popularity; Servers; Streaming media; transcoding; Video codecs,"Receiving Internet streaming services on various mobile devices is getting increasingly popular, and cloud platforms have also been gradually employed for delivering streaming services to mobile devices. While a number of studies have been conducted at the client side to understand and characterize Internet mobile streaming delivery, little is known about the server side, particularly for the recent cloud-based Internet mobile streaming delivery. In this work, we aim to investigate the Internet mobile streaming service at the server side. For this purpose, we have collected a 4-month server-side log on the cloud (with 1,002 TB delivered video traffic) from a top Internet mobile streaming service provider serving worldwide mobile users. Through trace analysis, we find that 1) a major challenge for providing Internet mobile streaming services is rooted from the mobile device hardware and software heterogeneity. In this workload, we find over 3,400 different hardware models with more than 100 different screen resolutions running 14 different mobile OS and three audio codecs and four video codecs. 2) To deal with the device heterogeneity, CPU-intensive transcoding is used on the cloud to customize the video to the appropriate versions at runtime for different devices. A video clip could be transcoded into more than 40 different versions to serve requests from different devices. 3) Compared to videos in traditional Internet streaming, mobile streaming videos are typically of much smaller size (a median of 1.68 MBytes) and shorter duration (a median of 2.7 minutes). Furthermore, the daily mobile user accesses are more skewed following a Zipf-like distribution but users' interests also quickly shift. Considering the huge demand of CPU cycles for online transcoding, we further examine server-side caching to reduce the total CPU cycle demand from the cloud. We show that a policy considering different versions of a video altogether outperforms other intuitive ones when the cache size is limited. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=90678082&site=ehost-live
238,Measurement and Analysis of an Internet Streaming Service to Mobile Devices.,Songqing Chen,IEEE Transactions on Parallel & Distributed Systems,10459219,,Nov2013,24,11,2240,11.0,90678082,10.1109/TPDS.2012.324,IEEE,Article,"STREAMING video & television; STREAMING audio; CLOUD computing; MOBILE communication systems software; CACHE memory; TRANSCODING; STREAMING technology; Wireless Telecommunications Carriers (except Satellite); Data Processing, Hosting, and Related Services; Internet Publishing and Broadcasting and Web Search Portals",Cloud computing; heterogeneity; Internet mobile streaming; Mobile communication; Mobile handsets; popularity; Servers; Streaming media; transcoding; Video codecs,"Receiving Internet streaming services on various mobile devices is getting increasingly popular, and cloud platforms have also been gradually employed for delivering streaming services to mobile devices. While a number of studies have been conducted at the client side to understand and characterize Internet mobile streaming delivery, little is known about the server side, particularly for the recent cloud-based Internet mobile streaming delivery. In this work, we aim to investigate the Internet mobile streaming service at the server side. For this purpose, we have collected a 4-month server-side log on the cloud (with 1,002 TB delivered video traffic) from a top Internet mobile streaming service provider serving worldwide mobile users. Through trace analysis, we find that 1) a major challenge for providing Internet mobile streaming services is rooted from the mobile device hardware and software heterogeneity. In this workload, we find over 3,400 different hardware models with more than 100 different screen resolutions running 14 different mobile OS and three audio codecs and four video codecs. 2) To deal with the device heterogeneity, CPU-intensive transcoding is used on the cloud to customize the video to the appropriate versions at runtime for different devices. A video clip could be transcoded into more than 40 different versions to serve requests from different devices. 3) Compared to videos in traditional Internet streaming, mobile streaming videos are typically of much smaller size (a median of 1.68 MBytes) and shorter duration (a median of 2.7 minutes). Furthermore, the daily mobile user accesses are more skewed following a Zipf-like distribution but users' interests also quickly shift. Considering the huge demand of CPU cycles for online transcoding, we further examine server-side caching to reduce the total CPU cycle demand from the cloud. We show that a policy considering different versions of a video altogether outperforms other intuitive ones when the cache size is limited. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=90678082&site=ehost-live
239,Risk-aware multi-objective optimized virtual machine placement in the cloud.,Songqing Chen,Journal of Computer Security,0926227X,,2018,26,5,707,24.0,134583443,10.3233/JCS-171104,IOS Press,Article,VIRTUAL machine systems; CLOUD computing security measures; RESOURCE allocation; RISK assessment; COMPUTER network security,Cloud security; multiple objective; risk metrics model; virtual machine placement; VM allocation strategy,"Cloud computing, while becoming more and more popular as a dominant computing platform, introduces new security challenges. When virtual machines are deployed in a cloud environment, virtual machine placement strategies can significantly affect the overall security risks of the entire cloud. In recent years, the attacks are specifically designed to co-locate with target virtual machines in the cloud. The virtual machine placement without considering the security risks may put the users, or even the entire cloud, in danger. In this paper, we present a comprehensive approach to quantify the security risk of cloud environments from network, host and VM. Accordingly, we propose a Security-aware Multi-Objective Optimization based virtual machine Placement scheme (SMOOP) to seek a Pareto-optimal solution that reduces the overall security risks of a cloud, while considering workload balance, resource utilization on CPU, memory, disk, and network traffic. New placement strategies are designed and our evaluation results demonstrate their effectiveness. The security of clouds could be improved with affordable overheads. The latest VM allocation policies are further studied and integrated into our designs to defeat the co-residence attacks. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computer Security is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134583443&site=ehost-live
240,Some special minimum -geodetically connected graphs,Songqing Chen,Discrete Applied Mathematics,0166218X,,Jun2011,159,10,1002,11.0,60157345,10.1016/j.dam.2011.03.009,Elsevier B.V.,Article,GEODESICS; GRAPH connectivity; PATHS & cycles in graph theory; ROBUST control; GRAPH theory; MATHEMATICAL analysis,Distance invulnerability; Geodetic connectivity; Minimum size graph; Robust system design; Survivable network,"Abstract: A connected graph is -geodetically connected (-GC) if the removal of less than vertices does not affect the distances (lengths of the shortest paths) between any pair of the remaining vertices. As such graphs have important applications in robust system designs, we are interested in the minimum number of edges required to make a -GC graph of order , and characterizing those minimum -GC graphs. When , minimum -GC graphs are not yet known in general, even the minimum number of edges is not determined. In this paper, we will determine all of the minimum -GC graphs for an infinite set of special pairs that were formerly unknown. To derive our results, we also developed new bounds on . Additionally, we show that -GC graphs with small relative optimality gaps can be easily constructed and expanded with great flexibilities, which gives convenient applications for robust system designs. [Copyright &y& Elsevier] Copyright of Discrete Applied Mathematics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=60157345&site=ehost-live
241,Using adaptively coupled models and high-performance computing for enabling the computability of dust storm forecasting.,Songqing Chen,International Journal of Geographical Information Science,13658816,,Apr2013,27,4,765,20.0,87044562,10.1080/13658816.2012.715650,Taylor & Francis Ltd,Article,"DUST storms; WEATHER forecasting; ATMOSPHERIC models; GEOGRAPHIC information systems; COMPUTING platforms; All Other Professional, Scientific, and Technical Services",applied sciences; atmospheric modelling; computing intensity; Cyber GIS; geospatial platform; nested models; parallel computing; spatiotemporal thinking and computing,"Forecasting dust storms for large geographical areas with high resolution poses great challenges for scientific and computational research. Limitations of computing power and the scalability of parallel systems preclude an immediate solution to such challenges. This article reports our research on using adaptively coupled models to resolve the computational challenges and enable the computability of dust storm forecasting by dividing the large geographical domain into multiple subdomains based on spatiotemporal distributions of the dust storm. A dust storm model (Eta-8bin) performs a quick forecasting with low resolution (22 km) to identify potential hotspots with high dust concentration. A finer model, non-hydrostatic mesoscale model (NMM-dust) performs high-resolution (3 km) forecasting over the much smaller hotspots in parallel to reduce computational requirements and computing time. We also adopted spatiotemporal principles among computing resources and subdomains to optimize parallel systems and improve the performance of high-resolution NMM-dust model. This research enabled the computability of high-resolution, large-area dust storm forecasting using the adaptively coupled execution of the two models Eta-8bin and NMM-dust. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Geographical Information Science is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=87044562&site=ehost-live
242,Utilize cloud computing to support dust storm forecasting.,Songqing Chen,International Journal of Digital Earth,17538947,,Jul2013,6,4,338,18.0,88395673,10.1080/17538947.2012.749949,Taylor & Francis Ltd,Article,"CLOUD computing; DUST storms; HIGH performance computing; PUBLIC health; ENERGY conservation; ELECTRONIC data processing; Data Processing, Hosting, and Related Services; Health and Welfare Funds",Amazon EC2; cloud GIS; CyberGIS; loosely coupled nested model; spatial cloud computing,"The simulations and potential forecasting of dust storms are of significant interest to public health and environment sciences. Dust storms have interannual variabilities and are typical disruptive events. The computing platform for a dust storm forecasting operational system should support a disruptive fashion by scaling up to enable high-resolution forecasting and massive public access when dust storms come and scaling down when no dust storm events occur to save energy and costs. With the capability of providing a large, elastic, and virtualized pool of computational resources, cloud computing becomes a new and advantageous computing paradigm to resolve scientific problems traditionally requiring a large-scale and high-performance cluster. This paper examines the viability for cloud computing to support dust storm forecasting. Through a holistic study by systematically comparing cloud computing using Amazon EC2 to traditional high performance computing (HPC) cluster, we find that cloud computing is emerging as a credible solution for (1) supporting dust storm forecasting in spinning off a large group of computing resources in a few minutes to satisfy the disruptive computing requirements of dust storm forecasting, (2) performing high-resolution dust storm forecasting when required, (3) supporting concurrent computing requirements, (4) supporting real dust storm event forecasting for a large geographic domain by using recent dust storm event in Phoniex, 05 July 2011 as example, and (5) reducing cost by maintaining low computing support when there is no dust storm events while invoking a large amount of computing resource to perform high-resolution forecasting and responding to large amount of concurrent public accesses. [ABSTRACT FROM PUBLISHER] Copyright of International Journal of Digital Earth is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=88395673&site=ehost-live
243,CaptorX: A Class-Adaptive Convolutional Neural Network Reconfiguration Framework.,Xiang Chen,IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems,02780070,,Mar2022,41,3,530,14.0,155458664,10.1109/TCAD.2021.3061520,IEEE,Article,CONVOLUTIONAL neural networks; DEEP learning; MOBILE apps; ENERGY consumption,Adaptation models; CNN visualization; Computational modeling; Convolutional neural network (CNN) reconfiguration; distributed learning; Energy consumption; Load modeling; mobile computing; Task analysis; Training; Visualization,"Nowadays, the evolution of deep learning and cloud service significantly promotes neural network-based mobile applications. Although intelligent and prolific, those applications still lack certain flexibility: for classification tasks, neural networks are generally trained with vast classification targets to cover various utilization contexts. However, only partial classes are practically inferred due to individual mobile user preference and application specificity, which causes unnecessary computation consumption. Thus, we proposed CaptorX—a class-adaptive convolutional neural network (CNN) reconfiguration framework to adaptively prune convolutional filters associated with unneeded classes. CaptorX can reconfigure a pretrained full-class CNN model into class-specific lightweight models based on the visualization analysis of convolutional filters’ exclusive functionality for a single class. These lightweight models can be directly deployed to mobile devices without the retraining cost of traditional pruning-based reconfiguration. Furthermore, we can apply the CaptorX framework into a distributed collaboration setting. With dedicated local training regulation and collaborative aggregation schemes, the class-adaptive models on individual mobile devices can further contribute back to the central full-class model. Experiments on representative CNNs and image classification datasets show that, CaptorX can reduce the CNN computation workload up to 50.22% and save 46.58% energy consumption for varied local devices, meanwhile improving accuracy for their targeted classes with better task focus. With our distributed collaboration paradigm, CaptorX also provides further potential to enhance the central model accuracy, while reducing up to 37.58% communication cost compared to traditional distributed learning methods. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155458664&site=ehost-live
244,DiReCtX: Dynamic Resource-Aware CNN Reconfiguration Framework for Real-Time Mobile Applications.,Xiang Chen,IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems,02780070,,Feb2021,40,2,246,14.0,148281662,10.1109/TCAD.2020.2995813,IEEE,Article,MOBILE apps; CONVOLUTIONAL neural networks,Adaptation models; Computational modeling; Convolutional neural network (CNN); Dynamic scheduling; Integrated circuit modeling; mobile device; model reconfiguration; neuron pruning; Neurons; Optimization; Real-time systems; resource-aware,"Although convolutional neural networks (CNNs) have been widely applied in various cognitive applications, they are still very computationally intensive for resource-constrained mobile systems. To reduce the resource consumption of CNN computation, many optimization works have been proposed for mobile CNN deployment. However, most works are merely targeting CNN model compression from the perspective of parameter size or model structure, ignoring different resource constraints in mobile systems with respect to memory, energy, and real-time requirement. Moreover, previous works take accuracy as their primary consideration, requiring a time-costing retraining process to compensate the inference accuracy loss after compression. To address these issues, we propose DiReCtX—a dynamic resource-aware CNN model reconfiguration framework. DiReCtX is based on a set of accurate CNN profiling models for different resource consumption and inference accuracy estimation. With manageable consumption/accuracy tradeoffs, DiReCtX can reconfigure a CNN model to meet distinct resource constraint types and levels with expected inference performance maintained. To further achieve fast model reconfiguration in real-time, improved CNN model pruning and its corresponding accuracy tuning strategies are also proposed in DiReCtX. The experiments show that the proposed CNN profiling models can achieve 94.6% and 97.1% accuracy for CNN model resource consumption and inference accuracy estimation. Meanwhile, the proposed reconfiguration scheme of DiReCtX can achieve at most 44.44% computation acceleration, 31.69% memory reduction, and 32.39% energy saving, respectively. On field-tests with state-of-the-art smartphones, DiReCtX can adapt CNN models to various resource constraints in mobile application scenarios with optimal real-time performance. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148281662&site=ehost-live
245,HOW CONVOLUTIONAL NEURAL NETWORKS SEE THE WORLD | A SURVEY OF CONVOLUTIONAL NEURAL NETWORK VISUALIZATION METHODS.,Xiang Chen,Mathematical Foundations of Computing,25778838,,May2018,1,2,149,32.0,131505811,10.3934/mfc.2018008,American Institute of Mathematical Sciences,Article,ARTIFICIAL neural networks; VISUALIZATION; PEER-to-peer architecture (Computer networks); DYNAMIC programming; SCALABILITY,58F17; CNN feature; CNN visualization; convolutional neural network; Deep learning; network interpretability; network interpretability.; Primary: 58F15; Secondary: 53C35.,"Nowadays, the Convolutional Neural Networks (CNNs) have achieved impressive performance on many computer vision related tasks, such as object detection, image recognition, image retrieval, etc. These achievements benefit from the CNNs' outstanding capability to learn the input features with deep layers of neuron structures and iterative training process. However, these learned features are hard to identify and interpret from a human vision perspective, causing a lack of understanding of the CNNs' internal working mechanism. To improve the CNN interpretability, the CNN visualization is well utilized as a qualitative analysis method, which translates the internal features into visually perceptible patterns. And many CNN visualization works have been proposed in the literature to interpret the CNN in perspectives of network structure, operation, and semantic concept. In this paper, we expect to provide a comprehensive survey of several rep-resentative CNN visualization methods, including Activation Maximization, Network Inversion, Deconvolutional Neural Networks (DeconvNet), and Network Dissection based visualization. These methods are presented in terms of motivations, algorithms, and experiment results. Based on these visualization methods, we also discuss their practical applications to demonstrate the significance of the CNN interpretability in areas of network design, optimization, security enhancement, etc. [ABSTRACT FROM AUTHOR] Copyright of Mathematical Foundations of Computing is the property of American Institute of Mathematical Sciences and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131505811&site=ehost-live
246,REIN the RobuTS: Robust DNN-Based Image Recognition in Autonomous Driving Systems.,Xiang Chen,IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems,02780070,,Jun2021,40,6,1258,14.0,150448923,10.1109/TCAD.2020.3033498,IEEE,Article,"IMAGE recognition (Computer vision); AUTONOMOUS vehicles; DRIVERLESS cars; TRAFFIC signs & signals; MOTOR vehicle driving; WEATHER; TRAFFIC safety; Electrical Contractors and Other Wiring Installation Contractors; All Other Support Services; Highway, Street, and Bridge Construction",Autonomous driving; Autonomous vehicles; Data models; deep neural network (NN); Neural networks; Rain; robust image recognition; Task analysis; Training,"In recent years, the neural network (NN) has shown its great potential in image recognition tasks of autonomous driving systems, such as traffic sign recognition, pedestrian detection, etc. However, theoretically well-trained NNs usually fail their performance when facing real-world scenarios. For example, adverse real-world conditions, e.g., bad weather and lighting conditions, can introduce different physical variations and cause considerable accuracy degradation. As for now, the generalization capability of NNs is still one of the most critical challenges for the autonomous driving system. To facilitate the robust image recognition tasks, in this work, we build the RobuTS dataset: a comprehensive Robust Traffic Sign Recognition dataset, which includes images with different environmental variations, e.g., rain, fog, darkening, and blurring. Then to enhance the NN’s generalization capability, we propose two generalization-enhanced training schemes: 1) REIN for robust training without data in adverse scenarios and 2) Self-Teaching (ST) for robust training with unlabeled adverse data. The great advantages of such two training schemes are they are data-free (REIN) and label-free (ST), thus effectively reducing the huge human efforts/cost of on-road driving data collection, as well as the expensive manual data annotation. We conduct extensive experiments to validate our methods’ performance on both classification and detection tasks. For classification tasks, our proposed training algorithms could consistently improve model performance by +15%–25% (REIN) and +16%–30% (ST) in all adverse scenarios of our RobuTS datasets. For detection tasks, our ST could also improve the detector’s performance by +10.1 mean average precision (mAP) on Foggy-Cityscapes, outperforming previous state-of-the-art works by +2.2 mAP. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computer-Aided Design of Integrated Circuits & Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150448923&site=ehost-live
247,Customizable Scale-Out Key-Value Stores.,Yue Cheng,IEEE Transactions on Parallel & Distributed Systems,10459219,,Sep2020,31,9,2081,16.0,143316160,10.1109/TPDS.2020.2982640,IEEE,Article,HIGH performance computing; RETAIL stores; APPLICATION stores; SOFTWARE-defined networking; PEER-to-peer architecture (Computer networks); DATA management; NONVOLATILE memory; All Other Miscellaneous Store Retailers (except Tobacco Stores); Commercial and Institutional Building Construction; All other miscellaneous store retailers (except beer and wine-making supplies stores); All other miscellaneous general merchandise stores,application tailored storage; Cloud computing; Distributed databases; Fault tolerance; Fault tolerant systems; HPC KV stores; Key-value stores; Nonvolatile memory; Peer-to-peer computing; scale-out KV stores; Topology,"Enterprise KV stores are often not well suited for HPC applications, and thus cumbersome end-to-end KV design customization is required to meet the needs of modern HPC applications. To this end, in this article we present bespoKV, an adaptive, extensible, and scale-out KV store framework. bespoKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. For the control plane, bespoKVprovides pre-built modules, called controlets, supporting common distributed functionalities (e.g., replication, consistency, and topology) and their various combinations. This decoupling allows bespoKV to take a user-provided single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Such specializations enable innovative uses of KV stores in HPC applications, especially for emerging applications that utilize KV-friendly workloads. We evaluate bespoKV in a local testbed as well as in a public cloud settings. Experiments show that bespoKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes 1.2× to 2.6× better than the state-of-the-art systems. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143316160&site=ehost-live
248,Correlation of rupture dynamics to the nonlinear backscatter response from polymer-shelled ultrasound contrast agents.,Parag Chitnis,IEEE Transactions on Ultrasonics Ferroelectrics & Frequency Control,08853010,,Mar2015,62,3,494,8.0,101560800,10.1109/TUFFC.2014.006828,IEEE,Article,RUPTURES (Structural failure); BACKSCATTERING; STRUCTURAL shells; CONTRAST media; ULTRASONIC imaging; ENCAPSULATION (Catalysis); NONLINEAR oscillations,Acoustics; Backscatter; Microscopy; Polymers; Sociology; Statistics; Ultrasonic imaging,"Polymer-shelled ultrasound contrast agents (UCAs) may expel their encapsulated gas subject to ultrasound- induced shell buckling or rupture. Nonlinear oscillations of this gas bubble can produce a subharmonic component in the ultrasound backscatter. This study investigated the relationship between this gas-release mechanism and shell-thickness? to?radius ratios (STRRs) of polymer-shelled UCAs. Three types of polylactide-shelled UCAs with STRRs of 7.5, 40, and 100 nm/?m were studied. Each UCA population had a nominal mean diameter of 2 ?m. UCAs were subjected to increasing static overpressure ranging from 2 to 330 kPa over a duration of 2 h in a custom-designed test chamber while being imaged using a 200? magnification video microscope at a frame rate of 5 frames/s. Digitized video images were binarized and processed to obtain the cross-sectional area of individual UCAs. Integration of the normalized cross-sectional area over normalized time, defined as buckling factor (Bf), provided a dimensionless parameter for quantifying and comparing the degree of pre-rupture buckling exhibited by the UCAs of different STRRs in response to overpressure. The UCAs with an STRR of 7.5 nm/?m exhibited a distinct shell-buckling phase before shell rupture (Bf < 1), whereas the UCAs with higher STRRs (40 and 100 nm/?m) did not undergo significant prerupture buckling (Bf ? 1). The difference in the overpressure response was correlated with the subharmonic response produced by these UCAs. When excited using 20-MHz ultrasound, individual UCAs (N = 3000) in populations that did not exhibit a buckling phase produced a subharmonic response that was an order of magnitude greater than the UCA population with a prominent pre-rupture buckling phase. These results indicate the mechanism of gas expulsion from these UCAs might be a relevant factor in determining the level of subharmonic response in response to high-frequency ultrasound. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Ultrasonics Ferroelectrics & Frequency Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101560800&site=ehost-live
249,Limited-View and Sparse Photoacoustic Tomography for Neuroimaging with Deep Learning.,Siddhartha Sikdar,Scientific Reports,20452322,,5/22/2020,10,1,1,12.0,143387426,10.1038/s41598-020-65235-2,Springer Nature,Article,BRAIN imaging; DEEP learning; OPTICAL imaging sensors; IMAGE quality analysis; ACOUSTIC imaging,,"Photoacoustic tomography (PAT) is a non-ionizing imaging modality capable of acquiring high contrast and resolution images of optical absorption at depths greater than traditional optical imaging techniques. Practical considerations with instrumentation and geometry limit the number of available acoustic sensors and their ""view"" of the imaging target, which result in image reconstruction artifacts degrading image quality. Iterative reconstruction methods can be used to reduce artifacts but are computationally expensive. In this work, we propose a novel deep learning approach termed pixel-wise deep learning (Pixel-DL) that first employs pixel-wise interpolation governed by the physics of photoacoustic wave propagation and then uses a convolution neural network to reconstruct an image. Simulated photoacoustic data from synthetic, mouse-brain, lung, and fundus vasculature phantoms were used for training and testing. Results demonstrated that Pixel-DL achieved comparable or better performance to iterative methods and consistently outperformed other CNN-based approaches for correcting artifacts. Pixel-DL is a computationally efficient approach that enables for real-time PAT rendering and improved image reconstruction quality for limited-view and sparse PAT. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143387426&site=ehost-live
250,"M3VR—A multi-stage, multi-resolution, and multi-volumes-of-interest volume registration method applied to 3D endovaginal ultrasound.",Parag Chitnis,PLoS ONE,19326203,,11/21/2019,14,11,1,21.0,139787859,10.1371/journal.pone.0224583,Public Library of Science,Article,DIAGNOSTIC ultrasonic imaging; ULTRASONIC imaging; COST functions; THREE-dimensional imaging; DIAGNOSTIC imaging; PELVIC floor; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance,,"Heterogeneity of echo-texture and lack of sharply delineated tissue boundaries in diagnostic ultrasound images make three-dimensional (3D) registration challenging, especially when the volumes to be registered are considerably different due to local changes. We implemented a novel computational method that optimally registers volumetric ultrasound image data containing significant and local anatomical differences. It is A Multi-stage, Multi-resolution, and Multi-volumes-of-interest Volume Registration Method. A single region registration is optimized first for a close initial alignment to avoid convergence to a locally optimal solution. Multiple sub-volumes of interest can then be selected as target alignment regions to achieve confident consistency across the volume. Finally, a multi-resolution rigid registration is performed on these sub-volumes associated with different weights in the cost function. We applied the method on 3D endovaginal ultrasound image data acquired from patients during biopsy procedure of the pelvic floor muscle. Systematic assessment of our proposed method through cross validation demonstrated its accuracy and robustness. The algorithm can also be applied on medical imaging data of other modalities for which the traditional rigid registration methods would fail. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139787859&site=ehost-live
250,"M3VR—A multi-stage, multi-resolution, and multi-volumes-of-interest volume registration method applied to 3D endovaginal ultrasound.",Siddhartha Sikdar,PLoS ONE,19326203,,11/21/2019,14,11,1,21.0,139787859,10.1371/journal.pone.0224583,Public Library of Science,Article,DIAGNOSTIC ultrasonic imaging; ULTRASONIC imaging; COST functions; THREE-dimensional imaging; DIAGNOSTIC imaging; PELVIC floor; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance,,"Heterogeneity of echo-texture and lack of sharply delineated tissue boundaries in diagnostic ultrasound images make three-dimensional (3D) registration challenging, especially when the volumes to be registered are considerably different due to local changes. We implemented a novel computational method that optimally registers volumetric ultrasound image data containing significant and local anatomical differences. It is A Multi-stage, Multi-resolution, and Multi-volumes-of-interest Volume Registration Method. A single region registration is optimized first for a close initial alignment to avoid convergence to a locally optimal solution. Multiple sub-volumes of interest can then be selected as target alignment regions to achieve confident consistency across the volume. Finally, a multi-resolution rigid registration is performed on these sub-volumes associated with different weights in the cost function. We applied the method on 3D endovaginal ultrasound image data acquired from patients during biopsy procedure of the pelvic floor muscle. Systematic assessment of our proposed method through cross validation demonstrated its accuracy and robustness. The algorithm can also be applied on medical imaging data of other modalities for which the traditional rigid registration methods would fail. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139787859&site=ehost-live
250,"M3VR—A multi-stage, multi-resolution, and multi-volumes-of-interest volume registration method applied to 3D endovaginal ultrasound.",Qi Wei,PLoS ONE,19326203,,11/21/2019,14,11,1,21.0,139787859,10.1371/journal.pone.0224583,Public Library of Science,Article,DIAGNOSTIC ultrasonic imaging; ULTRASONIC imaging; COST functions; THREE-dimensional imaging; DIAGNOSTIC imaging; PELVIC floor; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance,,"Heterogeneity of echo-texture and lack of sharply delineated tissue boundaries in diagnostic ultrasound images make three-dimensional (3D) registration challenging, especially when the volumes to be registered are considerably different due to local changes. We implemented a novel computational method that optimally registers volumetric ultrasound image data containing significant and local anatomical differences. It is A Multi-stage, Multi-resolution, and Multi-volumes-of-interest Volume Registration Method. A single region registration is optimized first for a close initial alignment to avoid convergence to a locally optimal solution. Multiple sub-volumes of interest can then be selected as target alignment regions to achieve confident consistency across the volume. Finally, a multi-resolution rigid registration is performed on these sub-volumes associated with different weights in the cost function. We applied the method on 3D endovaginal ultrasound image data acquired from patients during biopsy procedure of the pelvic floor muscle. Systematic assessment of our proposed method through cross validation demonstrated its accuracy and robustness. The algorithm can also be applied on medical imaging data of other modalities for which the traditional rigid registration methods would fail. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139787859&site=ehost-live
256,All (Mayoral) Politics Is Local?,Sanmay Das,Journal of Politics,00223816,,Apr2022,84,2,1021,14.0,156554546,10.1086/716945,University of Chicago Press,Article,"POLITICAL attitudes; MAYORS; VOTERS; GOVERNORS; POLITICAL oratory; RHETORIC; Executive Offices; Other local, municipal and regional public administration; Marketing Research and Public Opinion Polling; UNITED States politics & government; TWITTER (Web resource)",mayors; nationalization; rhetoric; Twitter,"One of the defining characteristics of modern politics in the United States is the increasing nationalization of elite- and voter-level behavior. Relying on measures of electoral vote shares, previous research has found evidence indicating a significant amount of state-level nationalization. Using an alternative source of data—the political rhetoric used by mayors, state governors, and members of Congress on Twitter—we examine and compare the amount of between-office nationalization throughout the federal system. We find that gubernatorial rhetoric closely matches that of members of Congress, but that there are substantial differences in the topics and content of mayoral speech. These results suggest that, on average, American mayors have largely remained focused on their local mandate. More broadly, our findings suggest a limit to which American politics has become nationalized—in some cases, all politics remains local. [ABSTRACT FROM AUTHOR] Copyright of Journal of Politics is the property of The Southern Political Science Association and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156554546&site=ehost-live
257,Bayesian co-clustering.,Carlotta Domeniconi,WIREs: Computational Statistics,19395108,,Sep2015,7,5,347,10.0,108866407,10.1002/wics.1359,Wiley-Blackwell,Article,PROTEINS; BAYESIAN analysis; MOLECULES; MICROBIOLOGY; TEXT mining (Information retrieval); MATHEMATICAL models,Bayesian data mining; Bayesian nonparametrics; co‐clustering; co-clustering; ensemble methods; Markov chain Monte Carlo,"Co-clustering means simultaneously identifying natural clusters in different kinds of objects. Examples include simultaneously clustering customers and products for a recommender application; simultaneously clustering proteins and molecules in microbiology; or simultaneously clustering documents and words in a text mining application. Important insights into a problem can be gained by understanding the interactions between clusters for the different kinds of objects. This paper considers Bayesian models for co-clustering. The Bayesian approach begins by developing a model for the data generating process, and inverting that model through Bayesian inference to infer cluster membership, learn characteristics of the clusters, and fill in missing observations. We consider a basic Bayesian clustering model and several extensions to the model. Experimental evaluations and comparisons among the clustering methods are presented. WIREs Comput Stat 2015, 7:347-356. doi: 10.1002/wics.1359 For further resources related to this article, please visit the . [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108866407&site=ehost-live
258,CMAL: Cost-Effective Multi-Label Active Learning by Querying Subexamples.,Carlotta Domeniconi,IEEE Transactions on Knowledge & Data Engineering,10414347,,May2022,34,5,2091,15.0,156273244,10.1109/TKDE.2020.3003899,IEEE,Article,ACTIVE learning; ANNOTATIONS,Annotations; Correlation; label correlation; label sparsity; Measurement uncertainty; multi-instance learning; Multi-label active learning; representative; Semantics; Training; Uncertainty,"Multi-label active learning (MAL) aims to learn an accurate multi-label classifier by selecting which examples (or example-label pairs) will be annotated and reducing query effort. MAL is a more complicated and expensive process than single-label active learning, due to one example can be associated with a set of non-exclusive labels and the annotator has to scrutinize the whole example and label space to provide correct annotations. Instead of scrutinizing the whole example for annotation, we may just examine some of its subexamples with respect to a label for annotation. In this way, we can not only save the annotation cost but also speedup the annotation process. Given this observation, we introduce CMAL, a two-stage Cost-effective MAL strategy (CMAL) by querying subexamples. CMAL first selects the most informative example-label pairs by leveraging uncertainty, label correlation and label space sparsity. Specifically, the uncertainty of a label to an example can be reduced if its correlated labels already annotated to the example, and its uncertainty can be reduced also if more examples annotated to this label. Next, CMAL greedily queries the most probable positive subexample-label pairs of the selected example-label pair. In addition, we propose rCMAL to account for the representative of examples to more reliably select example-label pairs in the first stage. Extensive experiments on multi-label datasets from diverse domains show that our proposed CMAL and rCMAL can better save the query cost than state-of-the-art MAL methods. The contribution of leveraging label correlation, label sparsity, and representative for saving cost is also confirmed. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156273244&site=ehost-live
259,"Cooperative driver pathway discovery via fusion of multi-relational data of genes, miRNAs and pathways.",Carlotta Domeniconi,Briefings in Bioinformatics,14675463,,Mar2021,22,2,1984,16.0,149507185,10.1093/bib/bbz167,Oxford University Press / USA,Article,GENES; RECEIVER operating characteristic curves; MULTISENSOR data fusion; MICRORNA; GENE regulatory networks; ENDOMETRIAL cancer,biological network; cancer gene; cooperative driver pathway; data fusion; microRNA,"Discovering driver pathways is an essential step to uncover the molecular mechanism underlying cancer and to explore precise treatments for cancer patients. However, due to the difficulties of mapping genes to pathways and the limited knowledge about pathway interactions, most previous work focus on identifying individual pathways. In practice, two (or even more) pathways interplay and often cooperatively trigger cancer. In this study, we proposed a new approach called CDPathway to discover cooperative driver pathways. First, CDPathway introduces a driver impact quantification function to quantify the driver weight of each gene. CDPathway assumes that genes with larger weights contribute more to the occurrence of the target disease and identifies them as candidate driver genes. Next, it constructs a heterogeneous network composed of genes, miRNAs and pathways nodes based on the known intra(inter)-relations between them and assigns the quantified driver weights to gene–pathway and gene–miRNA relational edges. To transfer driver impacts of genes to pathway interaction pairs, CDPathway collaboratively factorizes the weighted adjacency matrices of the heterogeneous network to explore the latent relations between genes, miRNAs and pathways. After this, it reconstructs the pathway interaction network and identifies the pathway pairs with maximal interactive and driver weights as cooperative driver pathways. Experimental results on the breast, uterine corpus endometrial carcinoma and ovarian cancer data from The Cancer Genome Atlas show that CDPathway can effectively identify candidate driver genes [area under the receiver operating characteristic curve (AUROC) of |$\geq $| 0.9] and reconstruct the pathway interaction network (AUROC of>0.9), and it uncovers much more known (potential) driver genes than other competitive methods. In addition, CDPathway identifies 150% more driver pathways and 60% more potential cooperative driver pathways than the competing methods. The code of CDPathway is available at http://mlda.swu.edu.cn/codes.php?name=CDPathway. [ABSTRACT FROM AUTHOR] Copyright of Briefings in Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149507185&site=ehost-live
260,Differentiating isoform functions with collaborative matrix factorization.,Carlotta Domeniconi,Bioinformatics,13674803,,3/15/2020,36,6,1864,8.0,142563680,10.1093/bioinformatics/btz847,Oxford University Press / USA,Article,"MATRIX decomposition; WEB servers; RECEIVER operating characteristic curves; MATRIX functions; LOW-rank matrices; GENE ontology; Data Processing, Hosting, and Related Services",,"Motivation Isoforms are alternatively spliced mRNAs of genes. They can be translated into different functional proteoforms, and thus greatly increase the functional diversity of protein variants (or proteoforms). Differentiating the functions of isoforms (or proteoforms) helps understanding the underlying pathology of various complex diseases at a deeper granularity. Since existing functional genomic databases uniformly record the annotations at the gene-level, and rarely record the annotations at the isoform-level, differentiating isoform functions is more challenging than the traditional gene-level function prediction. Results Several approaches have been proposed to differentiate the functions of isoforms. They generally follow the multi-instance learning paradigm by viewing each gene as a bag and the spliced isoforms as its instances, and push functions of bags onto instances. These approaches implicitly assume the collected annotations of genes are complete and only integrate multiple RNA-seq datasets. As such, they have compromised performance. We propose a data integrative solution (called DisoFun) to D ifferentiate iso form Fun ctions with collaborative matrix factorization. DisoFun assumes the functional annotations of genes are aggregated from those of key isoforms. It collaboratively factorizes the isoform data matrix and gene-term data matrix (storing Gene Ontology annotations of genes) into low-rank matrices to simultaneously explore the latent key isoforms, and achieve function prediction by aggregating predictions to their originating genes. In addition, it leverages the PPI network and Gene Ontology structure to further coordinate the matrix factorization. Extensive experimental results show that DisoFun improves the area under the receiver operating characteristic curve and area under the precision-recall curve of existing solutions by at least 7.7 and 28.9%, respectively. We further investigate DisoFun on four exemplar genes (LMNA, ADAM15, BCL2L1 and CFLAR) with known functions at the isoform-level, and observed that DisoFun can differentiate functions of their isoforms with 90.5% accuracy. Availability and implementation The code of DisoFun is available at mlda.swu.edu.cn/codes.php?name=DisoFun. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142563680&site=ehost-live
261,DMIL-IsoFun: predicting isoform function using deep multi-instance learning.,Carlotta Domeniconi,Bioinformatics,13674803,,12/15/2021,37,24,4818,8.0,154328735,10.1093/bioinformatics/btab532,Oxford University Press / USA,Article,DEEP learning; CONVOLUTIONAL neural networks; HUMAN genes,,"Motivation Alternative splicing creates the considerable proteomic diversity and complexity on relatively limited genome. Proteoforms translated from alternatively spliced isoforms of a gene actually execute the biological functions of this gene, which reflect the functional knowledge of genes at a finer granular level. Recently, some computational approaches have been proposed to differentiate isoform functions using sequence and expression data. However, their performance is far from being desirable, mainly due to the imbalance and lack of annotations at isoform-level, and the difficulty of modeling gene–isoform relations. Result We propose a deep multi-instance learning-based framework (DMIL-IsoFun) to differentiate the functions of isoforms. DMIL-IsoFun firstly introduces a multi-instance learning convolution neural network trained with isoform sequences and gene-level annotations to extract the feature vectors and initialize the annotations of isoforms, and then uses a class-imbalance Graph Convolution Network to refine the annotations of individual isoforms based on the isoform co-expression network and extracted features. Extensive experimental results show that DMIL-IsoFun improves the S min and F max of state-of-the-art solutions by at least 29.6% and 40.8%. The effectiveness of DMIL-IsoFun is further confirmed on a testbed of human multiple-isoform genes, and maize isoforms related with photosynthesis. Availability and implementation The code and data are available at http://www.sdu-idea.cn/codes.php?name=DMIL-Isofun. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154328735&site=ehost-live
262,Hub-based subspace clustering.,Carlotta Domeniconi,Neurocomputing,09252312,,Nov2020,413,,193,17.0,146412823,10.1016/j.neucom.2020.06.098,Elsevier B.V.,Article,NETWORK hubs; DATA mining; ALGORITHMS,Graph-based meta-features; Hubness; Selective sampling; Subspace clustering,"Data often exists in subspaces embedded within a high-dimensional space. Subspace clustering seeks to group data according to the dimensions relevant to each subspace. This requires the estimation of subspaces as well as the clustering of data. Subspace clustering becomes increasingly challenging in high dimensional spaces due to the curse of dimensionality which affects reliable estimations of distances and density. Recently, another aspect of high-dimensional spaces has been observed, known as the hubness phenomenon, whereby few data points appear frequently as nearest neighbors of the rest of the data. The distribution of neighbor occurrences becomes skewed with increasing intrinsic dimensionality of the data, and few points with high neighbor occurrences emerge as hubs. Hubs exhibit useful geometric properties and have been leveraged for clustering data in the full-dimensional space. In this paper, we study hubs in the context of subspace clustering. We present new characterizations of hubs in relation to subspaces, and design graph-based meta-features to identify a subset of hubs which are well fit to serve as seeds for the discovery of local latent subspaces and clusters. We propose and evaluate a hubness-driven algorithm to find subspace clusters, and show that our approach is superior to the baselines, and is competitive against state-of-the-art subspace clustering methods. We also identify the data characteristics that make hubs suitable for subspace clustering. Such characterization gives valuable guidelines to data mining practitioners. [ABSTRACT FROM AUTHOR] Copyright of Neurocomputing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146412823&site=ehost-live
263,Integrating multiple networks for protein function prediction.,Carlotta Domeniconi,BMC Systems Biology,17520509,,2015,9,1,1,11.0,101990706,10.1186/1752-0509-9-S1-S3,BioMed Central,Article,"PROTEINS; SYSTEMS biology; COMPUTATIONAL biology; METABOLOMICS; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences",,"Background: High throughput techniques produce multiple functional association networks. Integrating these networks can enhance the accuracy of protein function prediction. Many algorithms have been introduced to generate a composite network, which is obtained as a weighted sum of individual networks. The weight assigned to an individual network reflects its benefit towards the protein functional annotation inference. A classifier is then trained on the composite network for predicting protein functions. However, since these techniques model the optimization of the composite network and the prediction tasks as separate objectives, the resulting composite network is not necessarily optimal for the follow-up protein function prediction. Results: We address this issue by modeling the optimization of the composite network and the prediction problems within a unified objective function. In particular, we use a kernel target alignment technique and the loss function of a network based classifier to jointly adjust the weights assigned to the individual networks. We show that the proposed method, called MNet, can achieve a performance that is superior (with respect to different evaluation criteria) to related techniques using the multiple networks of four example species (yeast, human, mouse, and fly) annotated with thousands (or hundreds) of GO terms. Conclusion: MNet can effectively integrate multiple networks for protein function prediction and is robust to the input parameters. Supplementary data is available at https://sites.google.com/site/guoxian85/home/mnet. The Matlab code of MNet is available upon request. [ABSTRACT FROM AUTHOR] Copyright of BMC Systems Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101990706&site=ehost-live
264,Integrating multiple networks for protein function prediction.,Carlotta Domeniconi,BMC Systems Biology,17520509,,2015 Supplement,9,,1,11.0,103735921,10.1186/1752-0509-9-S1-S3,BioMed Central,Article,PROTEIN research; BIOMOLECULES; KERNEL (Mathematics); KERNEL functions; MATLAB (Computer software),,"Background: High throughput techniques produce multiple functional association networks. Integrating these networks can enhance the accuracy of protein function prediction. Many algorithms have been introduced to generate a composite network, which is obtained as a weighted sum of individual networks. The weight assigned to an individual network reflects its benefit towards the protein functional annotation inference. A classifier is then trained on the composite network for predicting protein functions. However, since these techniques model the optimization of the composite network and the prediction tasks as separate objectives, the resulting composite network is not necessarily optimal for the follow-up protein function prediction. Results: We address this issue by modeling the optimization of the composite network and the prediction problems within a unified objective function. In particular, we use a kernel target alignment technique and the loss function of a network based classifier to jointly adjust the weights assigned to the individual networks. We show that the proposed method, called MNet, can achieve a performance that is superior (with respect to different evaluation criteria) to related techniques using the multiple networks of four example species (yeast, human, mouse, and fly) annotated with thousands (or hundreds) of GO terms. Conclusion: MNet can effectively integrate multiple networks for protein function prediction and is robust to the input parameters. Supplementary data is available at https://sites.google.com/site/guoxian85/home/mnet. The Matlab code of MNet is available upon request. [ABSTRACT FROM AUTHOR] Copyright of BMC Systems Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103735921&site=ehost-live
265,Isoform function prediction based on bi-random walks on a heterogeneous network.,Carlotta Domeniconi,Bioinformatics,13674803,,1/1/2020,36,1,303,8.0,141288369,10.1093/bioinformatics/btz535,Oxford University Press / USA,Article,CANCER; GENE ontology; SPECIES diversity; HUMAN abnormalities; RNA splicing,,"Motivation Alternative splicing contributes to the functional diversity of protein species and the proteoforms translated from alternatively spliced isoforms of a gene actually execute the biological functions. Computationally predicting the functions of genes has been studied for decades. However, how to distinguish the functional annotations of isoforms, whose annotations are essential for understanding developmental abnormalities and cancers, is rarely explored. The main bottleneck is that functional annotations of isoforms are generally unavailable and functional genomic databases universally store the functional annotations at the gene level. Results We propose IsoFun to accomplish Isoform Function prediction based on bi-random walks on a heterogeneous network. IsoFun firstly constructs an isoform functional association network based on the expression profiles of isoforms derived from multiple RNA-seq datasets. Next, IsoFun uses the available Gene Ontology annotations of genes, gene–gene interactions and the relations between genes and isoforms to construct a heterogeneous network. After this, IsoFun performs a tailored bi-random walk on the heterogeneous network to predict the association between GO terms and isoforms, thus accomplishing the prediction of GO annotations of isoforms. Experimental results show that IsoFun significantly outperforms the state-of-the-art algorithms and improves the area under the receiver-operating curve (AUROC) and the area under the precision-recall curve (AUPRC) by 17% and 44% at the gene-level, respectively. We further validated the performance of IsoFun on the genes ADAM15 and BCL2L1. IsoFun accurately differentiates the functions of respective isoforms of these two genes. Availability and implementation The code of IsoFun is available at http://mlda.swu.edu.cn/codes.php? name=IsoFun. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141288369&site=ehost-live
266,Isoform function prediction by Gene Ontology embedding.,Carlotta Domeniconi,Bioinformatics,13674803,,10/1/2022,38,19,4581,8.0,159436881,10.1093/bioinformatics/btac576,Oxford University Press / USA,Article,,,"Motivation High-resolution annotation of gene functions is a central task in functional genomics. Multiple proteoforms translated from alternatively spliced isoforms from a single gene are actual function performers and greatly increase the functional diversity. The specific functions of different isoforms can decipher the molecular basis of various complex diseases at a finer granularity. Multi-instance learning (MIL)-based solutions have been developed to distribute gene(bag)-level Gene Ontology (GO) annotations to isoforms(instances), but they simply presume that a particular annotation of the gene is responsible by only one isoform, neglect the hierarchical structures and semantics of massive GO terms (labels), or can only handle dozens of terms. Results We propose an efficacy approach IsofunGO to differentiate massive functions of isoforms by GO embedding. Particularly, IsofunGO first introduces an attributed hierarchical network to model massive GO terms, and a GO network embedding strategy to learn compact representations of GO terms and project GO annotations of genes into compressed ones, this strategy not only explores and preserves hierarchy between GO terms but also greatly reduces the prediction load. Next, it develops an attention-based MIL network to fuse genomics and transcriptomics data of isoforms and predict isoform functions by referring to compressed annotations. Extensive experiments on benchmark datasets demonstrate the efficacy of IsofunGO. Both the GO embedding and attention mechanism can boost the performance and interpretability. Availabilityand implementation The code of IsofunGO is available at http://www.sdu-idea.cn/codes.php?name=IsofunGO. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159436881&site=ehost-live
267,Large Margin Nearest Neighbor Classifiers.,Carlotta Domeniconi,IEEE Transactions on Neural Networks,10459227,,Jul2005,16,4,899,11.0,17658536,10.1109/TNN.2005.849821,IEEE,Article,PROBABILITY theory; ESTIMATES; MATHEMATICS; COMPUTER systems; TECHNOLOGY; STATISTICAL correlation; Computer systems design and related services (except video game design and development); Computer Systems Design Services,Feature relevance; margin; nearest neighbor classification; support vector machines (SVMs),"The nearest neighbor technique is a simple and appealing approach to addressing classification problems. It relies on the assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. Severe bias can be introduced under these conditions when using the nearest neighbor rule. The employment of a locally adaptive metric becomes crucial in order to keep class conditional probabilities close to uniform, thereby minimizing the bias of estimates. We propose a technique that computes a locally flexible metric by means of support vector machines (SVMs). The decision function constructed by SVMs is used to determine the most discriminant direction in a neighborhood around the query. Such a direction provides a local feature weighting scheme. We formally show that our method increases the margin in the weighted space where classification takes place. Moreover, our method has the important advantage of online computational efficiency over competing locally adaptive techniques for nearest neighbor classification. We demonstrate the efficacy of our method using both real and simulated data. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Neural Networks is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=17658536&site=ehost-live
268,Matrix factorization-based data fusion for the prediction of lncRNA--disease associations.,Carlotta Domeniconi,Bioinformatics,13674803,,5/1/2018,34,9,1529,9.0,129490337,10.1093/bioinformatics/btx794,Oxford University Press / USA,Article,RNA; MATRICES; DIAGNOSIS; BIOMEDICAL engineering; VECTOR analysis,,"Motivation: Long non-coding RNAs (lncRNAs) play crucial roles in complex disease diagnosis, prognosis, prevention and treatment, but only a small portion of lncRNA--disease associations have been experimentally verified. Various computational models have been proposed to identify lncRNA--disease associations by integrating heterogeneous data sources. However, existing models generally ignore the intrinsic structure of data sources or treat them as equally relevant, while they may not be. Results: To accurately identify lncRNA--disease associations, we propose a Matrix Factorization based LncRNA--Disease Association prediction model (MFLDA in short). MFLDA decomposes data matrices of heterogeneous data sources into low-rank matrices via matrix tri-factorization to explore and exploit their intrinsic and shared structure. MFLDA can select and integrate the data sources by assigning different weights to them. An iterative solution is further introduced to simultaneously optimize the weights and low-rank matrices. Next, MFLDA uses the optimized low-rank matrices to reconstruct the lncRNA--disease association matrix and thus to identify potential associations. In 5-fold cross validation experiments to identify verified lncRNA--disease associations, MFLDA achieves an area under the receiver operating characteristic curve (AUC) of 0.7408, at least 3% higher than those given by state-of-the-art data fusion based computational models. An empirical study on identifying masked lncRNA--disease associations again shows that MFLDA can identify potential associations more accurately than competing models. A case study on identifying lncRNAs associated with breast, lung and stomach cancers show that 38 out of 45 (84%) associations predicted by MFLDA are supported by recent biomedical literature and further proves the capability of MFLDA in identifying novel lncRNA--disease associations. MFLDA is a general data fusion framework, and as such it can be adopted to predict associations between other biological entities. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129490337&site=ehost-live
269,Multi-label zero-shot learning with graph convolutional networks.,Carlotta Domeniconi,Neural Networks,08936080,,Dec2020,132,,333,9.0,146953319,10.1016/j.neunet.2020.09.010,Elsevier B.V.,Article,LABELS; GRAPH labelings; KNOWLEDGE transfer; LEARNING; GLOBAL production networks; LEARNING goals; Commercial Printing (except Screen and Books); Other printing; Packaging and Labeling Services,Graph Convolutional Networks; Label correlations; Multi-label classification; Zero-shot learning,"The goal of zero-shot learning (ZSL) is to build a classifier that recognizes novel categories with no corresponding annotated training data. The typical routine is to transfer knowledge from seen classes to unseen ones by learning a visual-semantic embedding. Existing multi-label zero-shot learning approaches either ignore correlations among labels, suffer from large label combinations, or learn the embedding using only local or global visual features. In this paper, we propose a Graph Convolution Networks based Multi-label Zero-Shot Learning model, abbreviated as MZSL-GCN. Our model first constructs a label relation graph using label co-occurrences and compensates the absence of unseen labels in the training phase by semantic similarity. It then takes the graph and the word embedding of each seen (unseen) label as inputs to the GCN to learn the label semantic embedding, and to obtain a set of inter-dependent object classifiers. MZSL-GCN simultaneously trains another attention network to learn compatible local and global visual features of objects with respect to the classifiers, and thus makes the whole network end-to-end trainable. In addition, the use of unlabeled training data can reduce the bias toward seen labels and boost the generalization ability. Experimental results on benchmark datasets show that our MZSL-GCN competes with state-of-the-art approaches. [ABSTRACT FROM AUTHOR] Copyright of Neural Networks is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146953319&site=ehost-live
270,Multiview Multi-Instance Multilabel Active Learning.,Carlotta Domeniconi,IEEE Transactions on Neural Networks & Learning Systems,2162237X,,Sep2022,33,9,4311,11.0,158869777,10.1109/TNNLS.2021.3056436,IEEE,Article,PREDICTION models,Active learning; Biological systems; commonality and individuality; Compounds; Correlation; Drugs; Learning systems; multi-instance multilabel (MIML) learning; multiview learning; Semantics; Uncertainty,"Multiview multi-instance multilabel learning (M3L) is a framework for modeling complex objects. In this framework, each object (or bag) contains one or more instances, is represented with different feature views, and simultaneously annotated with a set of nonexclusive semantic labels. Given the multiplicity of the studied objects, traditional M3L methods generally demand a large number of labeled bags to train a predictive model to annotate bags (or instances) with semantic labels. However, annotating sufficient bags is very expensive and often impractical. In this article, we present an active learning-based M3L approach (M3AL) to reduce the labeling costs of bags and to improve the performance as much as possible. M3AL first adapts the multiview self-representation learning to evacuate the shared and individual information of bags and to learn the shared/individual similarities between bags across/within views. Next, to avoid scrutinizing all the possible labels, M3AL introduces a new query strategy that leverages the shared and individual information, and the diverse instance distribution of bags across views, to select the most informative bag-label pair for the query. Experimental studies on benchmark data sets show that M3AL can significantly reduce the query costs while achieving a better performance than other related competitive methods at the same cost. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Neural Networks & Learning Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158869777&site=ehost-live
271,Parallel boosted clustering.,Carlotta Domeniconi,Neurocomputing,09252312,,Jul2019,351,,87,14.0,136417001,10.1016/j.neucom.2019.04.003,Elsevier B.V.,Article,SYSTEMS on a chip; PARALLEL algorithms; GRID cells; DATA distribution; DATA structures; SCALABILITY; Semiconductor and Related Device Manufacturing,Boosting; Clustering; Parallel computation; Stochastic optimization,"Scalability of clustering algorithms is a critical issue in real world clustering applications. Usually, data sampling and parallelization are two common ways to address the scalability issue. Despite their wide utilization in a number of clustering algorithms, they suffer from several major drawbacks. For example, most data sampling can often lead to biased solutions due to its inability in accurately capturing the distribution of the entire data set. On the other hand, the performance of parallelization highly depends on the original clustering routines which are not parallel algorithms in nature, such that customizing each algorithm to be parallel may hurt the clustering performance. To alleviate these problems, we propose a general two-step framework for scalable clustering in this work, where the first step is to obtain skeleton structure of data and the second step is to obtain the final clustering. Concretely, data are first partitioned and located across a two-dimensional grid, and then local clustering algorithms are iteratively applied on the cells of the grid, each providing a set of intermediate core points. These core points represent the dense or central regions of data, which can be centers, modes and means for centroid-based, density-based and probability-based clustering, respectively. Finally, these core points are further used to obtain the final clustering. The proposed framework enjoys several benefits: (1) the local clustering on partitioned cells are conducted in parallel and thus can lead to high speed-up; (2) the clustering on the representative core points can be more robust; (3) the framework can be easily applied to other basic clustering methods and thus achieves a general scalable solution. Theoretical analysis is provided and extensive experimental results have demonstrated the effectiveness and efficiency of the proposed framework. [ABSTRACT FROM AUTHOR] Copyright of Neurocomputing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136417001&site=ehost-live
272,Predicting protein function via downward random walks on a gene ontology.,Carlotta Domeniconi,BMC Bioinformatics,14712105,,Aug2015,16,1,1,13.0,109221121,10.1186/s12859-015-0713-y,BioMed Central,Article,PROTEIN analysis; RANDOM walks; GENE ontology; HUMAN proteins; PLANT protein analysis; PROTEOMICS; GENOMICS; YEAST; Other specialty-line food merchant wholesalers; All other food manufacturing; All Other Miscellaneous Food Manufacturing,Downward random walk; Function prediction; Gene ontology; Partially annotated proteins,"Background: High-throughput bio-techniques accumulate ever-increasing amount of genomic and proteomic data. These data are far from being functionally characterized, despite the advances in gene (or gene's product proteins) functional annotations. Due to experimental techniques and to the research bias in biology, the regularly updated functional annotation databases, i.e., the Gene Ontology (GO), are far from being complete. Given the importance of protein functions for biological studies and drug design, proteins should be more comprehensively and precisely annotated. Results: We proposed downward Random Walks (dRW) to predict missing (or new) functions of partially annotated proteins. Particularly, we apply downward random walks with restart on the GO directed acyclic graph, along with the available functions of a protein, to estimate the probability of missing functions. To further boost the prediction accuracy, we extend dRW to dRW-kNN. dRW-kNN computes the semantic similarity between proteins based on the functional annotations of proteins; it then predicts functions based on the functions estimated by dRW, together with the functions associated with the k nearest proteins. Our proposed models can predict two kinds of missing functions: (i) the ones that are missing for a protein but associated with other proteins of interest; (ii) the ones that are not available for any protein of interest, but exist in the GO hierarchy. Experimental results on the proteins of Yeast and Human show that dRW and dRW-kNN can replenish functions more accurately than other related approaches, especially for sparse functions associated with no more than 10 proteins. Conclusion: The empirical study shows that the semantic similarity between GO terms and the ontology hierarchy play important roles in predicting protein function. The proposed dRW and dRW-kNN can serve as tools for replenishing functions of partially annotated proteins. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109221121&site=ehost-live
273,Predicting protein functions using incomplete hierarchical labels.,Carlotta Domeniconi,BMC Bioinformatics,14712105,,2015,16,1,171,21.0,101019327,10.1186/s12859-014-0430-y,BioMed Central,Article,"BIOINFORMATICS; GENE ontology; EMPIRICAL research; BIOCHEMICAL models; BIOCHEMICAL genetics; Research and Development in Biotechnology; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Combined similarity; Function prediction; Gene ontology; Incomplete hierarchical labels,"Background Protein function prediction is to assign biological or biochemical functions to proteins, and it is a challenging computational problem characterized by several factors: (1) the number of function labels (annotations) is large; (2) a protein may be associated with multiple labels; (3) the function labels are structured in a hierarchy; and (4) the labels are incomplete. Current predictive models often assume that the labels of the labeled proteins are complete, i.e. no label is missing. But in real scenarios, we may be aware of only some hierarchical labels of a protein, and we may not know whether additional ones are actually present. The scenario of incomplete hierarchical labels, a challenging and practical problem, is seldom studied in protein function prediction. Results In this paper, we propose an algorithm to Predict protein functions using Incomplete hierarchical LabeLs (PILL in short). PILL takes into account the hierarchical and the flat taxonomy similarity between function labels, and defines a Combined Similarity (ComSim) to measure the correlation between labels. PILL estimates the missing labels for a protein based on ComSim and the known labels of the protein, and uses a regularization to exploit the interactions between proteins for function prediction. PILL is shown to outperform other related techniques in replenishing the missing labels and in predicting the functions of completely unlabeled proteins on publicly available PPI datasets annotated with MIPS Functional Catalogue and Gene Ontology labels. Conclusion The empirical study shows that it is important to consider the incomplete annotation for protein function prediction. The proposed method (PILL) can serve as a valuable tool for protein function prediction using incomplete labels. The Matlab code of PILL is available upon request. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101019327&site=ehost-live
274,Predicting protein functions using incomplete hierarchical labels.,Carlotta Domeniconi,BMC Bioinformatics,14712105,,2015,16,1,1,12.0,102597410,10.1186/s12859-014-0430-y,BioMed Central,Article,PROTEIN research; PREDICTION models; ALGORITHMS; TAXONOMY; MATHEMATICAL regularization; MATLAB (Computer software),Combined similarity; Function prediction; Gene ontology; Incomplete hierarchical labels,"Background: Protein function prediction is to assign biological or biochemical functions to proteins, and it is a challenging computational problem characterized by several factors: (1) the number of function labels (annotations) is large; (2) a protein may be associated with multiple labels; (3) the function labels are structured in a hierarchy; and (4) the labels are incomplete. Current predictive models often assume that the labels of the labeled proteins are complete, i.e. no label is missing. But in real scenarios, we may be aware of only some hierarchical labels of a protein, and we may not know whether additional ones are actually present. The scenario of incomplete hierarchical labels, a challenging and practical problem, is seldom studied in protein function prediction. Results: In this paper, we propose an algorithm to Predict protein functions using Incomplete hierarchical LabeLs (PILL in short). PILL takes into account the hierarchical and the flat taxonomy similarity between function labels, and defines a Combined Similarity (ComSim) to measure the correlation between labels. PILL estimates the missing labels for a protein based on ComSim and the known labels of the protein, and uses a regularization to exploit the interactions between proteins for function prediction. PILL is shown to outperform other related techniques in replenishing the missing labels and in predicting the functions of completely unlabeled proteins on publicly available PPI datasets annotated with MIPS Functional Catalogue and Gene Ontology labels. Conclusion: The empirical study shows that it is important to consider the incomplete annotation for protein function prediction. The proposed method (PILL) can serve as a valuable tool for protein function prediction using incomplete labels. The Matlab code of PILL is available upon request. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=102597410&site=ehost-live
275,Reducing Ensembles of Protein Tertiary Structures Generated De Novo via Clustering.,Ahmed Bin Zaman,Molecules,14203049,,May2020,25,9,2228,1.0,143318659,10.3390/molecules25092228,MDPI,Article,TERTIARY structure; SOURCE code; SPACETIME; FORECASTING; QUALITY control,clustering; decoy ensemble; protein structure prediction; reduction; tertiary structure,"Controlling the quality of tertiary structures computed for a protein molecule remains a central challenge in de-novo protein structure prediction. The rule of thumb is to generate as many structures as can be afforded, effectively acknowledging that having more structures increases the likelihood that some will reside near the sought biologically-active structure. A major drawback with this approach is that computing a large number of structures imposes time and space costs. In this paper, we propose a novel clustering-based approach which we demonstrate to significantly reduce an ensemble of generated structures without sacrificing quality. Evaluations are related on both benchmark and CASP target proteins. Structure ensembles subjected to the proposed approach and the source code of the proposed approach are publicly-available at the links provided in Section 1. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143318659&site=ehost-live
275,Reducing Ensembles of Protein Tertiary Structures Generated De Novo via Clustering.,Carlotta Domeniconi,Molecules,14203049,,May2020,25,9,2228,1.0,143318659,10.3390/molecules25092228,MDPI,Article,TERTIARY structure; SOURCE code; SPACETIME; FORECASTING; QUALITY control,clustering; decoy ensemble; protein structure prediction; reduction; tertiary structure,"Controlling the quality of tertiary structures computed for a protein molecule remains a central challenge in de-novo protein structure prediction. The rule of thumb is to generate as many structures as can be afforded, effectively acknowledging that having more structures increases the likelihood that some will reside near the sought biologically-active structure. A major drawback with this approach is that computing a large number of structures imposes time and space costs. In this paper, we propose a novel clustering-based approach which we demonstrate to significantly reduce an ensemble of generated structures without sacrificing quality. Evaluations are related on both benchmark and CASP target proteins. Structure ensembles subjected to the proposed approach and the source code of the proposed approach are publicly-available at the links provided in Section 1. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143318659&site=ehost-live
275,Reducing Ensembles of Protein Tertiary Structures Generated De Novo via Clustering.,Amarda Shehu,Molecules,14203049,,May2020,25,9,2228,1.0,143318659,10.3390/molecules25092228,MDPI,Article,TERTIARY structure; SOURCE code; SPACETIME; FORECASTING; QUALITY control,clustering; decoy ensemble; protein structure prediction; reduction; tertiary structure,"Controlling the quality of tertiary structures computed for a protein molecule remains a central challenge in de-novo protein structure prediction. The rule of thumb is to generate as many structures as can be afforded, effectively acknowledging that having more structures increases the likelihood that some will reside near the sought biologically-active structure. A major drawback with this approach is that computing a large number of structures imposes time and space costs. In this paper, we propose a novel clustering-based approach which we demonstrate to significantly reduce an ensemble of generated structures without sacrificing quality. Evaluations are related on both benchmark and CASP target proteins. Structure ensembles subjected to the proposed approach and the source code of the proposed approach are publicly-available at the links provided in Section 1. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143318659&site=ehost-live
276,Theoretical and Empirical Analysis of a Spatial EA Parallel Boosting Algorithm.,Carlotta Domeniconi,Evolutionary Computation,10636560,,Spring2018,26,1,43,24.0,128237118,10.1162/evco_a_00202,MIT Press,Article,EVOLUTIONARY algorithms; EVOLUTIONARY computation; COMPUTER programming; GENETIC algorithms; NUMERICAL analysis; Computer systems design and related services (except video game design and development); Custom Computer Programming Services; Other Computer Related Services,large margin classifiers; machine learning.; parallel boosting; scalability; Spatial evolutionary algorithms,"Many real-world problems involve massive amounts of data. Under these circumstances learning algorithms often become prohibitively expensive, making scalability a pressing issue to be addressed. A common approach is to perform sampling to reduce the size of the dataset and enable efficient learning. Alternatively, one customizes learning algorithms to achieve scalability. In either case, the key challenge is to obtain algorithmic efficiency without compromising the quality of the results. In this article we discuss a meta-learning algorithm (PSBML) that combines concepts from spatially structured evolutionary algorithms (SSEAs) with concepts from ensemble and boosting methodologies to achieve the desired scalability property. We present both theoretical and empirical analyses which show that PSBML preserves a critical property of boosting, specifically, convergence to a distribution centered around the margin. We then present additional empirical analyses showing that this meta-level algorithm provides a general and effective framework that can be used in combination with a variety of learning classifiers. We perform extensive experiments to investigate the trade-off achieved between scalability and accuracy, and robustness to noise, on both synthetic and real-world data. These empirical results corroborate our theoretical analysis, and demonstrate the potential of PSBML in achieving scalability without sacrificing accuracy. [ABSTRACT FROM AUTHOR] Copyright of Evolutionary Computation is the property of MIT Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128237118&site=ehost-live
277,Active Multilabel Crowd Consensus.,Carlotta Domeniconi,IEEE Transactions on Neural Networks & Learning Systems,2162237X,,Apr2021,32,14,1448,12.0,149773418,10.1109/TNNLS.2020.2984729,IEEE,Article,CONSENSUS (Social sciences); ACTIVE learning; LABELS; LEARNING strategies; CROWDS; CROWDSOURCING; Packaging and Labeling Services; Commercial Printing (except Screen and Books); Other printing,Active learning (AL); Biological system modeling; Computational modeling; Correlation; cost; Crowdsourcing; Learning systems; multilabel crowd consensus; Reliability; specialty and commonality; Task analysis,"Crowdsourcing is an economic and efficient strategy aimed at collecting annotations of data through an online platform. Crowd workers with different expertise are paid for their service, and the task requester usually has a limited budget. How to collect reliable annotations for multilabel data and how to compute the consensus within budget are an interesting and challenging, but rarely studied, problem. In this article, we propose a novel approach to accomplish active multilabel crowd consensus (AMCC). AMCC accounts for the commonality and individuality of workers and assumes that workers can be organized into different groups. Each group includes a set of workers who share a similar annotation behavior and label correlations. To achieve an effective multilabel consensus, AMCC models workers’ annotations via a linear combination of commonality and individuality and reduces the impact of unreliable workers by assigning smaller weights to their groups. To collect reliable annotations with reduced cost, AMCC introduces an active crowdsourcing learning strategy that selects sample–label–worker triplets. In a triplet, the selected sample and label are the most informative for the consensus model, and the selected worker can reliably annotate the sample at a low cost. Our experimental results on multilabel data sets demonstrate the advantages of AMCC over state-of-the-art solutions on computing crowd consensus and on reducing the budget by choosing cost-effective triplets. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Neural Networks & Learning Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149773418&site=ehost-live
278,Attributed heterogeneous network fusion via collaborative matrix tri-factorization.,Carlotta Domeniconi,Information Fusion,15662535,,Nov2020,63,,153,13.0,145698744,10.1016/j.inffus.2020.06.012,Elsevier B.V.,Article,LOW-rank matrices; MATRIX decomposition; MULTISENSOR data fusion; LINEAR network coding; MATRICES; PANCREATIC cancer,Attributed heterogeneous networks; Data fusion; Insufficient relations; LncRNA-disease associations; Matrix factorization,"• A comprehensive data fusion model for attributed multi-relational data is proposed. • AHNF conquers the negative impact of insufficient relations between network nodes. • AHNF avoids the loss when converting attributes into homo-networks for fusion. • AHNF can selectively integrate diverse relational and attribute data sources. • AHNF outperforms state-of-the-art matrix factorization based data fusion solutions. Heterogeneous network based data fusion can encode diverse inter- and intra-relations between objects, and has been sparking increasing attention in recent years. Matrix factorization based data fusion models have been invented to fuse multiple data sources. However, these models generally suffer from the widely-witnessed insufficient relations between nodes and from information loss when heterogeneous attributes of diverse network nodes are transformed into ad-hoc homologous networks for fusion. In this paper, we introduce a general data fusion model called Attributed Heterogeneous Network Fusion (AHNF). AHNF firstly constructs an attributed heterogeneous network composed with different types of nodes and the diverse attribute vectors of these nodes. It uses indicator matrices to differentiate the observed inter-relations from the latent ones, and thus reduces the impact of insufficient relations between nodes. Next, it collaboratively factorizes multiple adjacency matrices and attribute data matrices of the heterogeneous network into low-rank matrices to explore the latent relations between these nodes. In this way, both the network topology and diverse attributes of nodes are fused in a coordinated fashion. Finally, it uses the optimized low-rank matrices to approximate the target relational data matrix of objects and to effectively accomplish the relation prediction. We apply AHNF to predict the lncRNA-disease associations using diverse relational and attribute data sources. AHNF achieves a larger area under the receiver operating curve 0.9367 (by at least 2.14%), and a larger area under the precision-recall curve 0.5937 (by at least 28.53%) than competitive data fusion approaches. AHNF also outperforms competing methods on predicting de novo lncRNA-disease associations, and precisely identifies lncRNAs associated with breast, stomach, prostate, and pancreatic cancers. AHNF is a comprehensive data fusion framework for universal attributed multi-type relational data. The code and datasets are available at http://mlda.swu.edu.cn/codes.php?name=AHNF. [ABSTRACT FROM AUTHOR] Copyright of Information Fusion is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=145698744&site=ehost-live
279,Co-Clustering Ensembles Based on Multiple Relevance Measures.,Carlotta Domeniconi,IEEE Transactions on Knowledge & Data Engineering,10414347,,Apr2021,33,4,1389,12.0,149122312,10.1109/TKDE.2019.2942029,IEEE,Article,MATRIX multiplications; INFORMATION resources; MATHEMATICAL optimization; WEIGHTS & measures,Bipartite graph; Co-clustering; co-clustering ensemble; Fuses; Gene expression; Minimization; multiple relevance measures; Optimization; Robustness; Runtime; trace minimization,"Co-clustering aims at discovering groups of both objects and features from a given data matrix. Co-clustering ensembles can produce robust co-clusters by combining multiple base co-clusterings. However, current co-clustering ensemble solutions either ignore the constraints resulting from feature-to-feature and object-to-object relevance information, or ignore feature-to-object relevance information. In this paper, we advocate that all three information sources contribute to the achievement of good consensus solutions, and propose a co-clustering ensemble (CoCE) approach based on multiple relevance measures. CoCE first evaluates the quality of base co-clusters and consequently measures feature-to-object relevance. The latter, along with feature-to-feature and object-to-object relevance measures, contribute to the definition of a hybrid graph. The consensus process uses the resulting hybrid graph; it's formulated as a trace minimization problem and introduces a block-wise matrix multiplication technique to perform the optimization. Experimental results on various datasets show that CoCE not only frequently outperforms other related co-clustering ensembles, but also has reduced runtime cost and is more robust to poor base co-clusterings. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149122312&site=ehost-live
280,Flexible Cross-Modal Hashing.,Carlotta Domeniconi,IEEE Transactions on Neural Networks & Learning Systems,2162237X,,Jan2022,33,1,304,11.0,154800821,10.1109/TNNLS.2020.3027729,IEEE,Article,MATHEMATICAL optimization; INFORMATION retrieval; LINEAR programming,Clustering-based match; Correlation; cross-modal hashing; flexibility; Hash functions; Linear programming; optimization; Semantics; STEM; Training; weakly paired data,"Hashing has been widely adopted for large-scale data retrieval in many domains due to its low storage cost and high retrieval speed. Existing cross-modal hashing methods optimistically assume that the correspondence between training samples across modalities is readily available. This assumption is unrealistic in practical applications. In addition, existing methods generally require the same number of samples across different modalities, which restricts their flexibility. We propose a flexible cross-modal hashing approach (FlexCMH) to learn effective hashing codes from weakly paired data, whose correspondence across modalities is partially (or even totally) unknown. FlexCMH first introduces a clustering-based matching strategy to explore the structure of each cluster and, thus, to find the potential correspondence between clusters (and samples therein) across modalities. To reduce the impact of an incomplete correspondence, it jointly optimizes the potential correspondence, the cross-modal hashing functions derived from the correspondence, and a hashing quantitative loss in a unified objective function. An alternative optimization technique is also proposed to coordinate the correspondence and hash functions and reinforce the reciprocal effects of the two objectives. Experiments on public multimodal data sets show that FlexCMH achieves significantly better results than state-of-the-art methods, and it, indeed, offers a high degree of flexibility for practical cross-modal hashing tasks. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Neural Networks & Learning Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154800821&site=ehost-live
281,Kernel Pooled Local Subspaces for Classification.,Carlotta Domeniconi,"IEEE Transactions on Systems, Man & Cybernetics: Part B",10834419,,Jun2005,35,3,489,14.0,17176860,10.1109/TSMCB.2005.846641,IEEE,Article,STATISTICAL correlation; KERNEL functions; FUNCTIONAL analysis; LEARNING; DISCRIMINANT analysis; MULTIVARIATE analysis,,"We investigate the use of subspace analysis methods for learning low-dimensional representations for classification. We propose a kernel-pooled local discriminant subspace method and compare it against competing techniques: kernel principal component analysis (KPCA) and generalized discriminant analysis (GDA) in classification problems. We evaluate the classification performance of the nearest-neighbor rule with each subspace representation. The experimental results using several data sets demonstrate the effectiveness and performance superiority of the kernel-pooled subspace method over competing methods such as KPCA and GDA in some classification problems. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part B is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=17176860&site=ehost-live
282,Semi-supervised classification based on random subspace dimensionality reduction,Carlotta Domeniconi,Pattern Recognition,00313203,,Mar2012,45,3,1119,17.0,66944275,10.1016/j.patcog.2011.08.024,Elsevier B.V.,Article,SUPERVISED learning; PATTERN recognition systems; INVARIANT subspaces; DIMENSION reduction (Statistics); GRAPHIC methods; EXPERIMENTS,Dimensionality reduction; Ensembles of classifiers; Graph construction; Random subspaces; Semi-supervised classification,"Abstract: Graph structure is vital to graph based semi-supervised learning. However, the problem of constructing a graph that reflects the underlying data distribution has been seldom investigated in semi-supervised learning, especially for high dimensional data. In this paper, we focus on graph construction for semi-supervised learning and propose a novel method called Semi-Supervised Classification based on Random Subspace Dimensionality Reduction, SSC-RSDR in short. Different from traditional methods that perform graph-based dimensionality reduction and classification in the original space, SSC-RSDR performs these tasks in subspaces. More specifically, SSC-RSDR generates several random subspaces of the original space and applies graph-based semi-supervised dimensionality reduction in these random subspaces. It then constructs graphs in these processed random subspaces and trains semi-supervised classifiers on the graphs. Finally, it combines the resulting base classifiers into an ensemble classifier. Experimental results on face recognition tasks demonstrate that SSC-RSDR not only has superior recognition performance with respect to competitive methods, but also is robust against a wide range of values of input parameters. [Copyright &y& Elsevier] Copyright of Pattern Recognition is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=66944275&site=ehost-live
283,Weighted matrix factorization on multi-relational data for LncRNA-disease association prediction.,Carlotta Domeniconi,Methods,10462023,,Feb2020,173,,32,12.0,142144930,10.1016/j.ymeth.2019.06.015,Academic Press Inc.,Article,MATRIX decomposition; LOW-rank matrices; DATA fusion (Statistics); MULTISENSOR data fusion; NON-coding RNA,Data fusion; LncRNA-disease associations; Matrix factorization; Multiple heterogeneous networks,"• We proposed a weighted matrix factorization based data fusion approach to predict associations between lncRNAs and diseases. • Our approach can selectively integrate multiple inter-relational and intra-relational data sources. • Our approach can explore and exploit the intrinsic and shared structure of heterogeneous data sources. • Our approach is readily available for various link prediction problems. Influx evidences show that red long non-coding RNAs (lncRNAs) play important roles in various critical biological processes, and they afffect the development and progression of various human diseases. Therefore, it is necessary to precisely identify the lncRNA-disease associations. The identification precision can be improved by developing data integrative models. However, current models mainly need to project heterogeneous data onto the homologous networks, and then merge these networks into a composite one for integrative prediction. We recognize that this projection overrides the individual structure of the heterogeneous data, and the combination is impacted by noisy networks. As a result, the performance is compromised. Given that, we introduce a weighted matrix factorization model on multi-relational data to predict LncRNA-disease associations (WMFLDA). WMFLDA firstly uses a heterogeneous network to capture the inter(intra)-associations between different types of nodes (including genes, lncRNAs, and Disease Ontology terms). Then, it presets weights to these inter-association and intra-association matrices of the network, and cooperatively decomposes these matrices into low-rank ones to explore the underlying relationships between nodes. Next, it jointly optimizes the low-rank matrices and the weights. After that, WMFLDA approximates the lncRNA-disease association matrix using the optimized matrices and weights, and thus to achieve the prediction. WMFLDA obtains a much better performance than related data integrative solutions across different experiment settings and evaluation metrics. It can not only respect the intrinsic structures of individual data sources, but can also fuse them with selection. [ABSTRACT FROM AUTHOR] Copyright of Methods is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142144930&site=ehost-live
284,A heuristic scheme for multivariate set partitioning problems with application to classifying heterogeneous populations for multiple binary attributes.,Hadi El-Amine,IISE Transactions,24725854,,Jun2022,54,6,537,13.0,156075381,10.1080/24725854.2021.1959964,Taylor & Francis Ltd,Article,SEXUALLY transmitted diseases; POLYNOMIAL time algorithms; HEURISTIC; MEDICAL screening; ALGORITHMS; UNITED States; All Other Miscellaneous Ambulatory Health Care Services,combinatorial optimization; constrained shortest path; group testing; polynomial-time algorithm; Set partitioning problem,"We provide a novel heuristic approach to solve a class of multivariate set partitioning problems in which each item is characterized by three attribute values. The scheme first identifies a series of orderings of the items and then solves a corresponding sequence of shortest path problems. We provide theoretical findings on the structure of an optimal solution that motivate the design of the proposed heuristic scheme. The proposed algorithm runs in polynomial-time and is independent of the number of groups in the partition, making it more efficient than existing algorithms. To measure the performance of our solutions, we construct bounds for special instances which allow us to provide optimality gaps. We conduct an extensive numerical experiment in which we solve a large number of problem instances and show that our proposed approach converges to the global optimal solution in the vast majority of cases and in the case it does not, it yields very low optimality gaps. We demonstrate our findings with an application in the context of classifying a large heterogeneous population as positive or negative for multiple binary attributes as efficiently as possible. We conduct a case study on the screening of three of the most prevalent sexually transmitted diseases in the United States. The resulting solutions are shown to be within 2.6% of optimality and lead to a 26% cost saving over current screening practices. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156075381&site=ehost-live
285,An Exact Algorithm for Large-Scale Continuous Nonlinear Resource Allocation Problems with Minimax Regret Objectives.,Hadi El-Amine,INFORMS Journal on Computing,10919856,,Summer2021,33,3,1213,16.0,152160091,10.1287/ijoc.2020.0999,INFORMS: Institute for Operations Research,Article,RESOURCE allocation; DIFFERENTIABLE functions; ALGORITHMS; ROBUST optimization,Benders-type decomposition; continuous nonlinear optimization; interval minimax regret; robust optimization,"We study a large-scale resource allocation problem with a convex, separable, not necessarily differentiable objective function that includes uncertain parameters falling under an interval uncertainty set, considering a set of deterministic constraints. We devise an exact algorithm to solve the minimax regret formulation of this problem, which is NP-hard, and we show that the proposed Benders-type decomposition algorithm converges to an ɛ -optimal solution in finite time. We evaluate the performance of the proposed algorithm via an extensive computational study, and our results show that the proposed algorithm provides efficient solutions to large-scale problems, especially when the objective function is differentiable. Although the computation time takes longer for problems with nondifferentiable objective functions as expected, we show that good quality, near-optimal solutions can be achieved in shorter runtimes by using our exact approach. We also develop two heuristic approaches, which are partially based on our exact algorithm, and show that the merit of the proposed exact approach lies in both providing an ɛ -optimal solution and providing good quality near-optimal solutions by laying the foundation for efficient heuristic approaches. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152160091&site=ehost-live
286,Optimal clustering of frequency data with application to disease risk categorization.,Hadi El-Amine,IISE Transactions,24725854,,Aug2022,54,8,728,13.0,157056506,10.1080/24725854.2021.1973158,Taylor & Francis Ltd,Article,STATISTICS; DISTRIBUTION (Probability theory); DATA distribution; COMBINATORIAL analysis; MEDICAL screening; HIV testing kits; All Other Miscellaneous Ambulatory Health Care Services; Pharmaceutical and medicine manufacturing; In-Vitro Diagnostic Substance Manufacturing,Cluster analysis; constrained shortest path; disease risk categorization; hierarchical clustering; set partitioning problem,"We provide a clustering procedure for a special type of dataset, known as frequency data, which counts the frequency of a certain binary outcome. An interpretation of the data as a discrete distribution enables us to extract statistical information, which we embed within an optimization-based framework. Our analysis of the resulting combinatorial optimization problem allows us to reformulate it as a more tractable network flow problem. This, in turn, enables the construction of exact algorithms that converge to the optimal solution in quadratic time. In addition, to be able to handle large-scale datasets, we provide two hierarchical heuristic algorithms that run in linearithmic time. Our moment-based method results in clustering solutions that are shown to perform well for a family of applications. We illustrate the benefits of our findings through a case study on HIV risk categorization within the context of large-scale screening through group testing. Our results on CDC data show that the proposed clustering framework consistently outperforms other popular clustering methods. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157056506&site=ehost-live
287,Optimal Screening of Populations with Heterogeneous Risk Profiles Under the Availability of Multiple Tests.,Hadi El-Amine,INFORMS Journal on Computing,10919856,,Winter2022,34,1,150,15.0,155827900,10.1287/ijoc.2020.1051,INFORMS: Institute for Operations Research,Article,MEDICAL screening; POLYNOMIAL time algorithms; COMBINATORIAL optimization; UNITED States; All Other Miscellaneous Ambulatory Health Care Services,group testing; heterogeneous population; multiple tests; network flow; polynomial time algorithm,"We study the design of large-scale group testing schemes under a heterogeneous population (i.e., subjects with potentially different risk) and with the availability of multiple tests. The objective is to classify the population as positive or negative for a given binary characteristic (e.g., the presence of an infectious disease) as efficiently and accurately as possible. Our approach examines components often neglected in the literature, such as the dependence of testing cost on the group size and the possibility of no testing, which are especially relevant within a heterogeneous setting. By developing key structural properties of the resulting optimization problem, we are able to reduce it to a network flow problem under a specific, yet not too restrictive, objective function. We then provide results that facilitate the construction of the resulting graph and finally provide a polynomial time algorithm. Our case study, on the screening of HIV in the United States, demonstrates the substantial benefits of the proposed approach over conventional screening methods. Summary of Contribution: This paper studies the problem of testing heterogeneous populations in groups in order to reduce costs and hence allow for the use of more efficient tests for high-risk groups. The resulting problem is a difficult combinatorial optimization problem that is NP-complete under a general objective. Using structural properties specific to our objective function, we show that the problem can be cast as a network flow problem and provide a polynomial time algorithm. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155827900&site=ehost-live
288,An EM Algorithm for Ion-Channel Current Estimation.,Yariv Ephraim,IEEE Transactions on Signal Processing,1053587X,,Jan2008,56,1,26,8.0,28153258,10.1109/TSP.2007.906743,IEEE,Article,EXPECTATION-maximization algorithms; PARAMETER estimation; MARKOV processes; ION channels; MAXIMUM likelihood statistics; CELL membranes,Ion-channel; Markov modulated process,"Parameter estimation of a continuous-time Markov chain observed through a discrete-time memoryless channel is studied. An expectation-maximization (EM) algorithm for maximum likelihood estimation of the parameter of this hidden Markov process is developed and applied to a simple example of modeling ion-channel currents in living cell membranes. The approach follows that of Asmussen, Nerman and Olsson, and Rydén, for EM estimation of an underlying continuous-time Markov chain. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Signal Processing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=28153258&site=ehost-live
289,An EM Algorithm for Markov Modulated Markov Processes.,Yariv Ephraim,IEEE Transactions on Signal Processing,1053587X,,Feb2009,57,2,463,8.0,39060423,10.1109/TSP.2008.2007919,IEEE,Article,EXPECTATION-maximization algorithms; ALGORITHMS; NUMERICAL solutions for Markov processes; STOCHASTIC processes; EXCESSIVE measures (Mathematics); FINITE element method; POISSON'S equation; POISSON processes,Expectation-maximization (EM) algorithm; Markov modulated Markov processes; Markov modulated Poisson processes,"An expectation-maximization (EM) algorithm for estimating the parameter of a Markov modulated Markov process in the maximum likelihood sense is developed. This is a doubly stochastic random process with an underlying continuous-time finite-state homogeneous Markov chain. Conditioned on that chain, the observable process is a continuous-time finite-state nonhomogeneous Markov chain. The generator of the observable process at any given time is determined by the state of the under- lying Markov chain at that time. The parameter of the process comprises the set of generators for the underlying and conditional Markov chains. The proposed approach generalizes an earlier approach by Rydén for estimating the parameter of a Markov modulated Poisson process. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Signal Processing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=39060423&site=ehost-live
290,Explicit Causal Recursive Estimators for Continuous-Time Bivariate Markov Chains.,Yariv Ephraim,IEEE Transactions on Signal Processing,1053587X,,May2014,62,10,2709,10.0,95801785,10.1109/TSP.2014.2314434,IEEE,Article,MARKOV processes; STOCHASTIC processes; STATISTICS; RADON; NUMERICAL integration,Educational institutions; Estimation; Hidden Markov models; Markov processes; Materials; Random processes; Real-time systems; recursive estimation,"A bivariate Markov chain comprises a pair of random processes which are jointly Markov. In this paper, both processes are assumed to be continuous-time with finite state space. One of the two processes is observable, while the other is an underlying process which affects the statistical properties of the observable process. Neither the observable, nor the underlying process , is required to be a Markov chain. Examples of bivariate Markov chains include the Markov modulated Markov process (MMMP), the Markov modulated Poisson process (MMPP), and the batch Markovian arrival process (BMAP). We develop explicit causal recursions for estimating the number of jumps from one state to another, and the total sojourn time in each state, of a general bivariate Markov chain. Explicit causal recursions of these statistics were previously developed for the MMMP and the MMPP using the transformation of measure approach. We argue that this approach cannot be extended to a general bivariate Markov chain. Instead, we modify the approach developed by Rydén for noncausal estimation of the same statistics of an MMPP, and use the state augmentation approach of Zeitouni and Dembo and a matrix recursion from Stiller and Radons, to derive the causal recursions. The new recursions do not require any numerical integration or sampling scheme of the continuous-time bivariate Markov chain. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Signal Processing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95801785&site=ehost-live
293,A methodology for ensuring fair allocation of CSOC effort for alert investigation.,Rajesh Ganesan,International Journal of Information Security,16155262,,Apr2019,18,2,199,20.0,135326761,10.1007/s10207-018-0407-3,Springer Nature,Article,METHODOLOGY; INTERNET security; SERVICE level agreements; INTERNET traffic; COMPUTER simulation; Internet Publishing and Broadcasting and Web Search Portals; Wired Telecommunications Carriers,Adaptive queueing; Cybersecurity analysts; Deficit model; Dynamic weighted queueing; Fair CSOC effort allocation; Integrated queueing approach; Simulation,"A Cyber Security Operations Center (CSOC) often sells services by entering into a service level agreement (SLA) with various customers (organizations) whose network traffic is monitored through sensors. The sensors produce data that are processed by automated systems (such as the intrusion detection system) that issue alerts. All alerts need further investigation by human analysts. The alerts are triaged into high-, medium-, and low-priority alerts, and the high-priority alerts are investigated first by cybersecurity analysts—a process known as priority queueing. In unexpected situations such as (i) higher than expected high-priority alert generation from some sensors, (ii) not enough analysts at the CSOC in a given time interval, and (iii) a new type of alert, which increases the time to analyze alerts from some sensors, the priority queueing mechanism leads to two major issues. The issues are: (1) some sensors with normal levels of alert generation are being analyzed less than those with excessive high-priority alerts, with the potential for complete starvation of alert analysis for sensors with only medium- or low-priority alerts, and (2) the above ad hoc allocation of CSOC effort to sensors with excessive high-priority alerts over other sensors results in SLA violations, and there is no enforcement mechanism to ensure the matching between the SLA and the actual service provided by a CSOC. This paper develops a new dynamic weighted alert queueing mechanism (DWQ) which relates the CSOC effort as per SLA to the actual allocated in practice, and ensures via a technical enforcement system that the total CSOC effort is proportionally divided among customers such that fairness is guaranteed in the long run. The results indicate that the DWQ mechanism outperforms priority queueing method by not only analyzing high-priority alerts first but also ensuring fairness in CSOC effort allocated to all its customers and providing a starvation-free alert investigation process. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Information Security is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135326761&site=ehost-live
294,A methodology to measure and monitor level of operational effectiveness of a CSOC.,Rajesh Ganesan,International Journal of Information Security,16155262,,Apr2018,17,2,121,14.0,128214788,10.1007/s10207-017-0365-1,Springer Nature,Article,INTRUSION detection systems (Computer security); INTERNET security; JOB absenteeism; SYSTEM downtime; COMPUTER worms; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals,Cybersecurity operations center; Intrusion detection; Level of operational effectiveness; Situational awareness of CSOC; Total time for alert investigation,"In a cybersecurity operations center (CSOC), under normal operating conditions in a day, sufficient numbers of analysts are available to analyze the amount of alert workload generated by intrusion detection systems (IDSs). For the purpose of this paper, this means that the cybersecurity analysts can fully investigate each and every alert that is generated by the IDSs in a reasonable amount of time. However, there are a number of disruptive factors that can adversely impact the normal operating conditions such as (1) higher alert generation rates from a few IDSs, (2) new alert patterns that decreases the throughput of the alert analysis process, and (3) analyst absenteeism. The impact of all the above factors is that the alerts wait for a long duration before being analyzed, which impacts the readiness of the CSOC. It is imperative that the readiness of the CSOC be quantified, which in this paper is defined as the level of operational effectiveness (LOE) of a CSOC. LOE can be quantified and monitored by knowing the exact deviation of the CSOC conditions from normal and how long it takes for the condition to return to normal. In this paper, we quantify LOE by defining a new metric called total time for alert investigation (TTA), which is the sum of the waiting time in the queue and the analyst investigation time of an alert after its arrival in the CSOC database. A dynamic TTA monitoring framework is developed in which a nominal average TTA per hour (avgTTA/hr) is established as the baseline for normal operating condition using individual TTA of alerts that were investigated in that hour. At the baseline value of avgTTA/hr, LOE is considered to be <italic>ideal</italic>. Also, an upper-bound (threshold) value for avgTTA/hr is established, below which the LOE is considered to be <italic>optimal</italic>. Several case studies illustrate the impact of the above disruptive factors on the dynamic behavior of avgTTA/hr, which provide useful insights about the current LOE of the system. Also, the effect of actions taken to return the CSOC to its normal operating condition is studied by varying both the amount and the time of action, which in turn impacts the dynamic behavior of avgTTA/hr. Results indicate that by using the insights learnt from measuring, monitoring, and controlling the dynamic behavior of avgTTA/hr, a manager can quantify and color-code the LOE of the CSOC. Furthermore, the above insights allow for a deeper understanding of acceptable downtime for the IDS, acceptable levels for absenteeism, and the recovery time and effort needed to return the CSOC to its ideal LOE. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Information Security is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128214788&site=ehost-live
295,A Multire solution Analysis-Assisted Reinforcement Learning Approach to Run-by-Run Control.,Rajesh Ganesan,IEEE Transactions on Automation Science & Engineering,15455955,,Apr2007,4,2,182,12.0,24812355,10.1109/TASE.2006.879915,IEEE,Article,"PROCESS control systems; MANUFACTURING processes; REINFORCEMENT learning; WAVELETS (Mathematics); STOCHASTIC systems; STOCHASTIC processes; Industrial Process Furnace and Oven Manufacturing; Instruments and Related Products Manufacturing for Measuring, Displaying, and Controlling Industrial Process Variables; Process, Physical Distribution, and Logistics Consulting Services",Chemical mechanical planarization (CMP); exponentially weighted moving average (EWMA); multiresolution; reinforcement learning; run-by-run control; wavelet; wavelet-modulated reinforcement learning-run-by run (WRL-RbR),"In recent years, the run-by run (RbR) control mechanism has emerged as a useful tool for keeping complex semi-conductor manufacturing processes on target during repeated short production runs. Many types of RbR controllers exist in the literature of which the exponentially weighted moving average (EWMA) controller is widely used in the industry. However, EWMA controllers are known to have several limitations. For example, in the presence of multiscale disturbances and lack of accurate process models, the performance of EWMA controller deteriorates and often fails to control the process. Also, the control of complex manufacturing processes requires sensing of multiple parameters that may be spatially distributed. New control strategies that can successfully use spatially distributed sensor data are required. This paper presents a new multiresolution analysis (wavelet) assisted reinforcement learning (RL)-based control strategy that can effectively deal with both multiscale disturbances in processes and the lack of process models. The novel idea of a wavelet-aided RL-based controller represents a paradigm shift in the control of large-scale stochastic dynamic systems of which the control problem is a subset. Henceforth, we refer our new control strategy as a WRL-RbR controller. The WRL-RbR controller is tested on a multiple-input-multiple-output chemical mechanical planarization process of wafer fabrication for which the process model is available. Results show that the RL controller outperforms EWMA-based controllers for low autocorrelation. The new controller also performs quite well for strongly auto-correlated processes for which the EWMA controllers are known to fail. Convergence analysis of the new breed of the WRL-RbR controller is presented. Further enhancement of the controller to deal with model-free processes and for inputs coming from spatially distributed environments are also discussed. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automation Science & Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24812355&site=ehost-live
296,A Multiscale Bayesian SPRT Approach for Online Process Monitoring.,Rajesh Ganesan,IEEE Transactions on Semiconductor Manufacturing,08946507,,Aug2008,21,3,399,14.0,34138387,10.1109/TSM.2008.2001217,IEEE,Article,"QUALITY control; PRODUCTION engineering; FACTORY management; SEMICONDUCTOR manufacturing; SEMICONDUCTOR industry; MANUFACTURING processes; BAYESIAN analysis; Industrial Process Furnace and Oven Manufacturing; Instruments and Related Products Manufacturing for Measuring, Displaying, and Controlling Industrial Process Variables; Semiconductor and Related Device Manufacturing",Bayesian sequential probability ratio test (SPRT); chemical-mechanical planarization (CMP); coefficient of friction; end point detection (EPD); multiresolution analysis; wavelet,"Online monitoring of complex processes, such as semiconductor manufacturing processes, often requires the need to analyze sensor data with multiple characteristics. Some of these characteristics include nonstationary behavior, non-Gaussian distribution, high frequency of data generation, and multiscale (multiple frequencies) noise that mask the true nature of the process. Furthermore, it is necessary to implement process monitoring schemes that take into consideration the cost associated with sampling and incorrect decision making without sacrificing sensitivity, robustness, and ease of implementation. In this paper, a novel multiscale Bayesian sequential probability ratio test (MBSPRT) is developed, which is shown to be efficient in monitoring processes with the above characteristics. The MBSPRT method is also made suitable for online application by developing a moving block data processing strategy, which can match the data processing speed with the rate of data acquisition. The efficacy of the MBSPRT method was tested via detection of the end point occurrence in a chemical-mechanical planarization (CMP) process of semiconductor manufacturing using coefficient of friction (CoF) data. The proposed methodology offers a cost effective alternative to the traditional end point method, which is based on expensive metrology. Test results from both oxide and copper metal CMP are presented which show that MBSPRT is uniquely capable of identifying the start and finish of the end point event. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Semiconductor Manufacturing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=34138387&site=ehost-live
297,Accuracy of reinforcement learning algorithms for predicting aircraft taxi-out times: A case-study of Tampa Bay departures,Rajesh Ganesan,Transportation Research: Part C,0968090X,,Dec2010,18,6,950,13.0,53303754,10.1016/j.trc.2010.03.003,Elsevier B.V.,Article,MACHINE learning; DYNAMIC programming; SYSTEMS engineering; REINFORCEMENT learning; INTERNATIONAL airports; DATABASES; FLIGHT delays & cancellations (Airlines); TAMPA International Airport,Flight delay; Reinforcement learning; Taxi-out prediction,"Abstract: Taxi-out delay is a significant portion of the block time of a flight. Uncertainty in taxi-out times reduces predictability of arrival times at the destination. This in turn results in inefficient use of airline resources such as aircraft, crew, and ground personnel. Taxi-out time prediction is also a first step in enabling schedule modifications that would help mitigate congestion and reduce emissions. The dynamically changing operation at the airport makes it difficult to accurately predict taxi-out time. In this paper we investigate the accuracy of taxi out time prediction using a nonparametric reinforcement learning (RL) based method, set in the probabilistic framework of stochastic dynamic programming. A case-study of Tampa International Airport (TPA) shows that on an average, with 93.7% probability, on any given day, our predicted mean taxi-out time for any given quarter, matches the actual mean taxi-out time for the same quarter with a standard error of 1.5min. Also, for individual flights, the taxi-out time of 81% of them were predicted accurately within a standard error of 2min. The predictions were done 15min before gate departure. Gate OUT, wheels OFF, wheels ON, and gate IN (OOOI) data available in the Aviation System Performance Metric (ASPM) database maintained by the Federal Aviation Administration (FAA) was used to model and analyze the problem. The prediction accuracy is high even without the use of detailed track data. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53303754&site=ehost-live
297,Accuracy of reinforcement learning algorithms for predicting aircraft taxi-out times: A case-study of Tampa Bay departures,Lance Sherry,Transportation Research: Part C,0968090X,,Dec2010,18,6,950,13.0,53303754,10.1016/j.trc.2010.03.003,Elsevier B.V.,Article,MACHINE learning; DYNAMIC programming; SYSTEMS engineering; REINFORCEMENT learning; INTERNATIONAL airports; DATABASES; FLIGHT delays & cancellations (Airlines); TAMPA International Airport,Flight delay; Reinforcement learning; Taxi-out prediction,"Abstract: Taxi-out delay is a significant portion of the block time of a flight. Uncertainty in taxi-out times reduces predictability of arrival times at the destination. This in turn results in inefficient use of airline resources such as aircraft, crew, and ground personnel. Taxi-out time prediction is also a first step in enabling schedule modifications that would help mitigate congestion and reduce emissions. The dynamically changing operation at the airport makes it difficult to accurately predict taxi-out time. In this paper we investigate the accuracy of taxi out time prediction using a nonparametric reinforcement learning (RL) based method, set in the probabilistic framework of stochastic dynamic programming. A case-study of Tampa International Airport (TPA) shows that on an average, with 93.7% probability, on any given day, our predicted mean taxi-out time for any given quarter, matches the actual mean taxi-out time for the same quarter with a standard error of 1.5min. Also, for individual flights, the taxi-out time of 81% of them were predicted accurately within a standard error of 2min. The predictions were done 15min before gate departure. Gate OUT, wheels OFF, wheels ON, and gate IN (OOOI) data available in the Aviation System Performance Metric (ASPM) database maintained by the Federal Aviation Administration (FAA) was used to model and analyze the problem. The prediction accuracy is high even without the use of detailed track data. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53303754&site=ehost-live
298,Adaptive Alert Management for Balancing Optimal Performance among Distributed CSOCs using Reinforcement Learning.,Rajesh Ganesan,IEEE Transactions on Parallel & Distributed Systems,10459219,,Jan2020,31,1,16,18.0,143316119,10.1109/TPDS.2019.2927977,IEEE,Article,DECISION support systems; DYNAMIC programming; STOCHASTIC programming,and adaptive resource allocation; centralized alert management; Computer security; Decision making; Distributed cybersecurity operations center (CSOC); level of operational effectiveness; Measurement; Monitoring; Organizations; Reinforcement learning; Sensors,"Large organizations typically have Cybersecurity Operations Centers (CSOCs) distributed at multiple locations that are independently managed, and they have their own cybersecurity analyst workforce. Under normal operating conditions, the CSOC locations are ideally staffed such that the alerts generated from the sensors in a work-shift are thoroughly investigated by the scheduled analysts in a timely manner. Unfortunately, when adverse events such as increase in alert arrival rates or alert investigation rates occur, alerts have to wait for a longer duration for analyst investigation, which poses a direct risk to organizations. Hence, our research objective is to mitigate the impact of the adverse events by dynamically and autonomously re-allocating alerts to other location(s) such that the performances of all the CSOC locations remain balanced. This is achieved through the development of a novel centralized adaptive decision support system whose task is to re-allocate alerts from the affected locations to other locations. This re-allocation decision is non-trivial because the following must be determined: (1) timing of a re-allocation decision, (2) number of alerts to be re-allocated, and (3) selection of the locations to which the alerts must be distributed. The centralized decision-maker (henceforth referred to as agent) continuously monitors and controls the level of operational effectiveness-LOE (a quantified performance metric) of all the locations. The agent's decision-making framework is based on the principles of stochastic dynamic programming and is solved using reinforcement learning (RL). In the experiments, the RL approach is compared with both rule-based and load balancing strategies. By simulating real-world scenarios, learning the best decisions for the agent, and applying the decisions on sample realizations of the CSOC's daily operation, the results show that the RL agent outperforms both approaches by generating (near-) optimal decisions that maintain a balanced LOE among the CSOC locations. Furthermore, the scalability experiments highlight the practicality of adapting the method to a large number of CSOC locations. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143316119&site=ehost-live
299,Improving quality of prediction in highly dynamic environments using approximate dynamic programming.,Rajesh Ganesan,Quality & Reliability Engineering International,07488017,,Nov2010,26,7,717,16.0,54861143,10.1002/qre.1127,Wiley-Blackwell,Article,DYNAMIC programming; DECISION making; AIRPORT traffic control; INTERNATIONAL airports; REGRESSION analysis; Air Traffic Control,air transportation; approximate dynamic programming; quality of prediction; reinforcement learning; taxi-out time,"In many applications, decision making under uncertainty often involves two steps-prediction of a certain quality parameter or indicator of the system under study and the subsequent use of the prediction in choosing actions. The prediction process is severely challenged by highly dynamic environments that particularly involve sequential decision making, such as air traffic control at airports in which congestion prediction is critical for smooth departure operations. Taxi-out time of a flight is an excellent indicator of surface congestion and is a quality parameter used in the assessment of airport delays. The regression, queueing, and moving average models have been shown to perform poorly in predicting taxi-out times because they are slow in adapting to the changing airport dynamics. This paper presents an approximate dynamic programming approach (reinforcement learning, RL) to taxi-out time prediction. The taxi-out prediction performance was tested on flight data obtained from the Federal Aviation Administration's (FAA) Aviation System Performance Metrics (ASPM) database on Detroit International (DTW), Washington Reagan National (DCA), Boston (BOS), New York John F. Kennedy (JFK) and Tampa International (TPA) airports. For example, at the Boston airport (presented in detail) the prediction accuracy by RL model was 14running-average model. In general, the RL model was 35-50% more accurate than the regression model for all of the above airports. Copyright © 2010 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of Quality & Reliability Engineering International is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=54861143&site=ehost-live
299,Improving quality of prediction in highly dynamic environments using approximate dynamic programming.,Lance Sherry,Quality & Reliability Engineering International,07488017,,Nov2010,26,7,717,16.0,54861143,10.1002/qre.1127,Wiley-Blackwell,Article,DYNAMIC programming; DECISION making; AIRPORT traffic control; INTERNATIONAL airports; REGRESSION analysis; Air Traffic Control,air transportation; approximate dynamic programming; quality of prediction; reinforcement learning; taxi-out time,"In many applications, decision making under uncertainty often involves two steps-prediction of a certain quality parameter or indicator of the system under study and the subsequent use of the prediction in choosing actions. The prediction process is severely challenged by highly dynamic environments that particularly involve sequential decision making, such as air traffic control at airports in which congestion prediction is critical for smooth departure operations. Taxi-out time of a flight is an excellent indicator of surface congestion and is a quality parameter used in the assessment of airport delays. The regression, queueing, and moving average models have been shown to perform poorly in predicting taxi-out times because they are slow in adapting to the changing airport dynamics. This paper presents an approximate dynamic programming approach (reinforcement learning, RL) to taxi-out time prediction. The taxi-out prediction performance was tested on flight data obtained from the Federal Aviation Administration's (FAA) Aviation System Performance Metrics (ASPM) database on Detroit International (DTW), Washington Reagan National (DCA), Boston (BOS), New York John F. Kennedy (JFK) and Tampa International (TPA) airports. For example, at the Boston airport (presented in detail) the prediction accuracy by RL model was 14running-average model. In general, the RL model was 35-50% more accurate than the regression model for all of the above airports. Copyright © 2010 John Wiley & Sons, Ltd. [ABSTRACT FROM AUTHOR] Copyright of Quality & Reliability Engineering International is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=54861143&site=ehost-live
300,Maintaining the level of operational effectiveness of a CSOC under adverse conditions.,Rajesh Ganesan,International Journal of Information Security,16155262,,Jun2022,21,3,637,15.0,157133985,10.1007/s10207-021-00573-4,Springer Nature,Article,BUSINESS hours; TIME management; DYNAMIC models,Alert analysis; Cybersecurity operations center; Discarding alerts; Level of operational effectiveness; Reinforcement learning,"The level of operational effectiveness (LOE) is a color-coded performance metric that is monitored by the cybersecurity operations center (CSOC). It is determined using the average time to analyze alerts (AvgTTA) in every hour of shift operation, where the time to analyze an alert (TTA) is the sum of waiting time in the queue and investigation time by the analysts. Ideally, the CSOC managers would set a predetermined baseline target for AvgTTA to be maintained for every hour of shift operation. However, due to adverse events, an imbalance may exist if the alert arrival rate far exceeds the service rate, resulting in high AvgTTA or low LOE. Upon exhausting all the analyst resources, the only option available to a CSOC manager is to discard alerts for restoring the LOE of the CSOC. The paper proposes two strategies: the value-based strategy is developed using a static optimization model while the reinforcement learning-based strategy is developed using a dynamic optimization model. The paper compares various strategies for discarding alerts and measures the following desiderata for comparing them: (1) minimize the number of alerts discarded, (2) ensure highest utilization of analysts, (3) determine the optimal time at which the alerts must be discarded in a shift, and (4) maintain the best possible LOE closest to the baseline target LOE. Results indicate that, overall, the RL strategy is the best performer among all strategies that guarantees the AvgTTA below the threshold value in every hour of shift operation while discarding the fewest number of alerts under adverse events. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Information Security is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157133985&site=ehost-live
301,Real-time monitoring of complex sensor data using wavelet-based multiresolution analysis.,Rajesh Ganesan,International Journal of Advanced Manufacturing Technology,02683768,,Dec2008,39,6-May,543,16.0,34649819,10.1007/s00170-007-1237-z,Springer Nature,Article,"MANUFACTURING processes; ENGINEERING instruments; ELECTRONIC data processing; PRODUCTION engineering; ELECTRIC conductivity; DETECTORS; Data Processing, Hosting, and Related Services; Instruments and Related Products Manufacturing for Measuring, Displaying, and Controlling Industrial Process Variables; Industrial Process Furnace and Oven Manufacturing",Nano-scale; Non-stationary data; Process monitoring; Semiconductor processes; Wavelet analysis,"Advancements made in sensor technology have resulted in complex sensor data that captures multiple process events. Real-time monitoring of complex manufacturing processes, such as nano-scale semiconductor polishing, often requires analysis of such complex sensor data. The multiple events in a process occur at multiple scales or frequencies (also referred to as multiscale) and are localized at different points in time. Recent literature contains several wavelet decomposition based multiscale sensor data analysis techniques including those that are developed for process monitoring applications, such as tool-life monitoring, bearing defect monitoring, and monitoring of ultra-precision processes. However, most of the above mentioned wavelet-based sensor data analysis techniques are designed for offline implementation. In an offline method, one can perform wavelet decomposition of longer data lengths in order to capture information needed for monitoring. However, this is computationally involved and needs longer processing time, which becomes a serious challenge in online (real time) applications. This paper first presents a complete online multiscale process monitoring methodology. The methodology is designed to deal with real-time analysis and testing of very high rate of process data collected by sensors. This is particularly critical and becomes a challenge for high rate of data collection by the sensors which pose additional difficulty of matching data processing rate with the data acquisition rate. The methodology is capable of displaying the analysis results through real time graphs for ease of process supervisory decision making. The methodology is demonstrated via a nano-scale silicon wafer polishing application. Sufficient details of the application are provided to assist readers in implementing this methodology for other processes. The results show that the methodology has the ability to deal with high rate of data collection as well as multiscale event detection. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Advanced Manufacturing Technology is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=34649819&site=ehost-live
302,Two Can Play That Game: An Adversarial Evaluation of a Cyber-Alert Inspection System.,Rajesh Ganesan,ACM Transactions on Intelligent Systems & Technology,21576904,,May2020,11,3,1,20.0,143388917,10.1145/3377554,Association for Computing Machinery,Article,GAME theory; REINFORCEMENT learning; DECISION making; MARKOV processes; MODEL theory,adversarial reinforcement learning; Cyber-security operations center; game theory,"Cyber-security is an important societal concern. Cyber-attacks have increased in numbers as well as in the extent of damage caused in every attack. Large organizations operate a Cyber Security Operation Center (CSOC), which forms the first line of cyber-defense. The inspection of cyber-alerts is a critical part of CSOC operations (defender or blue team). Recent work proposed a reinforcement learning (RL) based approach for the defender's decision-making to prevent the cyber-alert queue length from growing large and overwhelming the defender. In this article, we perform a red team (adversarial) evaluation of this approach. With the recent attacks on learning-based decision-making systems, it is even more important to test the limits of the defender's RL approach. Toward that end, we learn several adversarial alert generation policies and the best response against them for various defender's inspection policy. Surprisingly, we find the defender's policies to be quite robust to the best response of the attacker. In order to explain this observation, we extend the earlier defender's RL model to a game model with adversarial RL, and show that there exist defender policies that can be robust against any adversarial policy. We also derive a competitive baseline from the game theory model and compare it to the defender's RL approach. However, when we go further to exploit the assumptions made in the Markov Decision Process (MDP) in the defender's RL model, we discover an attacker policy that overwhelms the defender. We use a double oracle like approach to retrain the defender with episodes from this discovered attacker policy. This made the defender robust to the discovered attacker policy and no further harmful attacker policies were discovered. Overall, the adversarial RL and double oracle approach in RL are general techniques that are applicable to other RL usage in adversarial environments. [ABSTRACT FROM AUTHOR] Copyright of ACM Transactions on Intelligent Systems & Technology is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143388917&site=ehost-live
303,A spherical projection of a complex Hilbert space is conformal iff it is the stereographic projection.,Yotam Gingold,Differential Geometry--Dynamical Systems,1454511X,,2018,20,,38,33.0,131541853,,Balkan Society of Geometers (Societatea Balcanica a Geometrilor),Article,HILBERT space; NONLINEAR analysis; METRIC projections; CONFORMAL geometry; COMPACTIFICATION (Mathematics),angle; bijection; Boundization; compactification; Complex Hilbert space; conformal; directions at infinity; eye; geometry; infinity; inner product space; nonlinear; nonlinear transformations; perspective; projective; similarity; sphere; spherical bowl; stereographic projection,"We consider a family of nonlinear projections that map a complex Hilbert space onto a bounded ""bowl"" shaped subset of a sphere. Our main result states that a projection is conformal iff it is the stereographic projection and iff the projection renders all certain pairs of triangles induced by the projection to be similar. It follows that various so called ""compactifications"" that are given in the literature are special members of this family of nonlinear projections. These include the stereographic projection and the Poincare compactification. Background and motivation are discussed and several examples illustrating the results are provided. [ABSTRACT FROM AUTHOR] Copyright of Differential Geometry--Dynamical Systems is the property of Balkan Society of Geometers (Societatea Balcanica a Geometrilor) and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131541853&site=ehost-live
304,Foreword to the Special Section on Expressive 2016.,Yotam Gingold,Computers & Graphics,00978493,,Jun2017,65,,A1,1.0,123309984,10.1016/j.cag.2017.05.004,Elsevier B.V.,Article,MOTION estimation (Signal processing); OPTICAL motion tracking (Computer vision),,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123309984&site=ehost-live
305,Hyperspectral Inverse Skinning.,Yotam Gingold,Computer Graphics Forum,01677055,,Sep2020,39,6,49,17.0,145974300,10.1111/cgf.13903,Wiley-Blackwell,Article,INVERSE problems; MOTION capture (Human mechanics); AFFINE geometry; SKIN; GEOMETRIC modeling,affine geometry; animation; Computing methodologies → Mesh geometry models; deformation; hyperspectral unmixing; linear blend skinning; Motion processing,"In example‐based inverse linear blend skinning (LBS), a collection of poses (e.g. animation frames) are given, and the goal is finding skinning weights and transformation matrices that closely reproduce the input. These poses may come from physical simulation, direct mesh editing, motion capture or another deformation rig. We provide a re‐formulation of inverse skinning as a problem in high‐dimensional Euclidean space. The transformation matrices applied to a vertex across all poses can be thought of as a point in high dimensions. We cast the inverse LBS problem as one of finding a tight‐fitting simplex around these points (a well‐studied problem in hyperspectral imaging). Although we do not observe transformation matrices directly, the 3D position of a vertex across all of its poses defines an affine subspace, or flat. We solve a 'closest flat' optimization problem to find points on these flats, and then compute a minimum‐volume enclosing simplex whose vertices are the transformation matrices and whose barycentric coordinates are the skinning weights. We are able to create LBS rigs with state‐of‐the‐art reconstruction error and state‐of‐the‐art compression ratios for mesh animation sequences. Our solution does not consider weight sparsity or the rigidity of recovered transformations. We include observations and insights into the closest flat problem. Its ideal solution and optimal LBS reconstruction error remain an open problem. [ABSTRACT FROM AUTHOR] Copyright of Computer Graphics Forum is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=145974300&site=ehost-live
306,Interacting with Self-Similarity.,Yotam Gingold,Computer-Aided Design,00104485,,Jan2021,130,,N.PAG,1.0,146933449,10.1016/j.cad.2020.102931,Elsevier B.V.,Article,"MAP design; ALGORITHMS; HYBRID systems; GEOMETRY; Other printing; Book, Periodical, and Newspaper Merchant Wholesalers",Interaction; Shape analysis; Signature; Similarity,"Shape similarity is a fundamental problem in geometry processing, enabling applications such as surface correspondence, segmentation, and edit propagation. For example, a user may paint a stroke on one finger of a model and desire the edit to propagate to all fingers. Automatic approaches have difficulty matching user expectations, either due to an algorithm's inability to guess the scale at which the user is intending to edit or due to underlying deficiencies in the similarity metric (e.g., semantic information not present in the geometry). We propose an approach to interactively design self-similarity maps. We investigate two primitive operations, useful in a variety of scenarios: region and curve similarity. Users select example similar and dissimilar regions. Starting with an automatically generated multi-scale shape signature, our approach solves for a scale parameter and thresholds that group the example regions as specified. We propose a new Smooth Shape Diameter Signature (SSDS) as a more efficient alternative to the Heat or Wave Kernel Signature. If no such parameters can be found, our approach modifies the shape signature itself. Given a curve drawn on the surface, we perform hybrid discrete/continuous optimization to find similar curves elsewhere. We apply our approach for interactive editing scenarios: propagating mesh geometry, patterns duplication, and segmentation. • An approach for interactively defining self-similarity across a class of signatures. • The Smooth Shape Diameter Signature (SSDS), a new descriptive shape signature. • A method for distortion-free curve propagation via tangent-space curve unrolling. [ABSTRACT FROM AUTHOR] Copyright of Computer-Aided Design is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146933449&site=ehost-live
307,PosterChild: Blend‐Aware Artistic Posterization.,Yotam Gingold,Computer Graphics Forum,01677055,,Jul2021,40,4,87,13.0,151432863,10.1111/cgf.14343,Wiley-Blackwell,Article,ALGORITHMS; POLYMER blends; IMAGE processing; Photofinishing Laboratories (except One-Hour); One-Hour Photofinishing,Applied computing → Fine arts; CCS Concepts; Computing methodologies → Non‐photorealistic rendering; Image processing,"Posterization is an artistic effect which converts continuous images into regions of constant color with smooth boundaries, often with an artistically recolored palette. Artistic posterization is extremely time‐consuming and tedious. We introduce a blend‐aware algorithm for generating posterized images with palette‐based control for artistic recoloring. Our algorithm automatically extracts a palette and then uses multi‐label optimization to find blended‐color regions in terms of that palette. We smooth boundaries away from image details with frequency‐guided median filtering. We evaluate our algorithm with a comparative user study and showcase its ability to produce compelling posterizations of a variety of inputs. Our parameters provide artistic control and enable cohesive, real‐time recoloring after posterization pre‐processing. [ABSTRACT FROM AUTHOR] Copyright of Computer Graphics Forum is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151432863&site=ehost-live
308,Using isophotes and shadows to interactively model normal and height fields.,Yotam Gingold,Computers & Graphics,00978493,,Oct2016,59,,130,13.0,117587399,10.1016/j.cag.2016.02.004,Elsevier B.V.,Article,SHADES & shadows; MATHEMATICAL models; IMAGE processing; SILHOUETTES; Photofinishing Laboratories (except One-Hour); One-Hour Photofinishing,Isophotes; Non-photorealistic rendering; Sketching,"We introduce an interactive modeling tool for designing (a) a smooth 3D normal field from the isophotes of a discretely shaded 2D image and (b) lifting the normal field into a smooth height field given a cast shadow. Block or cartoon shading is a visual style in which artists depict a smoothly shaded 3D object using a small number of discrete brightness values, manifested as regions or bands of constant color. In our approach, artists trace isophotes, or curves of constant brightness, along the boundaries between constant color bands. Our algorithm first estimates light directions and computes 3D normals along the object silhouette and at intersections between isophotes from different light sources. We then propagate these 3D normals smoothly along isophotes, and subsequently throughout the interior of the shape. We describe our user interface for editing isophotes and correcting unintended normals produced by our algorithm. We also describe a technique for lifting the generated normal field into a height field given the boundary of the shadow cast by the object. We validate our approach with a perceptual experiment and comparisons to ground truth data. Finally, we present a set of 3D renderings created using our interface. [ABSTRACT FROM AUTHOR] Copyright of Computers & Graphics is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117587399&site=ehost-live
315,Detecting and classifying online dark visual propaganda.,Mahdi Hashemi,Image & Vision Computing,02628856,,Sep2019,89,,95,11.0,138459637,10.1016/j.imavis.2019.06.001,Elsevier B.V.,Article,PROPAGANDA; WORLD Wide Web; ONLINE social networks; WEBSITES; ORGANIZATIONAL communication; Internet Publishing and Broadcasting and Web Search Portals,Convolutional neural networks; Dark Web; Deep learning; Image classification; Machine learning; Violent extremist organizations,"The staggering increase in the amount of information on the World Wide Web (referred to as Web) has made Web page classification essential to retrieve useful information while filtering out unwanted, futile, or harmful contents. This massive information-sharing platform is occasionally abused for propagating extreme and radical ideologies and posing threats to national security and citizens. Detecting the so called dark material has gained more impetus following the recent outbreak of extremist groups and radical ideologies across the Web. The goal of this project, being the first of its own, is to surveil online social networks (OSN) and Web for real-time detection of visual propaganda by violent extremist organizations (VEOs). This is valuable not only for flagging and removing such content from OSN and Web, but also to provide military insight and narrative context inside VEOs. Visual propaganda by VEOs are not only detected, but also further classified based on the type of VEO and focus or intent of the image into hard propaganda, soft propaganda, symbolic propaganda, landscape, and organizational communications. Over 1.2 million images were automatically collected from suspicious OSN accounts and Web pages over a course of four years. Out of which, 120,000 images were manually classified to provide the training data for a convolutional neural network. An overall generalization accuracy of 97.02% and F 1 of 97.89% were achieved for a binary classification or mere detection of visual VEO propaganda and an overall generalization accuracy of 86.08% and F 1 ¯ of 85.76% for an eight-way classification based on the intent of the image. [ABSTRACT FROM AUTHOR] Copyright of Image & Vision Computing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138459637&site=ehost-live
318,idDock+: Integrating Machine Learning in Probabilistic Search for Protein-Protein Docking.,Irina Hashmi,Journal of Computational Biology,10665277,,Sep2015,22,9,806,17.0,109251700,10.1089/cmb.2015.0108,"Mary Ann Liebert, Inc.",Article,"MOLECULAR interactions; PROTEIN-protein interactions; MOLECULAR docking; MACHINE learning; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology",ensemble learning; informatics-driven; probabilistic search; Protein-protein docking; stochastic optimization,"Predicting the three-dimensional native structures of protein dimers, a problem known as protein-protein docking, is key to understanding molecular interactions. Docking is a computationally challenging problem due to the diversity of interactions and the high dimensionality of the configuration space. Existing methods draw configurations systematically or at random from the configuration space. The inaccuracy of scoring functions used to evaluate drawn configurations presents additional challenges. Evidence is growing that optimization of a scoring function is an effective technique only once the drawn configuration is sufficiently similar to the native structure. Therefore, in this article we present a method that employs optimization of a sophisticated energy function, FoldX, only to locally improve a promising configuration. The main question of how promising configurations are identified is addressed through a machine learning method trained a priori on an extensive dataset of functionally diverse protein dimers. To deal with the vast configuration space, a probabilistic search algorithm operates on top of the learner, feeding to it configurations drawn at random. We refer to our method as idDock+, for nformatics-riven ocking. idDock+is tested on 15 dimers of different sizes and functional classes. Analysis shows that on all systems idDock+finds a near-native structure and is comparable in accuracy to other state-of-the-art methods. idDock+ represents one of the first highly efficient hybrid methods that combines fast machine learning models with demanding optimization of sophisticated energy scoring functions. Our results indicate that this is a promising direction to improve both efficiency and accuracy in docking. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational Biology is the property of Mary Ann Liebert, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109251700&site=ehost-live
318,idDock+: Integrating Machine Learning in Probabilistic Search for Protein-Protein Docking.,Amarda Shehu,Journal of Computational Biology,10665277,,Sep2015,22,9,806,17.0,109251700,10.1089/cmb.2015.0108,"Mary Ann Liebert, Inc.",Article,"MOLECULAR interactions; PROTEIN-protein interactions; MOLECULAR docking; MACHINE learning; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology",ensemble learning; informatics-driven; probabilistic search; Protein-protein docking; stochastic optimization,"Predicting the three-dimensional native structures of protein dimers, a problem known as protein-protein docking, is key to understanding molecular interactions. Docking is a computationally challenging problem due to the diversity of interactions and the high dimensionality of the configuration space. Existing methods draw configurations systematically or at random from the configuration space. The inaccuracy of scoring functions used to evaluate drawn configurations presents additional challenges. Evidence is growing that optimization of a scoring function is an effective technique only once the drawn configuration is sufficiently similar to the native structure. Therefore, in this article we present a method that employs optimization of a sophisticated energy function, FoldX, only to locally improve a promising configuration. The main question of how promising configurations are identified is addressed through a machine learning method trained a priori on an extensive dataset of functionally diverse protein dimers. To deal with the vast configuration space, a probabilistic search algorithm operates on top of the learner, feeding to it configurations drawn at random. We refer to our method as idDock+, for nformatics-riven ocking. idDock+is tested on 15 dimers of different sizes and functional classes. Analysis shows that on all systems idDock+finds a near-native structure and is comparable in accuracy to other state-of-the-art methods. idDock+ represents one of the first highly efficient hybrid methods that combines fast machine learning models with demanding optimization of sophisticated energy scoring functions. Our results indicate that this is a promising direction to improve both efficiency and accuracy in docking. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational Biology is the property of Mary Ann Liebert, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109251700&site=ehost-live
319,Enhanced Security Authentication Based on Convolutional-LSTM Networks.,Monson Hayes,Sensors (14248220),14248220,,Aug2021,21,16,5379,1.0,152145933,10.3390/s21165379,MDPI,Article,DEEP learning; STATISTICAL decision making; CHANNEL estimation; PHYSICAL layer security; SHORT-term memory,classification algorithms; deep learning; physical layer security; wireless networks,"The performance of classical security authentication models can be severely affected by imperfect channel estimation as well as time-varying communication links. The commonly used approach of statistical decisions for the physical layer authenticator faces significant challenges in a dynamically changing, non-stationary environment. To address this problem, this paper introduces a deep learning-based authentication approach to learn and track the variations of channel characteristics, and thus improving the adaptability and convergence of the physical layer authentication. Specifically, an intelligent detection framework based on a Convolutional-Long Short-Term Memory (Convolutional-LSTM) network is designed to deal with channel differences without knowing the statistical properties of the channel. Both the robustness and the detection performance of the learning authentication scheme are analyzed, and extensive simulations and experiments show that the detection accuracy in time-varying environments is significantly improved. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152145933&site=ehost-live
321,Three-Dimensional Pose Estimation for Laboratory Mouse From Monocular Images.,Monson Hayes,IEEE Transactions on Image Processing,10577149,,Sep2019,28,9,4273,15.0,137295220,10.1109/TIP.2019.2908796,IEEE,Article,"LABORATORY mice; LABORATORIES; MICE; STREET addresses; MEASURE theory; HARDWARE stores; MEDICAL research; Home and auto supplies stores; All Other General Merchandise Stores; Hardware Stores; Veterinary Services; Testing Laboratories; Medical Laboratories; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); All Other Animal Production",3D pose estimation; Lenses; Mice; Monitoring; mouse behavior detection; mouse ethology; mouse home-cage monitoring; Pose estimation; Structured forests; Three-dimensional displays; trajectory features; Two dimensional displays,"Video-based activity and behavior analysis of mice has garnered wide attention in biomedical research. Animal facilities hold large numbers of mice housed in “home-cages” densely stored within ventilated racks. Automated analysis of mice activity in their home-cages can provide a new set of sensitive measures for detecting abnormalities and time-resolved deviation from the baseline behavior. Large-scale monitoring in animal facilities requires minimal footprint hardware that integrates seamlessly with the ventilated racks. The compactness of hardware imposes the use of fisheye lenses positioned in close proximity to the cage. In this paper, we propose a systematic approach to accurately estimate the 3D pose of the mouse from single-monocular fisheye-distorted images. Our approach employs a novel adaptation of a structured forest algorithm. We benchmark our algorithm against existing methods. We demonstrate the utility of the pose estimates in predicting mouse behavior in a continuous video. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Image Processing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137295220&site=ehost-live
326,Counterfactual time series analysis of short-term change in air pollution following the COVID-19 state of emergency in the United States.,Lucas Henneman,Scientific Reports,20452322,,12/7/2021,11,1,1,13.0,153996231,10.1038/s41598-021-02776-0,Springer Nature,Article,COVID-19 pandemic; AIR pollution; BOX-Jenkins forecasting; AIR quality standards; PARTICULATE matter; AUTOREGRESSIVE models; NEVADA,,"Lockdown measures implemented in response to the COVID-19 pandemic produced sudden behavioral changes. We implement counterfactual time series analysis based on seasonal autoregressive integrated moving average models (SARIMA), to examine the extent of air pollution reduction attained following state-level emergency declarations. We also investigate whether these reductions occurred everywhere in the US, and the local factors (geography, population density, and sources of emission) that drove them. Following state-level emergency declarations, we found evidence of a statistically significant decrease in nitrogen dioxide (NO2) levels in 34 of the 36 states and in fine particulate matter (PM2.5) levels in 16 of the 48 states that were investigated. The lockdown produced a decrease of up to 3.4 µg/m3 in PM2.5 (observed in California) with range (− 2.3, 3.4) and up to 11.6 ppb in NO2 (observed in Nevada) with range (− 0.6, 11.6). The state of emergency was declared at different dates for different states, therefore the period ""before"" the state of emergency in our analysis ranged from 8 to 10 weeks and the corresponding ""after"" period ranged from 8 to 6 weeks. These changes in PM2.5 and NO2 represent a substantial fraction of the annual mean National Ambient Air Quality Standards (NAAQS) of 12 µg/m3 and 53 ppb, respectively. As expected, we also found evidence that states with a higher percentage of mobile source emissions (obtained from 2014) experienced a greater decline in NO2 levels after the lockdown. Although the socioeconomic restrictions are not sustainable, our results provide a benchmark to estimate the extent of achievable air pollution reductions. Identification of factors contributing to pollutant reduction can help guide state-level policies to sustainably reduce air pollution. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153996231&site=ehost-live
327,Racial/Ethnic Disparities in Nationwide PM2.5 Concentrations: Perils of Assuming a Linear Relationship.,Lucas Henneman,Environmental Health Perspectives,00916765,,Jul2022,130,7,077701-1,3.0,158397459,10.1289/ehp11048,Superintendent of Documents,Article,"PARTICULATE matter; RACE; INDOOR air pollution; PSYCHOSOCIAL factors; HEALTH equity; ETHNIC groups; ENVIRONMENTAL justice; METROPOLITAN areas; RESIDENTIAL patterns; ENVIRONMENTAL exposure; MEDICAL research; UNITED States; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,"The article discusses research which explores the departures from linearity in the association between racial/ethnic composition and particulate matter 2:5 micrometers in aerodynamic diameter concentration across the U.S. in 2010. Topics covered include a comparison of the strength of the nonlinear association to the linear one and high-light implications for the quantification of resulting racial/ethnic disparities, and segments of the population for whom exposure has been underestimated.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158397459&site=ehost-live
331,Congestion pricing applications to manage high temporal demand for public services and their relevance to air space management.,Karla Hoffman,Transport Policy,0967070X,,Jul2013,28,,28,14.0,89278783,10.1016/j.tranpol.2012.02.004,Elsevier B.V.,Article,"CONGESTION pricing; MUNICIPAL services; AIRSPACE (Law); SURVEYS; GOVERNMENT agencies; POLITICAL science; SOCIAL sciences; Other federal government public administration; Other provincial and territorial public administration; Other local, municipal and regional public administration; Other General Government Support; Administration of Public Health Programs; Administration of Urban Planning and Community and Rural Development; Administration of Air and Water Resource and Solid Waste Management Programs; Administration of Housing Programs; Research and Development in the Social Sciences and Humanities",Airspace congestion; Auctions; Market-clearing mechanism; Runway service standards,"Abstract: This paper surveys pricing mechanisms used by government agencies to manage congestion, as well as highlights the many political and social issues that have to be addressed in order to implement a pricing mechanism. This survey was undertaken in order to be able to understand how congestion pricing could be used to help manage airspace capacity. This is an important question since a 2008 analysis by the Joint Economic Committee of the US Congress suggested that domestic air traffic delays in 2007 cost the economy as much as $41 billion, including $19 billion in increased operational costs for the airlines and $12 billion worth of lost time for passengers. The paper begins by surveying roadway congestion approaches throughout the world. We survey the successes that peak pricing charges have had on reducing congestion. We also report the other benefits that such practices have had: improving the public transportation network, improving the economy of the region, reducing carbon emissions, and creating new urban living spaces. We next examine other applications of congestion pricing, including managing demand for canal and bridges passage, port usage, access to city centers, and peak use of energy resources. The paper ends with a proposal for a two-staged approach to the management of air space and runway congestion. The first stage imposes a service standard on runway access that is consistent with an airport's capacity during good weather days. Then, when weather reduces capacity either in the airspace or on runways, we propose a congestion pricing mechanism that charges flights based on the amount of congestion the flight imposes on the entire system. [Copyright &y& Elsevier] Copyright of Transport Policy is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89278783&site=ehost-live
332,Estimating domestic US airline cost of delay based on European model.,Karla Hoffman,Transportation Research: Part C,0968090X,,Aug2013,33,,311,13.0,89307131,10.1016/j.trc.2011.10.003,Elsevier B.V.,Article,FLIGHT delays & cancellations (Airlines); AERONAUTICAL flights; COMMERCIAL aeronautics; AIRLINE industry; FEEDBACK control systems; TRANSPORTATION research; UNITED States; EUROPE; Scheduled air transportation; Non-scheduled specialty flying services; Nonscheduled Chartered Passenger Air Transportation; Scheduled Freight Air Transportation; Scheduled Passenger Air Transportation; Other support activities for transportation; All Other Support Activities for Transportation,Airline delay costs; Airline delays; Component; Economic modeling of airlines,"Abstract: Researchers are applying more holistic approaches to the feedback control of the air transportation system. Many of these approaches rely on economic feedback, including the cost of delays to the airlines. Establishing an accurate mechanism for estimating the cost of a delays for each portion of a flight (gate costs, taxiing in and out costs, and en-route costs) is useful for many aspects of modeling airline behavior and for better understanding the likely impact of regulations. Cook et al. (2004) developed a rigorous methodology and collected data for estimating the components of airline delay costs for various segments of a scheduled flight. The model, based on confidential information from European airlines for 12 types of aircraft ca. 2003, was not transparent with regards to how each of the major components of cost (crew costs, fuel costs, maintenance, depreciation, etc.) impacted that total. This paper describes the development of an airline cost model, based on the Eurocontrol model. The airline cost model explicitly identifies the components of airline costs, is based on US airline cost data, and includes 111 aircraft types. The new model is designed to allow costs to be updated whenever the basic costs change. It considers the type of the aircraft when making calculations, both from the perspective of fuel burn and other costs and uses publically available data on burn rates and block-hour direct operating costs (BHDOC) to obtain these estimates. A case-study analysis of airline costs of operation at 19 major US airports is provided. [Copyright &y& Elsevier] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89307131&site=ehost-live
332,Estimating domestic US airline cost of delay based on European model.,Lance Sherry,Transportation Research: Part C,0968090X,,Aug2013,33,,311,13.0,89307131,10.1016/j.trc.2011.10.003,Elsevier B.V.,Article,FLIGHT delays & cancellations (Airlines); AERONAUTICAL flights; COMMERCIAL aeronautics; AIRLINE industry; FEEDBACK control systems; TRANSPORTATION research; UNITED States; EUROPE; Scheduled air transportation; Non-scheduled specialty flying services; Nonscheduled Chartered Passenger Air Transportation; Scheduled Freight Air Transportation; Scheduled Passenger Air Transportation; Other support activities for transportation; All Other Support Activities for Transportation,Airline delay costs; Airline delays; Component; Economic modeling of airlines,"Abstract: Researchers are applying more holistic approaches to the feedback control of the air transportation system. Many of these approaches rely on economic feedback, including the cost of delays to the airlines. Establishing an accurate mechanism for estimating the cost of a delays for each portion of a flight (gate costs, taxiing in and out costs, and en-route costs) is useful for many aspects of modeling airline behavior and for better understanding the likely impact of regulations. Cook et al. (2004) developed a rigorous methodology and collected data for estimating the components of airline delay costs for various segments of a scheduled flight. The model, based on confidential information from European airlines for 12 types of aircraft ca. 2003, was not transparent with regards to how each of the major components of cost (crew costs, fuel costs, maintenance, depreciation, etc.) impacted that total. This paper describes the development of an airline cost model, based on the Eurocontrol model. The airline cost model explicitly identifies the components of airline costs, is based on US airline cost data, and includes 111 aircraft types. The new model is designed to allow costs to be updated whenever the basic costs change. It considers the type of the aircraft when making calculations, both from the perspective of fuel burn and other costs and uses publically available data on burn rates and block-hour direct operating costs (BHDOC) to obtain these estimates. A case-study analysis of airline costs of operation at 19 major US airports is provided. [Copyright &y& Elsevier] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89307131&site=ehost-live
333,Optimum Airport Capacity Utilization under Congestion Management: A Case Study of New York LaGuardia Airport.,Karla Hoffman,Transportation Planning & Technology,03081060,,Feb2008,31,1,93,20.0,28462559,10.1080/03081060701835779,Taylor & Francis Ltd,Article,TRAFFIC congestion; AIRLINE industry; SUPPLY & demand; FLIGHT delays & cancellations (Airlines); AIRPORT management; UNITED States; Scheduled air transportation; Scheduled Passenger Air Transportation; Other Airport Operations,airline; airport management; Congestion management; flight delays; regulation; supply–demand; supply-demand; upgauging,"In the United States, most airports do not place any limitations on airline schedules. At a few major airports, the current scheduling restrictions (mostly administrative measures) have not been sufficiently strict to avoid consistent delays and have raised debates about both the efficiency and the fairness of the allocations. With a forecast of 1.1 billion yearly air travelers within the US by 2015, airport expansion and technology enhancement alone are not enough to cope with the competition-driven scheduling practices of the airline industry. The policy legacy needs to change to be consistent with airport capacities. Flights on US airlines arrived late more often in the first four months of 2007 than in any other year since the government began tracking delays, and flight cancellations increased 91% over 2006. With a forecast of 1.1 billion yearly air travelers within the US by 2015, airport expansion and technology enhancement alone are not enough to cope with the competition-driven scheduling practices of the airline industry. Our research studies how flight schedules might change if airlines were required to restrict their schedules to runway capacity. To obtain these schedules, we model a profit-seeking, single benevolent airline whose goal is to maintain current competitive prices and service as many current passengers as possible, while remaining profitable. Our case study demonstrates that at Instrument Meteorological Conditions (IMC) runway rates, the market can find profitable flight schedules that reduce substantially the average flight delay to less than 6 minutes while simultaneously satisfying virtually all of the current demand with average prices remaining unchanged. This is accomplished through significant upgauging to high-demand markets. [ABSTRACT FROM AUTHOR] Copyright of Transportation Planning & Technology is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=28462559&site=ehost-live
334,A multi-objective production planning problem with the consideration of time and cost in clinical trials.,Edward Huang,Expert Systems with Applications,09574174,,Jun2019,124,,25,14.0,134849047,10.1016/j.eswa.2019.01.038,Elsevier B.V.,Article,"PRODUCTION planning; PROBLEM solving; CLINICAL trials; PRODUCTION quantity; PHARMACEUTICAL industry; SUPPLY chains; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Pharmaceutical and medicine manufacturing; Pharmaceutical Preparation Manufacturing; Pharmaceuticals and pharmacy supplies merchant wholesalers; Drugs and Druggists' Sundries Merchant Wholesalers; Process, Physical Distribution, and Logistics Consulting Services",Clinical trial; Multi-stage stochastic programming; Production and distribution planning; Progressive hedging algorithm; Supply chain management,"Highlights • The proposed algorithm gives better solutions than the direct solution approach. • The operational cost decreases with the increase of the production quantity. • We quantify the tradeoff between the duration and the total cost of clinical trials. • The tradeoff can be used to determine the optimal production quantity. Abstract Under increasingly challenging circumstances, pharmaceutical companies try to reduce the overproduction of clinical drugs, which is commonly seen in the pharmaceutical industry. When the overproduction is simply reduced without an efficient coordination of the inventories in the supply chain, the stock-outs at clinical sites and clinical trial delay can hardly be avoided. In this study, we propose a multi-objective model to optimize the production quantity, where the clinical trial duration and the total production and operational costs are minimized. The problem is formulated as a multi-stage stochastic programming model to capture the dynamic inventory allocation process in the supply chains. Since this problem's solving time and required memory can increase significantly with the increase of the stage and scenario numbers, the progressive hedging algorithm is applied as the solution approach in this paper. In the numerical experiments, we study this algorithm's performance and compare the solving efficiency with the direct solution approach. In addition, we identify the optimal production quantity of clinical drugs and give a discussion about the tradeoffs between the clinical trial delay and total cost. [ABSTRACT FROM AUTHOR] Copyright of Expert Systems with Applications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134849047&site=ehost-live
335,A Study on the Optimal Inventory Allocation for Clinical Trial Supply Chains.,Edward Huang,Applied Mathematical Modelling,0307904X,,Oct2021,98,,161,24.0,151558812,10.1016/j.apm.2021.04.029,Elsevier B.V.,Article,"SUPPLY chains; STOCHASTIC programming; WAREHOUSES; CLINICAL trials; INVENTORIES; OPERATING costs; PRODUCTION planning; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Process, Physical Distribution, and Logistics Consulting Services; General Warehousing and Storage; Other Warehousing and Storage; Commercial and Institutional Building Construction",Clinical trials; Inventory allocation; Stochastic programming; Supply chain management,"• A multi-period inventory allocation in multi-echelon clinical supply chains is investigated. • The reverse replenishment, transshipment, and stochastic demand are addressed. • A rolling horizon-based two-stage stochastic programming model is developed. • An algorithm extending Benders decomposition for clinical trial supply chains is proposed. • The reformulation method and row generation strategy are developed. With increasing competition in the pharmaceutical industry, pharmaceutical companies pay more attention to improving the efficiency of clinical trial supply chains to reduce the drug supplying cost, which takes up a considerable part of the total research and development expense. To improve the efficiency of clinical trial supply chains, this study investigates the inventory levels of clinical drugs in each period at a distribution center and clinics considering the reverse replenishment, transshipment, and generalized stockout cost. The inventory allocation problem in clinical trial supply chains is formulated as a rolling horizon-based two-stage stochastic mixed-integer model where the minimal operational cost constitutes the underage cost at the production planning level of clinical trial supply chains. An algorithm extending Benders decomposition is proposed as the solution approach. We also derive several structural results and develop the reformulation method and row generation strategy to improve the efficiency of the optimization process. The effectiveness of our approach is demonstrated in the numerical experiment. [ABSTRACT FROM AUTHOR] Copyright of Applied Mathematical Modelling is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151558812&site=ehost-live
336,Combating Sex Trafficking: The Role of the Hotel—Moral and Ethical Questions.,Edward Huang,Religions,20771444,,Feb2022,13,2,138,1.0,155523047,10.3390/rel13020138,MDPI,Article,SEX trafficking; HUMAN trafficking victims; SEX crimes; SEX industry; CIVIL procedure; HOTEL chains; HOSPITALITY; ATLANTIC Coast (U.S.); Courts; Hotels (except Casino Hotels) and Motels; All other miscellaneous store retailers (except beer and wine-making supplies stores),geographical crime; hospitality industry; human trafficking; illicit supply chain; sex trafficking,"Legitimate companies are key facilitators of human trafficking. These corporate facilitators include not only websites providing advertisements for commercial sex services but also hotels and motels. Analysis of all active federal criminal sex trafficking cases in 2018 and 2019 reveals that in approximately 80% of these cases, victims were exploited at either hotels or motels. This paper studies the prevalence of the hospitality industry in the crime of sex trafficking and the failure of this industry to address this problem until recent civil suits were filed by victims against individual hotels and chains. Drawing on the civil cases filed in federal courts by victims of human trafficking between 2015 and 2021 along the East Coast of the United States, this paper assesses the characteristics of these hotels and the conditions in the hotels that facilitated sex trafficking. The paper then explores the moral and ethical problems posed by the facilitating role of hotel owners/operators in sex trafficking either through collusion or failure to act on and/or report evidence of individual abuse. Suggestions on how to address the problem are provided. [ABSTRACT FROM AUTHOR] Copyright of Religions is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155523047&site=ehost-live
337,Job Dispatch Control for Production Lines With Overlapped Time Window Constraints.,Edward Huang,IEEE Transactions on Semiconductor Manufacturing,08946507,,May2018,31,2,206,9.0,129455497,10.1109/TSM.2018.2826530,IEEE,Article,SEMICONDUCTOR manufacturing; CONSTRAINTS (Physics); PRODUCT quality; PATTERN recognition systems; MICROFABRICATION; Semiconductor and Related Device Manufacturing,Job scheduling; Job shop scheduling; Microsoft Windows; Process control; semiconductor process modeling; Servers; Time factors; time windows; Workstations,"Semiconductor manufacturing processes often come with stringent requirements for product quality. In order to satisfy these requirements, time window constraints have been imposed. Typically, violation of the time windows can result in a lot being either reworked or scrapped. The presence of overlapped time window constraints renders the control of production lines very challenging as it involves managing the production process of many lots across multiple workstations. We study the performance of a production line with deterministic service times and predetermined, overlapped time windows. We solve the P|rj|C\max problem to estimate the queue time at each workstation, and develop an algorithm to decide whether a new lot can be released for processing at its first workstation so as to meet all time window constraints. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Semiconductor Manufacturing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129455497&site=ehost-live
338,Job Scheduling at Cascading Machines.,Edward Huang,IEEE Transactions on Automation Science & Engineering,15455955,,Oct2017,14,4,1634,9.0,125562292,10.1109/TASE.2017.2698919,IEEE,Article,CASCADING style sheets; PRODUCTION scheduling; DYNAMIC programming; MATHEMATICAL optimization; CUSTOMER services,Cascading; cluster tool; completion time minimization; Dynamic programming; Job shop scheduling; Processor scheduling; Schedules; scheduling,"We consider the serial batching scheduling problem in which a group of machines can process multiple jobs continuously to reduce the processing times of the second and subsequent jobs. The maximum batch size is finite. Since all jobs in the same batch are loaded and unloaded simultaneously, a completed job has to wait for the others. We examine how to schedule all jobs to minimize the total completion time. If the batch size is only one, i.e., a single job per batch, the total processing time will be longer, since no reduction in processing time is possible. As a consequence, the total completion time will also be longer. On the other hand, if the batch size is large, the total completion time can be large. Each job has to wait until all jobs in the same batch are completed. We identify several optimality properties of the optimal batching sequence. These properties are used to develop a dynamic programming algorithm to optimize the batching sequences efficiently. The complexity of the proposed method depends only on the maximum batch size and the number of jobs. The improvement achieved with the proposed method when compared with two other batching rules is illustrated using two practical applications.</p><p>Note to Practitioners— Machines with job cascading (or cluster tools) are commonly seen in semiconductor manufacturing processes. While meeting production move targets is a common goal for the nondelayed jobs, catching up with the schedule is an important task for the delayed jobs to achieve higher customer service level. From the viewpoint of scheduling, reducing the delay is equivalent to reducing completion time. Hence, we propose scheduling algorithms to reduce the completion time by taking advantage of the special structure of a cascading machine. In contrast to existing scheduling theory, the proposed algorithm can be efficient due to the small maximum batch size of real cascading machines. In addition to the theoretical contributions, this paper aims at solving practical problems through a rigorous approach. The finding and insight from this paper can be used to enhance shop floor control in a semiconductor fab. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Automation Science & Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125562292&site=ehost-live
339,Optimal assignment of airport baggage unloading zones to outgoing flights.,Edward Huang,Transportation Research: Part E,13665545,,Oct2016,94,,110,13.0,118078450,10.1016/j.tre.2016.07.012,Elsevier B.V.,Article,BAGGAGE handling in airports; CARGO handling; CHUTES; STOCHASTIC analysis; STOCHASTIC processes; Other Airport Operations; Marine Cargo Handling,Airport; Baggage handling system (BHS); Design; Optimization,"The outbound airport baggage handling system (BHS) consists of a set of unloading zones (chutes) which are assigned to outgoing flights. Airport baggage operations have inherent uncertainties such as flight delays and varying number of bags. In this paper, the chute assignment problem is modeled as a Stochastic Vector Assignment Problem (SVAP) and multiple extensions are presented to incorporate the various design needs of the airport. A real airport outbound BHS is presented. This case study also guided the optimization models’ design process. The performance of the optimization models is compared with the methods used in practice and literature. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part E is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=118078450&site=ehost-live
340,Optimal inventory control in a multi-period newsvendor problem with non-stationary demand.,Edward Huang,Advanced Engineering Informatics,14740346,,Jan2015,29,1,139,7.0,100798048,10.1016/j.aei.2014.12.002,Elsevier B.V.,Article,"INVENTORY control; OPTIMAL control theory; NEWSVENDOR model; SUPPLY chains; ECONOMIC demand; STOCHASTIC programming; Process, Physical Distribution, and Logistics Consulting Services; All Other Support Services",Inventory models; Multi-stage stochastic programming; Newsvendor models; Progressive hedging methods,"The optimal control of inventory in supply chains plays a key role in the competiveness of a corporation. The inventory cost can account for half of company’s logistics cost. The classical inventory models, e.g., newsvendor and EOQ models, assume either a single or infinite planning periods. However, these models may not be applied to perishable products which usually have a certain shelf life. To optimize the total logistic cost for perishable products, this paper presents a multi-period newsvendor model, and the problem is formulated as a multi-stage stochastic programming model with integer recourse decisions. We extend the progressive hedging method to solve the model efficiently. A numerical example and its sensitivity analysis are demonstrated. [ABSTRACT FROM AUTHOR] Copyright of Advanced Engineering Informatics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100798048&site=ehost-live
341,Robust model for the assignment of outgoing flights on airport baggage unloading areas.,Edward Huang,Transportation Research: Part E,13665545,,Jul2018,115,,110,16.0,129997127,10.1016/j.tre.2018.04.012,Elsevier B.V.,Article,BAGGAGE handling in airports; ROBUST optimization; UNCERTAINTY; LOADING & unloading; CONSUMERS; Other Airport Operations,Airport; Baggage Handling System (BHS); Robust optimization,"One of the major functions of airports is to handle the baggage of customers from check-in counters to aircrafts. To have an efficient baggage handling system (BHS), the assignment of flights on unloading areas must incorporate uncertainties. We develop a robust optimization (RO) model to find a robust plan with constant performance stability in terms of future uncertainty realization. We construct a BHS simulation model to compare different assignment models. The results indicated that the total number of manually handled baggage using the robust plan is 58% lower than that using the current assignment. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part E is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129997127&site=ehost-live
342,Study structure may compromise understanding of longitudinal decision regret stability: A systematic review.,Edward Huang,Patient Education & Counseling,07383991,,Aug2020,103,8,1507,11.0,143857460,10.1016/j.pec.2020.03.011,Elsevier B.V.,journal article,REGRET; META-analysis; ONLINE databases; TREATMENT effectiveness; TIME perspective; PROSTATE tumors treatment; PATIENT participation; SYSTEMATIC reviews; DECISION making; QUALITY of life; EMOTIONS; PROSTATE tumors,Cancer; Decision regret; Study structure; Time horizon,"<bold>Objectives: </bold>To perform a systematic review of decision regret studies in cancer patients to determine if regret is longitudinally stable, and whether these study structures account for late-emerging treatment effects.<bold>Methods: </bold>Online databases including the George Mason Libraries, Global Health, Nursing and Allied Health, and PubMed were searched to identify decision regret studies with longitudinal components in patients with cancer.<bold>Results: </bold>A total of 845 unique citations were identified; 20 studies met inclusion criteria. Data was also collected on the time horizon for 90 studies; 47 % of studies evaluated regret at time points of one year or less, although this has increased significantly in prostate cancer citations since 2010. Regret was infrequent, affecting less than 20 % of patients, and often stable. Effect sizes in studies where decision regret changed over time were small to negligible.<bold>Conclusion: </bold>Longitudinal effects can influence the expression of decision regret, yet many studies are not designed to collect long-term data; prostate cancer studies may be particularly disadvantaged. The degree of this influence in current studies is small, though this outcome must be interpreted with caution.<bold>Practice Implications: </bold>Providers should be aware of the risk of late-emerging regret and counsel patients appropriately. [ABSTRACT FROM AUTHOR] Copyright of Patient Education & Counseling is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143857460&site=ehost-live
343,Verifying SysML activity diagrams using formal transformation to Petri nets.,Edward Huang,Systems Engineering,10981241,,Jan2020,23,1,118,18.0,141131403,10.1002/sys.21524,Wiley-Blackwell,Article,"PETRI nets; UNIFIED modeling language; CHARTS, diagrams, etc.; MATHEMATICAL notation; HUMAN behavior models; SET theory",model transformation; model‐based systems engineering; system behavior modeling,"The development of contemporary systems is an extremely complex process. One approach to modeling system behavior uses activity diagrams from Unified Modeling Language (UML)/System Modeling Language (SysML), providing a standard object‐oriented graphical notation and enhancing reusability. However, UML/SysML activity diagrams do not directly support the kind of analysis needed to verify the system behavior, such as might be available with a Petri net (PN) model. We show that a behavior model represented by a set of fUML‐compliant modeling elements in UML/SysML activity diagrams can be transformed into an equivalent PN, so that the analysis capability of PN can be applied. We define a formal mathematical notation for a set of modeling elements in activity diagrams, show the mapping rules between PN and activity diagrams, and propose a formal transformation algorithm. Two example system behavior models represented by UML/SysML activity diagrams are used for illustration. [ABSTRACT FROM AUTHOR] Copyright of Systems Engineering is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141131403&site=ehost-live
346,A cross-sectional investigation of SARS-CoV-2 seroprevalence and associated risk factors in children and adolescents in the United States.,Brett Hunter,PLoS ONE,19326203,,11/8/2021,16,11,1,14.0,153460925,10.1371/journal.pone.0259823,Public Library of Science,Article,SARS-CoV-2; COVID-19; SEROPREVALENCE; TEENAGERS; JUVENILE diseases; ANTIBODY titer; ETHNICITY; VIRGINIA,,"Background: Pediatric SARS-CoV-2 data remain limited and seropositivity rates in children were reported as <1% early in the pandemic. Seroepidemiologic evaluation of SARS-CoV-2 in children in a major metropolitan region of the US was performed. Methods: Children and adolescents ≤19 years were enrolled in a cross-sectional, observational study of SARS-CoV-2 seroprevalence from July-October 2020 in Northern Virginia, US. Demographic, health, and COVID-19 exposure information was collected, and blood analyzed for SARS-CoV-2 spike protein total antibody. Risk factors associated with SARS-CoV-2 seropositivity were analyzed. Orthogonal antibody testing was performed, and samples were evaluated for responses to different antigens. Results: In 1038 children, the anti-SARS-CoV-2 total antibody positivity rate was 8.5%. After multivariate logistic regression, significant risk factors included Hispanic ethnicity, public or absent insurance, a history of COVID-19 symptoms, exposure to person with COVID-19, a household member positive for SARS-CoV-2 and multi-family or apartment dwelling without a private entrance. 66% of seropositive children had no symptoms of COVID-19. Secondary analysis included orthogonal antibody testing with assays for 1) a receptor binding domain specific antigen and 2) a nucleocapsid specific antigen had concordance rates of 80.5% and 79.3% respectively. Conclusions: A much higher burden of SARS-CoV-2 infection, as determined by seropositivity, was found in children than previously reported; this was also higher compared to adults in the same region at a similar time. Contrary to prior reports, we determined children shoulder a significant burden of COVID-19 infection. The role of children's disease transmission must be considered in COVID-19 mitigation strategies including vaccination. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153460925&site=ehost-live
346,A cross-sectional investigation of SARS-CoV-2 seroprevalence and associated risk factors in children and adolescents in the United States.,Jiayang Sun,PLoS ONE,19326203,,11/8/2021,16,11,1,14.0,153460925,10.1371/journal.pone.0259823,Public Library of Science,Article,SARS-CoV-2; COVID-19; SEROPREVALENCE; TEENAGERS; JUVENILE diseases; ANTIBODY titer; ETHNICITY; VIRGINIA,,"Background: Pediatric SARS-CoV-2 data remain limited and seropositivity rates in children were reported as <1% early in the pandemic. Seroepidemiologic evaluation of SARS-CoV-2 in children in a major metropolitan region of the US was performed. Methods: Children and adolescents ≤19 years were enrolled in a cross-sectional, observational study of SARS-CoV-2 seroprevalence from July-October 2020 in Northern Virginia, US. Demographic, health, and COVID-19 exposure information was collected, and blood analyzed for SARS-CoV-2 spike protein total antibody. Risk factors associated with SARS-CoV-2 seropositivity were analyzed. Orthogonal antibody testing was performed, and samples were evaluated for responses to different antigens. Results: In 1038 children, the anti-SARS-CoV-2 total antibody positivity rate was 8.5%. After multivariate logistic regression, significant risk factors included Hispanic ethnicity, public or absent insurance, a history of COVID-19 symptoms, exposure to person with COVID-19, a household member positive for SARS-CoV-2 and multi-family or apartment dwelling without a private entrance. 66% of seropositive children had no symptoms of COVID-19. Secondary analysis included orthogonal antibody testing with assays for 1) a receptor binding domain specific antigen and 2) a nucleocapsid specific antigen had concordance rates of 80.5% and 79.3% respectively. Conclusions: A much higher burden of SARS-CoV-2 infection, as determined by seropositivity, was found in children than previously reported; this was also higher compared to adults in the same region at a similar time. Contrary to prior reports, we determined children shoulder a significant burden of COVID-19 infection. The role of children's disease transmission must be considered in COVID-19 mitigation strategies including vaccination. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153460925&site=ehost-live
352,Untangling the R2* contrast in multiple sclerosis: A combined MRI-histology study at 7.0 Tesla.,Vasiliki Ikonomidou,PLoS ONE,19326203,,3/21/2018,13,3,1,19.0,128594124,10.1371/journal.pone.0193839,Public Library of Science,Article,MULTIPLE sclerosis diagnosis; MULTIPLE sclerosis; BRAIN imaging; MYELIN proteins; PROTEOLIPIDS; MAGNETIC resonance imaging; Diagnostic Imaging Centers,Anatomical pathology; Anatomy; Autoimmune diseases; Biology and life sciences; Brain; Brain diseases; Central nervous system; Clinical immunology; Clinical medicine; Demyelinating disorders; Diagnostic medicine; Diagnostic radiology; Histology; Imaging techniques; Immunology; Magnetic resonance imaging; Medicine and health sciences; Multiple sclerosis; Nervous system; Neurodegenerative diseases; Neuroimaging; Neurology; Neuroscience; Pathology and laboratory medicine; Radiology and imaging; Research and analysis methods; Research Article; Thalamus,"T2*-weighted multi-echo gradient-echo magnetic resonance imaging and its reciprocal R2* are used in brain imaging due to their sensitivity to iron content. In patients with multiple sclerosis who display pathological alterations in iron and myelin contents, the use of R2* may offer a unique way to untangle mechanisms of disease. Coronal slices from 8 brains of deceased multiple sclerosis patients were imaged using a whole-body 7.0 Tesla MRI scanner. The scanning protocol included three-dimensional (3D) T2*-w multi-echo gradient-echo and 2D T2-w turbo spin echo (TSE) sequences. Histopathological analyses of myelin and iron content were done using Luxol fast blue and proteolipid myelin staining and 3,3′-diaminobenzidine tetrahydrochloride enhanced Turnbull blue staining. Quantification of R2*, myelin and iron intensity were obtained. Variations in R2* were found to be affected differently by myelin and iron content in different regions of multiple sclerosis brains. The data shall inform clinical investigators in addressing the role of T2*/R2* variations as a biomarker of tissue integrity in brains of MS patients, in vivo. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128594124&site=ehost-live
354,An Empirical Study of Face-to-Face and Distance Learning Sections of a Core Telecommunication course.,Khondkar Islam,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,11.0,116024952,,ASEE,Article,FACE-to-face communication; DISTANCE education; LEARNING Management System; EDUCATION software; TELECOMMUNICATION systems; All Other Miscellaneous Schools and Instruction; Administration of Education Programs; Educational Support Services; Satellite Telecommunications,,"We present an empirical study that compared the student learning outcomes of face-to-face and distance learning sections of a Telecommunications course. Student performance was assessed based on the course grade, which included the final exam, quizzes, assignments, and midterm exam scores. Both classes were taught by the same instructor, and had similar content and assessment measures. The study factored in the students' demographics such as gender, work and residency status to assess their impact on student learning. In addition, data stored in the learning management system (LMS), BlackBoard ™, were collected and used to understand student activities within the system, and determine their relation with student performance. The number of times the material was accessed and the time duration spent on assessments are some of the examples of the data that were included in the study. The results show that there is a correlation between students' use of Blackboard and student performance. We found a significant statistical difference between course grades of the face-to-face and distance learning sections. We did not find any evidence for significant difference across a range of demographic factors. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116024952&site=ehost-live
354,An Empirical Study of Face-to-Face and Distance Learning Sections of a Core Telecommunication course.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,11.0,116024952,,ASEE,Article,FACE-to-face communication; DISTANCE education; LEARNING Management System; EDUCATION software; TELECOMMUNICATION systems; All Other Miscellaneous Schools and Instruction; Administration of Education Programs; Educational Support Services; Satellite Telecommunications,,"We present an empirical study that compared the student learning outcomes of face-to-face and distance learning sections of a Telecommunications course. Student performance was assessed based on the course grade, which included the final exam, quizzes, assignments, and midterm exam scores. Both classes were taught by the same instructor, and had similar content and assessment measures. The study factored in the students' demographics such as gender, work and residency status to assess their impact on student learning. In addition, data stored in the learning management system (LMS), BlackBoard ™, were collected and used to understand student activities within the system, and determine their relation with student performance. The number of times the material was accessed and the time duration spent on assessments are some of the examples of the data that were included in the study. The results show that there is a correlation between students' use of Blackboard and student performance. We found a significant statistical difference between course grades of the face-to-face and distance learning sections. We did not find any evidence for significant difference across a range of demographic factors. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116024952&site=ehost-live
355,Analysis and Modeling of Upstream Throughput in Multihop Packet CDMA Cellular Networks.,Bijan Jabbari,IEEE Transactions on Communications,00906778,,Apr2006,54,4,680,13.0,20720526,10.1109/TCOMM.2006.873076,IEEE,Article,CODE division multiple access; SPREAD spectrum communications; TIME division multiple access; WIRELESS communications; SHADOWING theorem (Mathematics); NETWORK routers; POWER transmission; ELECTRONICS; ELECTRICAL engineering; Engineering Services; Other Electronic and Precision Equipment Repair and Maintenance; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),Ad hoc networks; multihop networks; network capacity; network modeling; wireless packet code-division multiple access,"We consider the problem of throughput modeling of wireless multihop packet CDMA networks with cellular overlay using simple forwarding strategies in the upstream. Considering the effect of shadowing and distance-dependent path loss, we approximate the probability density of interference at each base station (BS) and compare numerical and simulation results for different path-loss parameters. We derive the probability density of the received power at each BS due to transmission of one packet from a random node, as well as the probability distribution of the number of packets received at each node per time slot. Subsequently, we use the above results to approximate the probability density of the total received power at each BS based on calculations of moments. We observe that the probability density of intercell interference due to transmissions from terminals and routers may be approximated by normal and log-normal densities, respectively. We quantify the network performance based on throughput, total consumed power, and outage probability for different system parameters. For homogeneous link efficiencies, introducing routers into the network while reducing the transmission power increases the mean and variance of interference to the desired signal, hence higher outage probability. However, there are ample opportunities inherent to multihop structure, applicable to any of the physical, data link, and network layers, which help increase the overall achievable network throughput. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Communications is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20720526&site=ehost-live
356,DRAGON: A Framework for Service Provisioning in Heterogeneous Grid Networks.,Bijan Jabbari,IEEE Communications Magazine,01636804,,Mar2006,44,3,84,7.0,20300193,10.1109/MCOM.2006.1607870,IEEE,Article,OPTICAL communications; RESOURCE allocation; SONET (Data transmission); DATA transmission systems; GRID computing; COMPUTER systems; COMMUNICATION infrastructure; TELECOMMUNICATION systems; TECHNOLOGY; Satellite Telecommunications; Computer Systems Design Services; Computer systems design and related services (except video game design and development),,"Dynamic Resource Allocation in GMPLS Optical Networks (DRAGON) defines a research and experimental framework for high-performance networks required by Grid computing and e-science applications. The DRAGON project is developing technology and deploying network infrastructure which allows dynamic provisioning of network resources in order to establish deterministic paths in direct response to end-user requests. This includes multidomain provisioning of traffic-engineering paths using a distributed control plane across heterogeneous network technologies while including mechanisms for authentication, authorization, accounting (AAA), and scheduling. A reference implementation of this framework has been instantiated in the Washington, DC area and is being utilized to conduct research and development into the deployment of optical networks technologies toward the satisfaction of very-high-performance science application requirements. [ABSTRACT FROM AUTHOR] Copyright of IEEE Communications Magazine is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20300193&site=ehost-live
357,Data-Driven Optimization of Reward-Risk Ratio Measures.,Ran Ji,INFORMS Journal on Computing,10919856,,Summer2021,33,3,1120,18.0,152160094,10.1287/ijoc.2020.1002,INFORMS: Institute for Operations Research,Article,NONLINEAR programming; SHARPE ratio; SET functions; MODULAR design; FRACTIONAL programming; ROBUST optimization,data-driven optimization; distributionally robust optimization; fractional programming; reward-risk ratio; Wasserstein metric,"We investigate a class of fractional distributionally robust optimization problems with uncertain probabilities. They consist in the maximization of ambiguous fractional functions representing reward-risk ratios and have a semi-infinite programming epigraphic formulation. We derive a new fully parameterized closed-form to compute a new bound on the size of the Wasserstein ambiguity ball. We design a data-driven reformulation and solution framework. The reformulation phase involves the derivation of the support function of the ambiguity set and the concave conjugate of the ratio function. We design modular bisection algorithms which enjoy the finite convergence property. This class of problems has wide applicability in finance, and we specify new ambiguous portfolio optimization models for the Sharpe and Omega ratios. The computational study shows the applicability and scalability of the framework to solve quickly large, industry-relevant-size problems, which cannot be solved in one day with state-of-the-art mixed-integer nonlinear programming (MINLP) solvers. [ABSTRACT FROM AUTHOR] Copyright of INFORMS Journal on Computing is the property of INFORMS: Institute for Operations Research and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152160094&site=ehost-live
358,Distributionally robust portfolio optimization with linearized STARR performance measure.,Ran Ji,Quantitative Finance,14697688,,Jan 2022,22,1,113,15.0,154901822,10.1080/14697688.2021.1993623,Taylor & Francis Ltd,Article,ROBUST optimization; DUALITY theory (Mathematics); LINEAR programming; PORTFOLIO performance; STOCK exchanges; Securities and Commodity Exchanges,Conditional value-at-risk; Distributionally robust optimization; LSTARR performance measure; Wasserstein metric,"We study the distributionally robust linearized stable tail adjusted return ratio (DRLSTARR) portfolio optimization problem, in which the objective is to maximize the worst-case linearized stable tail adjusted return ratio (LSTARR) performance measure under data-driven Wasserstein ambiguity. We consider two types of imperfectly known uncertainties, named uncertain probabilities and continuum of realizations, associated with the losses of assets. We account for two typical combinatorial trading constraints, called buy-in threshold and diversification constraints, to reflect stock market restrictions. Leveraging conic duality theory to tackle the distributionally robust worst-case expectation, the proposed problems are reformulated into mixed-integer linear programming problems. We carry out a series of empirical tests to illustrate the scalability and effectiveness of the proposed solution framework, and to evaluate the performance of the DRLSTARR-constructed portfolios. The cross-validation results obtained using a rolling-horizon procedure show the superior out-of-sample performance of the DRLSTARR portfolios under an uncertain continuum of realizations. [ABSTRACT FROM AUTHOR] Copyright of Quantitative Finance is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154901822&site=ehost-live
359,Analytics and patterns of knowledge creation: Experts at work in an online engineering community.,Aditya Johri,Computers & Education,03601315,,Sep2017,112,,18,19.0,123504643,10.1016/j.compedu.2017.04.011,Elsevier B.V.,Article,DISTANCE education; ENGINEERING; THEORY of knowledge; METAPHOR; EDUCATORS; Administration of Education Programs; All Other Miscellaneous Schools and Instruction,Engineering education; Knowledge creation; Learning analytics; Learning communities,"Online learning communities have gained popularity amongst engineering learners who seek to build knowledge and share their expertise with others; yet to date, limited research has been devoted to the development of analytics for engineering communities. This is addressed through our study of an online engineering community that serves 31,219 engineering learners who contributed 503,908 messages in 65,209 topics. The guiding theoretical framework is the knowledge creation metaphor, which conceptualizes learning as a collaborative process of developing shared knowledge artifacts for the collective benefit of a community of learners. The aims of this study are twofold: (1) to analyze the state of knowledge creation in the community; and (2) to evaluate the strength of association between proposed analytics and variables indicative of knowledge creation in online environments. Findings suggest that the community is vibrant as a whole but also reveal disparity in participation at the individual level. At the topic-level, knowledge creation activities are strongly associated with Topic Length and moderately associated with Topic Duration. At the individual-level, participation in knowledge creation activities is strongly associated with Individual Total Interactions and weakly associated with Individual Total Membership Period. The implications of the findings are discussed and may provide guidance for educators seeking to adopt learning analytics in online communities. [ABSTRACT FROM AUTHOR] Copyright of Computers & Education is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123504643&site=ehost-live
360,Artificial intelligence and engineering education.,Aditya Johri,Journal of Engineering Education,10694730,,Jul2020,109,3,358,4.0,144725934,10.1002/jee.20326,Wiley-Blackwell,Article,"ENGINEERING education; ARTIFICIAL intelligence; SOCIAL computing; AUTOMATIC control systems; COMPUTER software; Software publishers (except video game publishers); Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers",,"I was recently invited to give a research seminar at an academic institution and so I did what most academics do in such cases: I made a PowerPoint (PPT) presentation. This whole process would have gone unnoticed except that in this instance I was preparing a talk on the potential impact of artificial intelligence (AI) on engineering education. Most AI experts agree that although artificial general intelligence, the kind of AI that is just like humans, is a long way off, artificial narrow intelligence (ANI), which is programmed to perform a single task, is making rapid progress. The recent National AI R&D Plan (Parker, 2018) and NSF investment in AI Institutes are timely reminders that we indeed need to pay attention to AI. [Extracted from the article] Copyright of Journal of Engineering Education is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144725934&site=ehost-live
361,Contextual Constraining of Student Design Practices.,Aditya Johri,Journal of Engineering Education,10694730,,Jul2015,104,3,252,27.0,108426192,10.1002/jee.20079,Wiley-Blackwell,Article,ENGINEERING design education in universities & colleges; ENGINEERING teachers; ENGINEERING student research; STRUCTURATION theory; EFFECTIVE teaching; Engineering Services,engineering design education; nested structuration theory; qualitative case study,"Background Engineering design is of significant interest to engineering educators. As yet, how the higher education context shapes student outcomes in engineering design courses remains underexplored. Since design courses are the primary way students are taught the critical topic of design, it is important to understand how the institutional and organizational contexts shape student outcomes and how we could improve design projects, given the context. Purpose We sought to answer two questions: What aspects of the design education process are salient, or important, for students? How do these salient aspects affect their design practices? Design/Method We used a qualitative case study approach to address the research questions because of our emphasis on understanding process-related aspects of design work and developing an interpretive understanding from the students' perspective. Results Using a nested structuration framework, we show that the context of design practices shaped students' outcomes by constraining their approach to the project and by providing a framework for their design process. We provide recommendations for design educators to help students overcome impediments to achieving learning objectives for design activities. Our research questions the efficacy of teaching engineering design when a design problem lacks a context beyond the classroom. Conclusions The institutional and organizational contexts influence student design practices. Engineering educators should carefully consider the potential effects of the design projects they implement within a higher education context. [ABSTRACT FROM AUTHOR] Copyright of Journal of Engineering Education is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108426192&site=ehost-live
362,Curating Tweets: A Framework for Using Twitter forWorkplace Learning.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2019,,,8314,19.0,139581986,,ASEE,Article,,,"Cybersecurity is a rapidly evolving field where professionals constantly need to keep up with new technologies and retrain. In this paper, we present a study that analyzed social media data and use the findings to aid professionals and students to learn more effectively using Twitter. We analyzed 23,000 cybersecurity related tweets posted on Twitter across two hashtags #cybersecurity and #infosec. Our analysis created a framework that explains how using descriptive, content, and network analysis can generate information that can help professionals learn. In addition, our analysis provided insights on the tweets and the cybersecurity community that use them. These insights include: Most tweets covered multiple topics and used three or more hashtags. Companies and other organizations had the highest numbers of followers, but individual users, experts in the field, were the most retweeted. Popular users, based on follower counts, were not necessarily the most influential (based on retweets). In terms of content, popular tweets consisted of infographics that packed a lot of information. Tweets were commonly used to announce file dumps of hacks and data leaks. Many highly used hashtags represented current threats and the overall sentiment of cybersecurity tweets are negative. Highly connected users on Twitter served as hubs across the three primary sub communities identified in the data. Insights from his study can assist with improving workforce development by guiding professionals in getting pertinent information and keeping up to date with the latest security threats and news. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139581986&site=ehost-live
363,Developing and Advancing a Cyberinfrastructure to Gain Insights into Research Investments: An Organizing Research Framework.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,9.0,116025299,,ASEE,Article,CYBERINFRASTRUCTURE; DISTRIBUTED computing; STEM education; ENGINEERING education; NATIONAL Science Foundation (U.S.),,"Although the National Science Foundation (NSF) funds approximately 24% of basic research conducted in U.S. colleges and universities 1, we know little about how NSF funding decisions have shaped the current research terrain. For instance, what new research areas have emerged and how have faculty collaborated over time? The Deep Insights Anytime, Anywhere (DIA2) project was precipitated by the need to better understand these issues and translate them into easy to understand insights for the STEM education community. We were also motivated to understand how research outcomes, particularly of STEM education projects, impact STEM education practice. As part of this project, the DIA2 project team has designed an information and visualization portal (http://www.dia2.org) that allows researchers and scientists to browse and search public data from NSF to understand the research terrain (including information about research on specific topics and researchers active in the area). There are many challenges associated with developing and using such a cyberinfrastructure, but also many potential advantages for practitioners, researchers, and policymakers. In this paper we discuss the research opportunities provided by DIA2 and present the research framework guiding the DIA2 project--a description of the three major themes/areas of research for the study. The paper summarizes the research questions and research activities corresponding to each of the themes, presents next steps, and based on our findings, highlights the value of DIA2 to members of the STEM education community. These concentrated efforts can not only help us better understand the impact and landscape of STEM education research, but this study can also serve as a framework for other large scale cyber-enabled research projects. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116025299&site=ehost-live
364,"Developing Global Engineering Competency Through Participation in ""Engineers Without Borders"".",Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,14.0,116025309,,ASEE,Article,ENGINEERING; CORE competencies; ENGINEERING education; ENGINEERS Without Borders (Organization),,"With a growing need for globally competent engineers, global engineering educational experiences, such as Engineers Without Borders (EWB), have become an important potential avenue for teaching students global engineering competencies. The purpose of this qualitative case study was to better understand engineering students' learning experiences in a EWB project, looking specifically at how students participating on the project exhibit attributes of global engineering competencies. The case study investigates an EWB project with the mission of designing and implementing a solar-powered electricity system for a school in Uganda. We found that students do exhibit attributes of global engineering competencies, although attributes regarding engineering cultures and ethics were exhibited more strongly than attributes regarding global regulations and standards. We discuss implications of these findings for educational practice and future research. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116025309&site=ehost-live
365,Engineering Time: Learning Analytics Initiative to Understand how Firstyear Engineering Students Spend their Time.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2019,,,10506,12.0,139582129,,ASEE,Article,,,"This Complete Research paper describes a learning analytics (LA) informed initiative to collect a detailed account of how first-year engineering students spend their time. With a plethora of calls to increase the number of engineering graduates, it is imperative to set students up for success during their first year. While there are multiple strategies infused in students' first year of college, and as many focused towards engineering students, there are still gaps in our understanding of what students do with their time outside of the classroom. This paper presents a study that uses a learning analytics initiative to uncover what students are doing outside the classroom and how they spend their time. Specifically, this study addresses one research question: How do first-year engineering students manage their time? Time management is one of the most critical aspects of a student's success in college. Analyzing time management practices of students can provide valuable information about how they work and what helps them succeed. Our research details a pilot study of 14 first-year engineering students across two weeks during the Fall semester of 2017. Students used a shared Google Sheet to keep track of their activities in half-hour increments using a template created by the research team. The template includes six categories for students to fill-in: date, time, location, activity, course, and notes. Results of the study highlight the daily habits of first-year engineering students with sleep (36.94%), leisure (19.22%), other (11.04%), studying short- and long-term (8.93%), and class (7.89%) as the top four categories where students spend their time. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139582129&site=ehost-live
366,Examining Learner-driven Constructs in Co-curricular Engineering Environments: The Role of Student Reflection in Assessment Development.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2017,,,11464,8.0,125730503,,ASEE,Article,CURRICULUM; ENGINEERING students; ENGINEERING education; FACULTY-college relationship; FACULTY advisors,,"Informal learning experiences are under-utilized in engineering education. Because of the voluntary nature of these experiences, many students may be unaware of their existence or how to access these experiences. Other students may not understand the benefits and, therefore, opt out. As such, students may be missing opportunities to extend their engineering understanding and skills to make them more competitive in the workforce. Therefore, it is important to examine the learning processes and outcomes supported by informal learning experiences. Co-curricular engineering experiences range from unstructured environments, such as social networking, to structured, such as engineering competitions that more closely approximate the workplace. Such experiences situate learning within an environment that may foster integration of knowledge and skills to solve problems (Pierrakos, Borrego, & Lo, 2007). These informal learning environments represent degrees of complexity. Therefore, students application of design or problem solving within such environments may also lead to other desired outcomes, such as increases in innovative thinking, the development of adaptive expertise, or being able to flexibly navigate multiple types of engineering environments (Kusano & Johri, 2014; Pellegrino, DiBello, & Brophy, 2014; Sawyer & Greeno, 2008; Stevens et al., 2008). However, research about how to assess outcomes attained via participation in informal learning environments is nascent. In this paper, we applied situated learning theory as a theoretical framework because our focus was on learning within engineering competitions (Johri & Olds, 2011; Johri, Olds, & O'Connor, 2014). Situated learning theory allowed us to focus on the activities in which students' engaged in order to understand the interactions that engendered different types of learning. Faculty study learning because it allows us to help students to improve, such as for formative purposes, or to certify that students have learned, such as for summative purposes. Thus, faculty make judgments about student learning based on assessment data. Validity is the most important criteria to examine the nature of the judgments and decisions related to test use. Validity does not exist as a property of a test. Rather, validity is about providing evidence that good and appropriate decisions were made based on assessment data (AERA, APA, & NCME, 2014). Therefore, validation processes must begin with the conception of the assessment. Validity is not simply a box to be checked for due diligence. Rather, validity must undergird every decision that is made to develop the assessment, demonstrate that it works for the purposes intended, and to draw any conclusions from assessment data, whether those decisions are about students, faculty teaching, programs, or policy. Response processes are one source of validity evidence. Gathering data about how students respond to assessment tasks or test items allows psychometricians to understand how learners think about, process, and respond to given performance tasks or test items. Messick (1990) stated, that response processes ""probe the ways in which individuals cope with the items or tasks, in an effort to illuminate the processes underlying item response and task performance"" (p. 5). For this paper, we extended the definition of response processes to elucidate the processes in extended performances, such as a competitions, which mirror workplace learning. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125730503&site=ehost-live
367,Informing the Sharing and Access of Engineering Education Research Data through Comparative Analysis.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,16.0,116025764,,ASEE,Article,ENGINEERING education; ENGINEERING students; COMPARATIVE studies; LEARNING; STUDENT surveys,,"The rapid growth of engineering education as a field of rigorous research has resulted in an explosion of available data and research results. There are numerous research efforts currently underway that gather data on a variety of topics that have the potential to help us better understand how students learn engineering. However, there are currently no easy methods to synthesize research results, share research data, and indeed validate research studies effectively. In general, topics related to data and data sharing are largely treated as taboos in the engineering education research space. Data sharing mechanisms to enable fundamental research in engineering education that has the potential to address systemic problems have not yet been clarified. The research goal of this paper is to identify and understand patterns for data sharing mechanisms in order to inform design requirements for data sharing practices and infrastructure in engineering education. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116025764&site=ehost-live
368,Lifelong and lifewide learning for the perpetual development of expertise in engineering.,Aditya Johri,European Journal of Engineering Education,03043797,,Feb 2022,47,1,70,15.0,154690360,10.1080/03043797.2021.1944064,Taylor & Francis Ltd,Article,LEARNING; SOFTWARE engineers; ENGINEERING education; EXPERTISE; ETHNOLOGY,digital participation; Engineering expertise; lifelong learning; lifewide learning; virtual ethnography,"Increasing digitisation of engineering and social practices has altered the relationship between formal schooling and development of expertise for professional engineering work. What does the development of expertise look like when knowledge is generated and shared at an accelerated pace due to shifts in technology? In this paper, I present case studies of two early career software engineers. Using methodological insights from digital ethnography, I trace their professional journeys over two decades. I empirically demonstrate how the development of engineering expertise is a continuous and perpetual endeavour and engineers learn throughout their lives (lifelong) and across all the different spaces they inhabit at any given time (lifewide). I argue for extending engineering work practices research and research in engineering education more broadly to take larger timescales of learning into account to build a comprehensive understanding of engineering expertise development. [ABSTRACT FROM AUTHOR] Copyright of European Journal of Engineering Education is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154690360&site=ehost-live
369,"Live, Love, Juul: User and Content Analysis of Twitter Posts about Juul.",Aditya Johri,American Journal of Health Behavior,10873244,,Mar/Apr2019,43,2,326,11.0,134748482,10.5993/AJHB.43.2.9,PNG Publications,Article,SMOKING prevention; AGE distribution; CHI-squared test; COMMUNICATION; CONTENT analysis; HEALTH education; HEALTH promotion; MARKETING; PRESS; PUBLIC opinion; RESEARCH funding; STATISTICAL sampling; STATISTICS; ELECTRONIC cigarettes; Marketing Consulting Services; News Syndicates; Marketing Research and Public Opinion Polling; Tobacco product manufacturing; Tobacco Manufacturing; TWITTER (Web resource),content analysis; electronic cigarettes; JUUL; smoking; social media; Twitter,"Objectives: In this study, we identified patterns of communication around Juul use and users on Twitter. Methods: Public tweets were collected from April 27, 2018 until June 27, 2018. We categorized 1008 randomly selected tweets on 4 dimensions: user type, sentiment, genre, and theme. Results: Most tweets were through personal accounts followed by ones of the tobacco industry. Participation by anti-tobacco campaigners, educational, and governmental entities was limited. Posts were mostly about first-hand use, use intentions, and personal opinions. Tweets advocating Juul were most common; meanwhile a handful of tweets discouraged Juul use. Young women, young men, and the tobacco industry expressed positive sentiments about Juul. Conclusions: Twitter data are a rich source of public communication to complement surveillance of emerging tobacco products. Youth actively and positively communicate about Juul on Twitter. Educational content and strategies must be examined for curtailing dissemination of positive sentiments and advocacy that normalize and promote Juul use among youth and non-smokers. We observed limited evidence supporting a claim for Juul to be a smoking cessation adjunct. [ABSTRACT FROM AUTHOR] Copyright of American Journal of Health Behavior is the property of PNG Publications and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134748482&site=ehost-live
370,Needle in a haystack: Identifying learner posts that require urgent response in MOOC discussion forums.,Aditya Johri,Computers & Education,03601315,,Mar2018,118,,1,9.0,126946178,10.1016/j.compedu.2017.11.002,Elsevier B.V.,Article,ONLINE education; COLLABORATIVE learning; EDUCATIONAL cooperation; DATA mining; BIG data,Computer-mediated communication; Improving classroom teaching; MOOC; Navigation,"Although massive open online courses or MOOCs have been successful in attracting a large number of learners, they have not been equally successful in retaining the learners to the point of course completion. One critical point of failure in many courses, especially those that use discussion forums as a means of collaborative learning, is the large number of messages exchanged on the forums. The extensive exchange of messages often creates chaos from the instructors' perspective and several questions remain unanswered. Lack of attention and response to urgent messages – those that are critical from the learners’ perspective to move forward – becomes a major challenge in this environment. This paper proposes a model to identify “urgent” posts that need immediate attention from instructors. In our analysis, we investigate different feature sets and different data mining techniques, and report the best set of features and classification techniques for addressing the problem of identifying messages that need urgent attention. The results demonstrate the ability to use a limited number of linguistic features with select metadata to build a moderate to substantially reliable classification model that can identify urgent posts in MOOC forums regardless of the course content. The work has potential application across a range of platforms that provide large scale courses and can help instructors efficiently navigate the discussion forums and prioritize the responses so that timely intervention can support learning and may reduce dropout rates. [ABSTRACT FROM AUTHOR] Copyright of Computers & Education is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126946178&site=ehost-live
370,Needle in a haystack: Identifying learner posts that require urgent response in MOOC discussion forums.,Huzefa Rangwala,Computers & Education,03601315,,Mar2018,118,,1,9.0,126946178,10.1016/j.compedu.2017.11.002,Elsevier B.V.,Article,ONLINE education; COLLABORATIVE learning; EDUCATIONAL cooperation; DATA mining; BIG data,Computer-mediated communication; Improving classroom teaching; MOOC; Navigation,"Although massive open online courses or MOOCs have been successful in attracting a large number of learners, they have not been equally successful in retaining the learners to the point of course completion. One critical point of failure in many courses, especially those that use discussion forums as a means of collaborative learning, is the large number of messages exchanged on the forums. The extensive exchange of messages often creates chaos from the instructors' perspective and several questions remain unanswered. Lack of attention and response to urgent messages – those that are critical from the learners’ perspective to move forward – becomes a major challenge in this environment. This paper proposes a model to identify “urgent” posts that need immediate attention from instructors. In our analysis, we investigate different feature sets and different data mining techniques, and report the best set of features and classification techniques for addressing the problem of identifying messages that need urgent attention. The results demonstrate the ability to use a limited number of linguistic features with select metadata to build a moderate to substantially reliable classification model that can identify urgent posts in MOOC forums regardless of the course content. The work has potential application across a range of platforms that provide large scale courses and can help instructors efficiently navigate the discussion forums and prioritize the responses so that timely intervention can support learning and may reduce dropout rates. [ABSTRACT FROM AUTHOR] Copyright of Computers & Education is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126946178&site=ehost-live
371,Retention and Persistence among STEM Students: A Comparison of Direct Admit and Transfer Students across Engineering and Science.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2017,,,1,11.0,125729975,,ASEE,Article,STEM education; COLLEGE dropouts; TRANSFER students; ENGINEERING; SCIENCE,,"Improving student retention in particular science, technology, engineering and mathematics majors has focused on identifying strategies, and practices that will encourage students to complete a degree in STEM major. In this paper, we present findings from a study of retention and migration among STEM students, comparing rates across both engineering and science students. We look at all students admitted between 2009- 2014, both direct admits and transfer, at a large public university. Transfer students are often neglected in studies of retention and persistence especially in engineering. We found that engineering students are more persistent than science students with retention rates over 60% for engineering students compared to 40% in math. Persistence rates for firsttime students are less than transfer students in the engineering enrollments. Also, as in previous studies, most migration out of discipline occurs in the first two years of enrollment. We also found that among enrolled students, a large number of engineering students (almost 20%) have not declared a major some until later in their studies. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125729975&site=ehost-live
371,Retention and Persistence among STEM Students: A Comparison of Direct Admit and Transfer Students across Engineering and Science.,Huzefa Rangwala,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2017,,,1,11.0,125729975,,ASEE,Article,STEM education; COLLEGE dropouts; TRANSFER students; ENGINEERING; SCIENCE,,"Improving student retention in particular science, technology, engineering and mathematics majors has focused on identifying strategies, and practices that will encourage students to complete a degree in STEM major. In this paper, we present findings from a study of retention and migration among STEM students, comparing rates across both engineering and science students. We look at all students admitted between 2009- 2014, both direct admits and transfer, at a large public university. Transfer students are often neglected in studies of retention and persistence especially in engineering. We found that engineering students are more persistent than science students with retention rates over 60% for engineering students compared to 40% in math. Persistence rates for firsttime students are less than transfer students in the engineering enrollments. Also, as in previous studies, most migration out of discipline occurs in the first two years of enrollment. We also found that among enrolled students, a large number of engineering students (almost 20%) have not declared a major some until later in their studies. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125729975&site=ehost-live
372,SeeMore: A kinetic parallel computer sculpture for educating broad audiences on parallel computation.,Aditya Johri,Journal of Parallel & Distributed Computing,07437315,,Jul2017,105,,183,17.0,122841763,10.1016/j.jpdc.2017.01.017,Academic Press Inc.,Article,PARALLEL algorithms; PARALLEL computers; RASPBERRY Pi; SERVOMECHANISMS; COMPUTATIONAL complexity,Computer science education; Kinetic art; Parallel and distributed computing,"We discuss the design, implementation, and evaluation of a 256-node Raspberry-Pi cluster with kinetic properties. Each compute node is attached to a servo mechanism such that movement results from local computation. The result is SeeMore, a kinetic parallel computer sculpture designed to enable visualization of parallel algorithms in an effort to educate broad audiences as to the beauty, complexity, and importance of parallel computation. The algorithms and interfaces were implemented by students from various related courses at VA Tech. We describe these designs in sufficient detail to enable others to build their own kinetic computing sculptures to augment their experiential learning programs. Our evaluations at exhibitions indicate 63% and 84% of visitors enjoyed interacting with SeeMore while 69% and 87% believed SeeMore has educational value. [ABSTRACT FROM AUTHOR] Copyright of Journal of Parallel & Distributed Computing is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122841763&site=ehost-live
373,SeeMore: An Interactive Kinetic Sculpture Designed to Teach Parallel Computational Thinking.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,14.0,116026163,,ASEE,Article,COMPUTER systems; KINETIC sculpture; ABSTRACT sculpture; PARALLEL programs (Computer programs); SCIENTISTS; EDUCATION; All Other Miscellaneous Schools and Instruction; Educational Support Services; Administration of Education Programs; Computer systems design and related services (except video game design and development); Computer Systems Design Services,,"Parallel computing is generally perceived to be difficult topic to understand and learn. This paper presents a design case study that was conceptualized and implemented to introduce non-computational savvy audience (e.g. students, K-12, senators, elderly, etc.) to the concepts of parallel computational thinking. In order to visualize and better comprehend the data transmission mechanism and algorithmic patterns of parallel computing, a kinetic computing sculpture comprising of a functional cluster of Raspberry Pi computers has been built by an interdisciplinary group of researchers. To evaluate users' learning experience, a focus group interview with high school students was conducted. Data from the study reveal that students found the sculpture an engaging and effective visual artifact that illustrated parallel computing patterns. After interacting with the sculpture, the participants were able to explain parallel computing to other participants and correctly answer all assessment related questions. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116026163&site=ehost-live
374,Student Autonomy: Implications of Design-Based Informal Learning Experiences in Engineering.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2014,,,1,12.0,115955926,,ASEE,Article,NONFORMAL education research; ENGINEERING student research; INSTRUCTIONAL innovations; EXPERIMENTAL methods in education; ENGINEERING design education in universities & colleges; Engineering Services,,"As part of their college-based undergraduate degree experience, a large portion of engineering students are involved in different informal learning experiences, such as co-curricular design teams, student organizations, and undergraduate research. The purpose of this qualitative study was to better understand engineering students' learning experiences in informal learning sites, particularly their sense of autonomy, which emerged as a major theme in initial data analysis. Specifically, this study investigates a hands-on design and manufacturing laboratory for engineering students in a large research and state institution, which is home to student engineering design teams, such as a Formula design team. We found that these experiences enhanced students' self-directed autonomy and allowed them to take control of their learning trajectory. We discuss implications for future research and educational practices. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the ASEE Annual Conference & Exposition is the property of ASEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=115955926&site=ehost-live
375,Student Experiences In An Interdisciplinary Studio-Based Design Course: The Role Of Peer Scaffolding.,Aditya Johri,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2014,,,1,14.0,115955922,,ASEE,Article,TEACHING methods research; INDUSTRIAL design education; ARCHITECTURAL design education; HIGHER education; TECHNICAL education; ARCHITECTURAL studios; ENGINEERING student research,,The article discusses a study which examined the role of peer scaffoldng in the investigation of engineering students' perception of an interdisciplinary studio-based architecture and industrial design course. The operationalized peer scaffolding attributes used in the study are presented including shared understanding and fading support. Information is also presented on student studio orientation and students' studio design experience.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=115955922&site=ehost-live
376,The Cambridge Handbook of Engineering Education Research and Reflections on the Future of the Field.,Aditya Johri,Journal of Engineering Education,10694730,,Jul2014,103,3,363,6.0,96924099,10.1002/jee.20047,Wiley-Blackwell,Editorial,"ENGINEERING education; NONFICTION; CAMBRIDGE Handbook of Engineering Education Research (Book); JOHRI, Aditya; OLDS, Barbara M.",,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=96924099&site=ehost-live
377,Uses and Gratifications of Pokémon Go: Why do People Play Mobile Location-Based Augmented Reality Games?,Aditya Johri,International Journal of Human-Computer Interaction,10447318,,2019,35,9,804,16.0,135932593,10.1080/10447318.2018.1497115,Taylor & Francis Ltd,Article,POKEMON Go; AUGMENTED reality; MOBILE games; MOBILE apps; HUMAN-computer interaction; Software Publishers,Augmented reality; freemium; gamification; location-based games; Uses and Gratifications,"In recent years, augmented reality games (ARGs) such as Pokémon Go have become increasingly popular. These games not only afford a novel gaming experience but also have the potential to alter how players view their physical realities. In addition to the common experiences and gratifications people derive from games, (location-based) ARGs can afford, for example outdoor adventures, communal activities, and health benefits, but also create problems stemming from, for example privacy concerns and poor usability. This raises some important research questions as to what drives people to use these new applications, and why they may be willing to spend money on the content sold within them. In this study, we investigate the various gratifications people derive from ARGs (Pokémon Go) and the relationship of these gratifications with the players' intentions to continue playing and spending money on them. We employ data drawn from players of Pokémon Go (N = 1190) gathered through an online survey. The results indicate that game enjoyment, outdoor activity, ease of use, challenge, and nostalgia are positively associated with intentions to reuse (ITR), meanwhile outdoor activity, challenge, competition, socializing, nostalgia and ITR are associated with in-app purchase intentions (IPI). In contrast with our expectations, privacy concerns or trendiness were not associated with reuse intentions or IPI. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Human-Computer Interaction is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135932593&site=ehost-live
378,Cryptography on a Speck of Dust.,Jens-Peter Kaps,Computer (00189162),00189162,,Feb2007,40,2,38,7.0,24104773,10.1109/MC.2007.52,IEEE,Article,"RADIO frequency identification systems; WIRELESS communications security; COMPUTER programming; ALGORITHMS; COMPUTER security software; DATA encryption; DATA transmission system security measures; PERSONAL communication service systems; CRYPTOGRAPHY; COMPUTER software; SECURITY systems; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Software publishers (except video game publishers); Wireless Telecommunications Carriers (except Satellite); Custom Computer Programming Services; Computer systems design and related services (except video game design and development); Other Computer Related Services; Electronic components, navigational and communications equipment and supplies merchant wholesalers; Security Systems Services (except Locksmiths); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing",,"The article discusses the security problems with wireless sensor networks (WSNs) and radio frequency identification (RFID) devices whose applications range from supply-chain management to home automation and health care. Since these devices are extremely small and have limited power, the task of applying security requires cryptographic services that can perform on a minuscule sized area. The authors give an overview of current cryptographic algorithms and then apply those ideas on a extremely small level. The article continues by giving design recommendations for new algorithms. INSET: Ultralow-Power Application Domain.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24104773&site=ehost-live
379,Implementation of efficient SR-Latch PUF on FPGA and SoC devices.,Jens-Peter Kaps,Microprocessors & Microsystems,01419331,,Aug2017,53,,92,14.0,124935137,10.1016/j.micpro.2017.07.006,Elsevier B.V.,Article,FIELD programmable gate arrays; SYSTEMS on a chip; CRYPTOGRAPHY; OSCILLATIONS; RELIABILITY in engineering; Semiconductor and Related Device Manufacturing,Key generation; Metastability; PUF; SR-Latch; Xilinx FPGAs; Zynq SoC,"In this paper we present a reliable and efficient SR-Latch based PUF design, with two times improvement in area over the state of the art, thus making it very attractive for low-area designs. This PUF is able to reliably generate a cryptographic key. The PUF response is generated by quantifying the number of oscillations during the metastability state for preselected latches. The derived design has been verified on 25 Xilinx Spartan-6 FPGAs (XC6SLX16) and 10 Xilinx Zynq SoC (XC7Z010) devices. The design exhibited ∼49% uniqueness figures when tested on both types of FPGAs. The reliability figures were >94% for temperature variation (0–85 °C) and ±5% of core voltage variation. [ABSTRACT FROM AUTHOR] Copyright of Microprocessors & Microsystems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124935137&site=ehost-live
380,DEVELOPING KNOWLEDGE-BASED SYSTEMS: REORGANIZING THE SYSTEM DEVELOPMENT LIFE CYCLE.,Larry Kerschberg,Communications of the ACM,00010782,,Apr89,32,4,482,7.0,5247997,10.1145/63334.63340,Association for Computing Machinery,Article,"COMPUTER systems; TRANSACTION systems (Computer systems); DECISION support systems; EXPERT systems; DATA mining; ARTIFICIAL intelligence; Data Processing, Hosting, and Related Services; Computer systems design and related services (except video game design and development); Computer Systems Design Services",Software development,"This article describes a Knowledge-Based-System Developmental Life Cycle that shows what must be changed and retained from conventional Synchronous Data Link Controls. Building computer-based information systems involves some basic tasks such as, problem detection, identification, and definition, solution definition, system analysis, logical and physical system design, procedure and program design, procedure and program writing, program testing, integrated testing, conversion and installation, and operation. The organization of these tasks may change, but the tasks still must be performed. Transaction processing systems (TPS), decision support systems and knowledge-based systems offer different development challenges. TPSs perform routine data processing, are designed around forms, procedures, inputs, and outputs, and often address well-structured problems. System development life cycles (SDLCs) originated when most systems were TPSs. Many organizations execute SDLC phases sequentially, with a sign-off after each phase, an approach that is suitable for many TPSs.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=5247997&site=ehost-live
381,The Role of Context in Social Semantic Search and Decision Making.,Larry Kerschberg,International Journal on Artificial Intelligence Tools,02182130,,Dec2014,23,6,-1,4.0,100100344,10.1142/S0218213014600227,World Scientific Publishing Company,Article,SEMANTICS; SEARCH engines; DECISION making; COMPUTER users; ONTOLOGIES (Information retrieval); RECOMMENDER systems,Context; search,"This paper is based on my Keynote Address at the Tools with AI conference on November 4, 2013. The talk focussed on some factors used to determine a user's context - those attributes, both tacit and explicit, which help to ascertain a user's intensions for a search request in order to make a decision. I also explored how social semantic search can assist in guiding the decision process, especially when the decision is of a personal nature, for example, in decisions involving health care, where there may be a number of avenues to pursue. I focussed on our patented-technology exemplified by the Knowledge Sifter system, and an application called Personal Health Explorer (PHE), a semantic recommender system that allows an individual to perform semantic search and discovery related to conditions and diseases contained in his personal health record (PHR). The PHE system consults authoritative ontologies and reputable information sources to provide semantically enhanced authoritative recommendations that can be stored in the individual's (PHR) for further research and consultation. The PHE is implemented using the Knowledge Sifter agent-based framework and Microsoft's HealthVault. [ABSTRACT FROM AUTHOR] Copyright of International Journal on Artificial Intelligence Tools is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100100344&site=ehost-live
383,PICAR: An Efficient Extendable Approach for Fitting Hierarchical Spatial Models.,Ben Seiyon Lee,Technometrics,00401706,,May2022,64,2,187,12.0,156475978,10.1080/00401706.2021.1933596,Taylor & Francis Ltd,Article,MARKOV chain Monte Carlo; MARKOV processes; ATMOSPHERIC sciences; CLOUDINESS; LATENT variables; RANDOM fields; MARKOV random fields,Basis representation; Gaussian random field; Markov chain Monte Carlo; Non-Gaussian spatial data; Ordinal spatial data; Spatially varying coefficients,"Hierarchical spatial models are very flexible and popular for a vast array of applications in areas such as ecology, social science, public health, and atmospheric science. It is common to carry out Bayesian inference for these models via Markov chain Monte Carlo (MCMC). Each iteration of the MCMC algorithm is computationally expensive due to costly matrix operations. In addition, the MCMC algorithm needs to be run for more iterations because the strong cross-correlations among the spatial latent variables result in slow mixing Markov chains. To address these computational challenges, we propose a projection-based intrinsic conditional autoregression (PICAR) approach, which is a discretized and dimension-reduced representation of the underlying spatial random field using empirical basis functions on a triangular mesh. Our approach exhibits fast mixing as well as a considerable reduction in computational cost per iteration. PICAR is computationally efficient and scales well to high dimensions. It is also automated and easy to implement for a wide array of user-specified hierarchical spatial models. We show, via simulation studies, that our approach performs well in terms of parameter inference and prediction. We provide several examples to illustrate the applicability of our method, including (i) a high-dimensional cloud cover dataset that showcases its computational efficiency, (ii) a spatially varying coefficient model that demonstrates the ease of implementation of PICAR in the probabilistic programming languages stan and nimble, and (iii) a watershed survey example that illustrates how PICAR applies to models that are not amenable to efficient inference via existing methods. [ABSTRACT FROM AUTHOR] Copyright of Technometrics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156475978&site=ehost-live
384,Delay of routine health care during the COVID-19 pandemic: A theoretical model of individuals' risk assessment and decision making.,Myeong Lee,Social Science & Medicine,02779536,,Aug2022,307,,N.PAG,1.0,158369630,10.1016/j.socscimed.2022.115164,Elsevier B.V.,Article,COMMUNICABLE diseases; MATHEMATICAL models; RESEARCH methodology; MEDICAL care; INTERVIEWING; UNCERTAINTY; RISK assessment; DECISION making; THEORY; COVID-19 pandemic; DISEASE management; UNITED States,COVID-19; Decision making; Delayed health care; Risk assessment,"Delaying routine health care has been prevalent during the COIVD-19 pandemic. Macro-level data from this period reveals that U.S. patients under-utilized routine health care services such as primary care visits, preventative tests, screenings, routine optometry care, dental appointments, and visits for chronic disease management. Yet, there is a gap in research on how and why patients understand risks associated with seeking or delaying routing health care during an infectious disease pandemic. Our research addresses this gap based on semi-structured interviews with 40 participants living in regions across the United States. By building upon Unger-Saldaña and Infante-Castañeda's model of delayed health care, we extend this model by articulating how health care delays happen during an infectious disease pandemic. Specifically, we show how perceptions of uncertainty and subjective risk assessments shape people's decisions to delay routine health care while they operate at two levels, internal and external to one's social bubble, interacting with each other. • During COVID-19, people delayed their routine health care due to various risks. • Interplays between internal and external factors lead to risk assessment. • One's perception and assessment of uncertainty and risk lead to health care delay. • The availability of alternative health care may moderate one's risk assessment. • We propose a new model of decision-making to delay health care. [ABSTRACT FROM AUTHOR] Copyright of Social Science & Medicine is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158369630&site=ehost-live
390,A comprehensive study of an online packet scheduling algorithm.,Fei Li,Theoretical Computer Science,03043975,,Jul2013,497,,31,8.0,89510870,10.1016/j.tcs.2012.06.002,Elsevier B.V.,Article,"DATA packeting; COMPUTER algorithms; COMPUTER scheduling; DISCRETE systems; BUFFER storage (Computer science); COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Buffer management; Competitive analysis; Online algorithm; Packet scheduling,"Abstract: We study the bounded-delay model for Qualify-of-Service buffer management. Time is discrete. There is a buffer. Unit-length jobs (also called packets) arrive at the buffer over time. Each packet has an integer release time, an integer deadline, and a positive real value. A packet’s characteristics are not known to an online algorithm until the packet actually arrives. In each time step, at most one packet can be sent out of the buffer. The objective is to maximize the total value of the packets sent by their respective deadlines in an online manner. An online algorithm’s performance is usually measured in terms of competitive ratio, when this online algorithm is compared with a clairvoyant algorithm achieving the maximum total value. In this paper, we study a simple and intuitive online algorithm. We analyze its performance in terms of competitive ratio for the general model and a few important variants. [Copyright &y& Elsevier] Copyright of Theoretical Computer Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89510870&site=ehost-live
391,A near-optimal memoryless online algorithm for FIFO buffering two packet classes.,Fei Li,Theoretical Computer Science,03043975,,Jul2013,497,,164,9.0,89510881,10.1016/j.tcs.2011.11.039,Elsevier B.V.,Article,"ONLINE algorithms; FIRST in, first out (Queuing theory); COMPUTER scheduling; DATA packeting; COMBINATORIAL optimization; COMPUTER networks; Computer Systems Design Services",Buffer management; Competitive analysis; Online algorithm; Packet scheduling,"Abstract: We consider scheduling packets with values in a capacity-bounded buffer in an online setting. In this model, there is a buffer with limited capacity . At any time, the buffer cannot accommodate more than packets. Packets arrive over time. Each packet has a non-negative value. Packets leave the buffer only because they are either sent or dropped. Those packets that have left the buffer will not be reconsidered for delivery any more. In each time step, at most one packet in the buffer can be sent. The order in which the packets are sent should comply with the order of their arrival time. The objective is to maximize the total value of the packets sent in an online manner. In this paper, we study a variant of this FIFO buffering model in which a packet’s value is either 1 or . We present a deterministic memoryless 1.304-competitive algorithm. This algorithm has the same competitive ratio as the one presented in Lotker and Patt-Shamir [Z. Lotker, B. Patt-Shamir, Nearly optimal FIFO buffer management for DiffServ, in: Proceedings of the 21st Annual ACM Symposium on Principles of Distributed Computing, PODC, 2002, pp. 134–142; Z. Lotker, B. Patt-Shamir, Nearly optimal FIFO buffer management for DiffServ, Computer Networks 17 (1) (2003) 77–89]. However, our algorithm is simpler and does not employ any marking bits. The idea used in our algorithm is novel and different from all previous approaches that have been applied for the general model and its variants. We do not proactively preempt one packet when a new packet arrives. Instead, we may preempt more than one 1-value packet at the time when the buffer contains sufficiently many -value packets. [Copyright &y& Elsevier] Copyright of Theoretical Computer Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89510881&site=ehost-live
392,Online packet scheduling with bounded delay and lookahead.,Fei Li,Theoretical Computer Science,03043975,,Jul2019,776,,95,19.0,136444306,10.1016/j.tcs.2019.01.013,Elsevier B.V.,Article,ONLINE algorithms; INTEGERS,Buffer management; Lookahead; Online algorithm; Online scheduling,"We study the online bounded-delay packet scheduling problem (PacketScheduling) , where packets of unit size arrive at a router over time and need to be transmitted over a network link. Each packet has two attributes: a non-negative weight and a deadline for its transmission. The objective is to maximize the total weight of the transmitted packets. This problem has been well studied in the literature; yet currently the best published upper bound is 1.828 [8] , still quite far from the best lower bound of ϕ ≈ 1.618 [11,2,6]. In the variant of PacketScheduling with s-bounded instances , each packet can be scheduled in at most s consecutive slots, starting at its release time. The lower bound of ϕ applies even to the special case of 2-bounded instances, and a ϕ -competitive algorithm for 3-bounded instances was given in [5]. Improving that result, and addressing a question posed by Goldwasser [9] , we present a ϕ -competitive algorithm for 4 -bounded instances. We also study a variant of PacketScheduling where an online algorithm has the additional power of 1-lookahead , knowing at time t which packets will arrive at time t + 1. For PacketScheduling with 1-lookahead restricted to 2-bounded instances, we present an online algorithm with competitive ratio 1 2 (13 − 1) ≈ 1.303 and we prove a nearly tight lower bound of 1 4 (1 + 17) ≈ 1.281. In fact, our lower bound result is more general: using only 2-bounded instances, for any integer ℓ ≥ 0 we prove a lower bound of 1 2 (ℓ + 1) (1 + 5 + 8 ℓ + 4 ℓ 2 ) for online algorithms with ℓ -lookahead, i.e., algorithms that at time t can see all packets arriving by time t + ℓ. Finally, for non-restricted instances we show a lower bound of 1.25 for randomized algorithms with ℓ -lookahead, for any ℓ ≥ 0. [ABSTRACT FROM AUTHOR] Copyright of Theoretical Computer Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136444306&site=ehost-live
393,Anisotropic thermoelectric behavior in armchair and zigzag mono- and fewlayer MoS2 in thermoelectric generator applications.,Qiliang Li,Scientific Reports,20452322,,9/4/2015,,,13706,1.0,109303311,10.1038/srep13706,Springer Nature,Article,ARMCHAIRS; THERMOELECTRIC generators; DENSITY functional theory; GREEN'S functions; NONEQUILIBRIUM flow; PHONON spectra,,"In this work, we have studied thermoelectric properties of monolayer and fewlayer MoS2 in both armchair and zigzag orientations. Density functional theory (DFT) using non-equilibrium Green's function (NEGF) method has been implemented to calculate the transmission spectra of mono- and fewlayer MoS2 in armchair and zigzag directions. Phonon transmission spectra are calculated based on parameterization of Stillinger-Weber potential. Thermoelectric figure of merit, ZT, is calculated using these electronic and phonon transmission spectra. In general, a thermoelectric generator is composed of thermocouples made of both n-type and p-type legs. Based on our calculations, monolayer MoS2 in armchair orientation is found to have the highest ZT value for both p-type and n-type legs compared to all other armchair and zigzag structures. We have proposed a thermoelectric generator based on monolayer MoS2 in armchair orientation. Moreover, we have studied the effect of various dopant species on thermoelectric current of our proposed generator. Further, we have compared output current of our proposed generator with those of Silicon thin films. Results indicate that thermoelectric current of MoS2 armchair monolayer is several orders of magnitude higher than that of Silicon thin films. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109303311&site=ehost-live
394,Autonomous Visual Perception for Unmanned Surface Vehicle Navigation in an Unknown Environment.,Qiliang Li,Sensors (14248220),14248220,,May2019,19,10,2216,1.0,136675160,10.3390/s19102216,MDPI,Article,AUTONOMOUS vehicles; COMPUTER vision; PATTERN recognition systems; IMAGE processing; ROBUST control; One-Hour Photofinishing; Photofinishing Laboratories (except One-Hour),deep-learning; recognition; unmanned surface vehicles; vision; water region,"Robust detection and recognition of water surfaces are critical for autonomous navigation of unmanned surface vehicles (USVs), since any none-water region is likely an obstacle posing a potential danger to the sailing vehicle. A novel water region visual detection method is proposed in this paper. First, the input image pixels are clustered into different regions and each pixel is assigned a label tag and a confidence value by adaptive multistage segmentation algorithm. Then the resulting label map and associated confidence map are fed into a convolutional neural network (CNN) as training samples to train the network online. Finally, the online trained CNN is used to segment the input image again but with greater precision and stronger robustness. Compared with other deep-learning image segmentation algorithms, the proposed method has two advantages. Firstly, it dispenses with the need of manual labeling training samples which is a costly and painful task. Secondly, it allows real-time online training for CNN, making the network adaptive to the navigational environment. Another contribution of this work relates to the training process of neuro network. An effective network training method is designed to learn from the imperfect training data. We present the experiments in the lake with a various scene and demonstrate that our proposed method could be applied to recognize the water region in the unknown navigation environment automatically. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136675160&site=ehost-live
395,Enhanced energy storage performance and thermal stability in relaxor ferroelectric (1‐x)BiFeO3‐x(0.85BaTiO3‐0.15Bi(Sn0.5Zn0.5)O3) ceramics.,Qiliang Li,Journal of the American Ceramic Society,00027820,,Jun2021,104,6,2646,9.0,149598200,10.1111/jace.17705,Wiley-Blackwell,Article,HEAT storage; RELAXOR ferroelectrics; THERMAL stability; ENERGY storage; ELECTRIC discharges; ELECTRIC breakdown,BiFeO3; energy‐storage density; relaxor ferroelectric; solid phase sintering,"Lead‐free (1‐x)BiFeO3‐x(0.85BaTiO3‐0.15Bi(Sn0.5Zn0.5)O3) [(1‐x)BF‐x(BT‐BSZ), x=0.45‐0.7] ceramic samples were prepared by solid phase sintering. It is revealed that the pure single‐phase perovskite structure can be obtained in samples with x ≥ 0.6. With increasing x, the measured ferroelectric hysteresis loop becomes gradually slimmed in accompanying with reduced remnant polarization, and a clear ferroelectric‐relaxor transition at x = 0.65 is identified. Furthermore, the measured electric breakdown strength can be significantly enhanced with increasing x, and the optimal energy storage performance is achieved at x = 0.65, characterized by the recoverable energy storage density up to ≈3.06 J/cm3 and energy storage efficiency as high as ≈92 %. Excellent temperature stability (25°C–110°C) and fatigue endurance (>105 cycles) for energy storage are demonstrated. Our results suggest that the BF‐based relaxor ceramics can be tailored for promising applications in high energy storage devices. [ABSTRACT FROM AUTHOR] Copyright of Journal of the American Ceramic Society is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149598200&site=ehost-live
396,High energy storage performances of Bi1−xSmxFe0.95Sc0.05O3 lead-free ceramics synthesized by rapid hot press sintering.,Qiliang Li,Journal of the European Ceramic Society,09552219,,Jul2019,39,7,2331,8.0,135228187,10.1016/j.jeurceramsoc.2019.02.009,Elsevier B.V.,Article,LEAD-free ceramics; ENERGY storage; BISMUTH compounds; HOT pressing; SINTERING; CHEMICAL synthesis; Iron Ore Mining,BiFeO3; Energy-storage density; Hot press sintering; Lead-free ceramics,"Abstract Lead-free Bi 1−x Sm x Fe 0.95 Sc 0.05 O 3 (x = 0.15–0.19) ceramics were fabricated by rapid hot press sintering, and their structure, ferroelectric and energy storage properties were comprehensively investigated. All the samples are in the mixed phases with R3c rhombohedral and Pbnm orthorhombic structures. With increasing x , the ferroelectric polarization decreases gradually, while the polarization loop becomes gradually slimed too. An high recoverable energy density (˜2.21 J/cm3) and a large efficiency (˜76%) with good thermal stability (20 °C–120 °C) are obtained under electric field (230 kV/cm) for the optimized sample x = 0.17. Moreover, transmission electron microscopy and piezo-response force microscopy measurements reveal that the presence of two-phase coexistence favors the formation of polar nano-regions, leading to the linear-towards polarization behaviors and the enhanced dielectric breakdown field, which is responsible for the superior energy storage performance of Bi 1−x Sm x Fe 0.95 Sc 0.05 O 3 ceramics. These results indicate a significant step to tailor lead-free BiFeO 3 -based ceramics towards high dielectric energy storage applications. [ABSTRACT FROM AUTHOR] Copyright of Journal of the European Ceramic Society is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135228187&site=ehost-live
397,High-Performance Nonequilibrium InSb PIN Infrared Photodetectors.,Qiliang Li,IEEE Transactions on Electron Devices,00189383,,Mar2019,66,3,1361,7.0,136509782,10.1109/TED.2019.2895032,IEEE,Article,PIN photodiodes; PHOTODETECTORS; CURRENT density (Electromagnetism); DARK currents (Electric); COOLING,Auger suppression; Cooling; Current density; Dark current; Detectors; Doping; infrared (IR) detectors; InSb; Photodetectors; Pins,"The cooling requirement is still a great burden of sensitive infrared (IR) detectors. To overcome this challenge, a detailed investigation of InSb P-intrinsic-N diodes was conducted. Its dark current was comprehensively studied for the design and optimization of high-performance nonequilibrium IR detectors. The doping concentration, exclusion junction, and layer geometry were engineered to lower the Auger suppression trigger current density ${J}_{\text {onset}}$ , leading to a low dark saturation current ${J}_{\text {sat}}$. The result indicated that the Auger suppression can be triggered at ${J}_{\text {onset}}={15}$ A/cm2 and ${V}_{\text {onset}} = {0.8}$ V, while ${J}_{\text {sat}}$ can be kept as low as 10 A/cm2. The nature of Auger suppression has been studied by calculating the carrier concentration profile in the diode with different bias voltages. It is proven that the exclusion diode has the most significant effect on the Auger suppression. The specific detectivity of optimized nonequilibrium InSb detectors at different temperatures has been compared favorably with the results reported by other groups. Such a nonequilibrium InSb IR detector has a high potential in room temperature and above application. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Electron Devices is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136509782&site=ehost-live
398,High-performance room-temperature TiO2-functionalized GaN nanowire gas sensors.,Qiliang Li,Applied Physics Letters,00036951,,9/16/2019,115,12,N.PAG,5.0,138756850,10.1063/1.5116677,American Institute of Physics,Article,GALLIUM nitride; N-type semiconductors; DETECTORS; NANOFABRICATION; GASES; TRANSDUCERS; NANOWIRE devices; NANOWIRES; Semiconductor and other electronic component manufacturing; Other Electronic Component Manufacturing,,"Hybrid gas sensors based on TiO2 functionalized gallium nitride nanowires have been prepared by nanofabrication and comprehensively studied for high-responsivity applications. The devices exhibited a high responsivity (25%) to 500 ppm NO2 assisted with ultraviolet illumination at room temperature. The thickness and doping concentration of TiO2 were engineered to improve the transducer function. The result indicated that an excellent n-type response can be stably obtained for a doping range from 1 × 1017 cm−3 to 1 × 1019 cm−3. The TiO2 thickness and doping concentration can be further fine-tuned to achieve optimal performance. In addition, a comprehensive device simulation was carried out to understand the device operation and gain insight for optimizing the device performance. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138756850&site=ehost-live
400,Methods to Characterize the Electrical and Mechanical Properties of Si Nanowires.,Qiliang Li,AIP Conference Proceedings,0094243X,,9/26/2007,931,1,457,5.0,26887919,10.1063/1.2799417,American Institute of Physics,Article,"MECHANICAL behavior of materials; NANOTUBES; NANOWIRES; METROLOGY; ELECTROMECHANICAL devices; SEMICONDUCTORS; Semiconductor and other electronic component manufacturing; Totalizing Fluid Meter and Counting Device Manufacturing; Automatic Environmental Control Manufacturing for Residential, Commercial, and Appliance Use; Other Electronic Component Manufacturing; Motor and Generator Manufacturing; Other Communications Equipment Manufacturing; Semiconductor and Related Device Manufacturing",Nanoelectromechanical System; Nanowire; Non-volatile memory; Transfer Length Method,"We report metrology methods to characterize nanowires. In this work, representative devices and test structures, including nanoelectromechanical switches, non-volatile nanowire memory devices with SONOS structure, and both transfer-length-method and Kelvin test structures, have been developed to investigate the electrical and mechanical properties of the silicon nanowires. These methods and test structures can be readily applied to other (non-Si) semiconductor nanowires/nanotubes. [ABSTRACT FROM AUTHOR] Copyright of AIP Conference Proceedings is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=26887919&site=ehost-live
401,New families of large band gap 2D topological insulators in ethynyl-derivative functionalized compounds.,Qiliang Li,Applied Surface Science,01694332,,Aug2019,484,,1208,6.0,139234583,10.1016/j.apsusc.2019.04.071,Elsevier B.V.,Article,TOPOLOGICAL insulators; HIGH temperatures; TOPOLOGICAL property; BISMUTH telluride; LOW temperatures; FAMILIES,Edge states; Functionalized; Spin orbital coupling (SOC); Topological insulators; Two-dimensional materials,"The search for large band gap systems with dissipationless edge states is essential to developing materials that function under a wide range of temperatures. Two-dimensional (2D) topological insulators (TIs) have recently attracted significant attention due to their dissipationless transport, robust properties and excellent compatibility with device integration. However, a major barrier of 2D TIs is their small bulk band gap, which allows for applications only in extremely low temperatures. In this work, first principle calculations were used to analyze the geometric, electronic, and topological properties of PbC 2 X and BiC 2 X (X = H, Cl, F, Br, I) compounds. The band gap values are remarkably large, ranging from 0.79 eV to 0.99 eV. The nanoribbons of these compounds exhibited nontrivial topological order in the simulation, thus proving ethynyl-derivative functionalized Pb and Bi films to be new classes of giant band gap 2D TIs. In addition, these findings indicate that chemical functionalization with ethynyl-derivatives is an effective method to tune the band gap and preserve the nontrivial topological order. These novel materials that are applicable at both room temperature and high temperatures open the door to a new generation of electronics. (a) Calculated zigzag nanoribbon band structure of a 2D BiC 2 I monolayer. (b) The zigzag nanoribbon geometric structure that was used to calculate the edge states and current vs. bias characteristics. Unlabelled Image [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139234583&site=ehost-live
402,Nonvolatile memory based on redox-active ruthenium molecular monolayers.,Qiliang Li,Applied Physics Letters,00036951,,10/14/2019,115,16,N.PAG,5.0,139218620,10.1063/1.5108675,American Institute of Physics,Article,NONVOLATILE memory; RUTHENIUM; COMPUTER storage devices; X-ray photoelectron spectroscopy; SILICON oxide; RANDOM access memory; MONOMOLECULAR films; Computer and peripheral equipment manufacturing; Computer Storage Device Manufacturing,,"A monolayer of diruthenium molecules was self-assembled onto the silicon oxide surface in a semiconductor capacitor structure with a ""click"" reaction for nonvolatile memory applications. The attachment of the active molecular monolayer was verified by x-ray photoelectron spectroscopy. The prototypical capacitor memory devices in this work employed a metal/oxide/molecule/oxide/Si structure. With the intrinsic redox-active charge-storage properties of diruthenium molecules, these capacitor memory devices exhibited fast Program and Erase speed, excellent endurance performance with negligible degradation of the memory window after 105 program/erase cycles, and very good 10-year memory retention. These experimental results indicate that the redox-active ruthenium molecular memory is very promising for use in nonvolatile memory applications. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139218620&site=ehost-live
403,Observation and control of the anomalous Aharonov-Bohm oscillation in enhanced-mode topological insulator nanowire field-effect transistors.,Qiliang Li,Applied Physics Letters,00036951,,8/12/2019,115,7,N.PAG,5.0,138145238,10.1063/1.5111180,American Institute of Physics,Article,TOPOLOGICAL insulators; FIELD-effect transistors; OSCILLATIONS; QUANTUM interference; SILICON nanowires; SEMICONDUCTOR nanowires; WAVE functions,,"Aharonov-Bohm (AB) oscillation is a quantum mechanical phenomenon which reveals the coupling of electromagnetic potentials with the electron wave function, affecting the phase of the wave function. Such a quantum interference effect can be demonstrated through the magnetotransport measurement focusing on low-dimensional electronic states. Here, we report the experimental observation of anomalous AB oscillation in an enhanced-mode topological insulator Bi2Se3 nanowire field-effect transistor (FET) under strong surface disorder, which is different from the reported AB oscillation in topological insulator nanostructures. The surrounding gate of the nanowire FET gives rise to tunability of the chemical potential and introduces strong disorder on the surface states, leading to primary oscillation with an anomalous h/e period. Furthermore, the oscillation exhibits a significant dependence on the gate voltage which has been preliminary explained with the quantization of the surface conduction channel. The experimental demonstration can be very attractive for further exploration of quantum phase interference through electrical approaches, enabling applications in future information and electromagnetic sensing technology. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138145238&site=ehost-live
404,Precise gas discrimination with cross-reactive graphene and metal oxide sensor arrays.,Qiliang Li,Applied Physics Letters,00036951,,11/26/2018,113,22,N.PAG,4.0,133317637,10.1063/1.5063375,American Institute of Physics,Article,"METAL oxide semiconductors; GAS detectors; GRAPHENE; DETECTORS; INTERNET of things; Professional machinery, equipment and supplies merchant wholesalers",,"Discriminating similar molecules remains a very challenging problem for semiconductor gas sensors. Here, we report a method to achieve precise gas discrimination of similar chemical vapors (mesitylene, o-xylene, and toluene) by using cross-reactive arrays consisting of metal oxide semiconductor and graphene sensors. It is difficult to identify these three chemicals as they have very similar responses to these sensors. Through cross-reactive Principal Component Analysis of the sensor response features, however, the discrimination accuracy improved from about 70% with a single gas sensor to almost 100% with the cross-reactive sensor array. Such a precise discrimination and the low-cost planar process make this approach a very attractive candidate for smart gas sensing and for future Internet of Things applications. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133317637&site=ehost-live
405,"Recent Advances in Electrochemical Sensors for Detecting Toxic Gases: NO2, SO2 and H2S.",Qiliang Li,Sensors (14248220),14248220,,Feb2019,19,4,905,1.0,135038022,10.3390/s19040905,MDPI,Article,NITRIC oxide; GAS mixtures; ELECTROCHEMICAL sensors; GRAPHENE; SENSITIVITY analysis,density-functional theory (DFT); gas sensor; hydrogen sulfide (H2S); Internet of Things (IoT); nitrogen dioxide (NO2); response/recovery time; sensitivity; sulphur dioxide (SO2),"Toxic gases, such as NOx, SOx, H2S and other S-containing gases, cause numerous harmful effects on human health even at very low gas concentrations. Reliable detection of various gases in low concentration is mandatory in the fields such as industrial plants, environmental monitoring, air quality assurance, automotive technologies and so on. In this paper, the recent advances in electrochemical sensors for toxic gas detections were reviewed and summarized with a focus on NO2, SO2 and H2S gas sensors. The recent progress of the detection of each of these toxic gases was categorized by the highly explored sensing materials over the past few decades. The important sensing performance parameters like sensitivity/response, response and recovery times at certain gas concentration and operating temperature for different sensor materials and structures have been summarized and tabulated to provide a thorough performance comparison. A novel metric, sensitivity per ppm/response time ratio has been calculated for each sensor in order to compare the overall sensing performance on the same reference. It is found that hybrid materials-based sensors exhibit the highest average ratio for NO2 gas sensing, whereas GaN and metal-oxide based sensors possess the highest ratio for SO2 and H2S gas sensing, respectively. Recently, significant research efforts have been made exploring new sensor materials, such as graphene and its derivatives, transition metal dichalcogenides (TMDs), GaN, metal-metal oxide nanostructures, solid electrolytes and organic materials to detect the above-mentioned toxic gases. In addition, the contemporary progress in SO2 gas sensors based on zeolite and paper and H2S gas sensors based on colorimetric and metal-organic framework (MOF) structures have also been reviewed. Finally, this work reviewed the recent first principle studies on the interaction between gas molecules and novel promising materials like arsenene, borophene, blue phosphorene, GeSe monolayer and germanene. The goal is to understand the surface interaction mechanism. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135038022&site=ehost-live
406,Recent Progress in Smart Electronic Nose Technologies Enabled with Machine Learning Methods.,Qiliang Li,Sensors (14248220),14248220,,Nov2021,21,22,7620,1.0,153873885,10.3390/s21227620,MDPI,Article,"MACHINE learning; GAS detectors; ELECTRONIC noses; FEATURE extraction; DIAGNOSIS; ROBOTICS; Professional machinery, equipment and supplies merchant wholesalers",electronic nose; gas sensor array; machine learning; neural networks; review,"Machine learning methods enable the electronic nose (E-Nose) for precise odor identification with both qualitative and quantitative analysis. Advanced machine learning methods are crucial for the E-Nose to gain high performance and strengthen its capability in many applications, including robotics, food engineering, environment monitoring, and medical diagnosis. Recently, many machine learning techniques have been studied, developed, and integrated into feature extraction, modeling, and gas sensor drift compensation. The purpose of feature extraction is to keep robust pattern information in raw signals while removing redundancy and noise. With the extracted feature, a proper modeling method can effectively use the information for prediction. In addition, drift compensation is adopted to relieve the model accuracy degradation due to the gas sensor drifting. These recent advances have significantly promoted the prediction accuracy and stability of the E-Nose. This review is engaged to provide a summary of recent progress in advanced machine learning methods in E-Nose technologies and give an insight into new research directions in feature extraction, modeling, and sensor drift compensation. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153873885&site=ehost-live
407,Self-aligned multi-channel silicon nanowire field-effect transistors,Qiliang Li,Solid-State Electronics,00381101,,Dec2012,78,,92,5.0,80032482,10.1016/j.sse.2012.05.058,Elsevier B.V.,Article,SILICON nanowires; FIELD-effect transistors; GATE array circuits; PHOTOLITHOGRAPHY; PERFORMANCE evaluation; ELECTRON mobility; SCHOTTKY barrier,Multi-channel nanowire FET; Nanowire field-effect transistor; Self-alignment; Voltage tolerance,"Abstract: Si nanowire field effect transistors (SiNW FETs) with multiple nanowire channels and different gate lengths have been fabricated by using a directed assembly approach combined with a standard photolithographic process. The electrical characteristics of SiNW FETs containing different numbers of nanowire channels were measured and compared. The multi-channel SiNW FETs show excellent performance: small subthreshold slope (≈75mV/dec), large ON/OFF ratio (≈108), good break-down voltage (>30V) and good carrier mobility (μ p ≈100cm2 V−1s−1). These excellent device properties were achieved by using a clean self-alignment process and an improved device structure with Schottky barriers at the source and drain contacts. Such high-performance multi-nanowire FETs are attractive for logic, memory, and sensor applications. [Copyright &y& Elsevier] Copyright of Solid-State Electronics is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=80032482&site=ehost-live
408,Self-assembled nanowire array capacitors: capacitance and interface state profile.,Qiliang Li,Nanotechnology,09574484,,4/ 4/2014,25,13,135201,6.0,94772196,10.1088/0957-4484/25/13/135201,IOP Publishing,Article,"CAPACITORS; ELECTRIC capacity; SEMICONDUCTORS; ELECTRIC conductivity; SILICON nanowires; TRANSISTORS; Other Electronic Parts and Equipment Merchant Wholesalers; Capacitor, Resistor, Coil, Transformer, and Other Inductor Manufacturing; All Other Miscellaneous Electrical Equipment and Component Manufacturing; Electronic components, navigational and communications equipment and supplies merchant wholesalers; Semiconductor and Related Device Manufacturing; Semiconductor and other electronic component manufacturing",,"Direct characterization of the capacitance and interface states is very important for understanding the electronic properties of a nanowire transistor. However, the capacitance of a single nanowire is too small to precisely measure. In this work we have fabricated metal–oxide–semiconductor capacitors based on a large array of self-assembled Si nanowires. The capacitance and conductance of the nanowire array capacitors are directly measured and the interface state profile is determined by using the conductance method. We demonstrate that the nanowire array capacitor is an effective platform for studying the electronic properties of nanoscale interfaces. This approach provides a useful and efficient metrology for the study of the physics and device properties of nanoscale metal–oxide–semiconductor structures. [ABSTRACT FROM AUTHOR] Copyright of Nanotechnology is the property of IOP Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94772196&site=ehost-live
409,Silicon nanowire NVM with high-k gate dielectric stack,Qiliang Li,Microelectronic Engineering,01679317,,Jul2009,86,9-Jul,1957,4.0,40631529,10.1016/j.mee.2009.03.095,Elsevier B.V.,Article,FLASH memory; COMPUTER storage devices; DIELECTRIC devices; MICROFABRICATION; WIRE; NANOSILICON; ELECTRIC properties of metals; SEMICONDUCTOR junctions; Computer Storage Device Manufacturing; Computer and peripheral equipment manufacturing; Metal Service Centers and Other Metal Merchant Wholesalers,Flash memory; NVM; Silicon nanowire; SiNW,"Abstract: Three flash memory cell structures with silicon nanowire channels and high-k dielectric stacks were fabricated with a “self-aligning” process and their characteristics are reported and compared in this paper: a Metal/SiO2/HfO2/SiO2/Si (MOHOS) cell with a SiO2 blocking layer and two Metal/Al2O3/HfO2/SiO2/Si (MAHOS) cells with Al2O3, all with HfO2 as the charge trapping layer. Compared to (control) planar cells, all three operate at higher speeds, attributed to the enhanced electric field across the tunneling oxide surrounding the channel. The MAHOS cells (Al2O3 blocking layer) outperform the MOHOS cells (SiO2 blocking layer) and both have large memory window, fast operation speed, good endurance and retention. [Copyright &y& Elsevier] Copyright of Microelectronic Engineering is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=40631529&site=ehost-live
410,Simultaneously enhanced energy storage density and efficiency in novel BiFeO3-based lead-free ceramic capacitors.,Qiliang Li,Journal of the European Ceramic Society,09552219,,Jan2021,41,1,387,7.0,146614794,10.1016/j.jeurceramsoc.2020.08.032,Elsevier B.V.,Article,LEAD-free ceramics; CERAMIC capacitors; ENERGY storage; ENERGY density; SPECIFIC gravity; ELECTRIC breakdown,BiFeO3; Energy-storage density; Relaxor ferroelectric; Solid phase sintering,"In this work, a series of novel lead-free (1- x)Bi 0.83 Sm 0.17 Fe 0.95 Sc 0.05 O 3 - x (0.85BaTiO 3 -0.15Bi(Mg 0.5 Zr 0.5)O 3) [(1- x)BSFS- x (BT-BMZ), x = 0.45−0.85] relaxor ceramics were prepared by solid phase sintering, and their dielectric properties and energy storage performances were explored. It was revealed that all the samples have a dense structure with pure pseudo-cubic phase. With the increase of x , the ferroelectric hysteresis loop is gradually slimmed accompanied by a decreasing polarization, indicating an enhanced relaxor behavior. Moreover, the electric breakdown strength increases linearly with x due to the fine grain size and enhanced relative density. Interestingly, a large recoverable energy density (∼3.2 J/cm3) with an outstanding efficiency (∼92 %) is achieved under an electric field ∼206 kV/cm for the optimized component x = 0.75, which is superior to other reported lead-free ceramic systems. Moreover, the optimized ceramics of 0.25BSFS-0.75(BT-BMZ) show good thermal stability (25−100 °C) and excellent fatigue endurance (cycle number: > 105) in energy storage performances. This work opens up a new route to tailor lead-free dielectric ceramics with high energy storage properties. [ABSTRACT FROM AUTHOR] Copyright of Journal of the European Ceramic Society is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146614794&site=ehost-live
411,SOI FED-SRAM Cell: Structure and Operation.,Qiliang Li,IEEE Transactions on Electron Devices,00189383,,Sep2015,62,9,2865,6.0,109065842,10.1109/TED.2015.2450693,IEEE,Article,FIELD-effect devices; SEMICONDUCTOR diodes; THYRISTORS; BISTABLE devices; COMPUTER storage devices; Computer Storage Device Manufacturing; Computer and peripheral equipment manufacturing; Semiconductor and other electronic component manufacturing; Semiconductor and Related Device Manufacturing,Anodes; Charge carrier density; Field-effect diode (FED); Logic gates; MOSFET; Random access memory; SOI; SRAM; thin-capacitively coupled thyristor (TCCT); thyristor; Thyristors; Timing,"A static memory cell (SRAM) based on the field-effect diode (FED) is presented, and its operation is explained with the help of numerical device simulations. Although this new cell resembles the thin-capacitively coupled-thyristor (TCCT) SRAM cell in concept and operation, it is nevertheless characterized by significant advantages. These advantages derive from the fact that the thyristorlike mode of operation of the FED is gate induced, whereas the TCCT is an actual built-in thyristor. The operation of the cell is explained with the help of suitable timing diagrams, and the mechanisms of storing 1 and 0 are analyzed with detailed numerical simulations. In one operation scheme (where the cell could better be termed quasi-SRAM), a sequence of restore pulses is periodically applied after the cell is put on Hold, which ensures that the stored data remain valid for as long as the cell is powered ON. High read 0/1 current margin, fast write/read time, and densely packed cells are among the cell advantages obtained. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Electron Devices is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109065842&site=ehost-live
412,Study of interfacial strain at the α-Al2O3/monolayer MoS2 interface by first principle calculations.,Qiliang Li,Applied Surface Science,01694332,,Jan2018,428,,593,5.0,126185471,10.1016/j.apsusc.2017.09.203,Elsevier B.V.,Article,CHALCOGENIDES; METAL oxide semiconductor field-effect transistors; DIELECTRICS; CHARGE transfer; NANOELECTRONICS,2D semiconductors; Dielectric engineering; Strain effect; Temperature effect,"With the advances in two-dimensional (2D) transition metal dichalcogenides (TMDCs) based metal–oxide–semiconductor field-effect transistor (MOSFET), the interface between the semiconductor channel and gate dielectrics has received considerable attention due to its significant impacts on the morphology and charge transport of the devices. In this study, first principle calculations were utilized to investigate the strain effect induced by the interface between crystalline α-Al 2 O 3 (0001)/h-MoS 2 monolayer. The results indicate that the 1.3 nm Al 2 O 3 can induce a 0.3% tensile strain on the MoS 2 monolayer. The strain monotonically increases with thicker dielectric layers, inducing more significant impact on the properties of MoS 2 . In addition, the study on temperature effect indicates that the increasing temperature induces monotonic lattice expansion. This study clearly indicates that the dielectric engineering can effectively tune the properties of 2D TMDCs, which is very attractive for nanoelectronics. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126185471&site=ehost-live
413,Detection of Deep-Levels in Doped Silicon Nanowires Using Low-Frequency Noise Spectroscopy.,Qiliang Li,IEEE Transactions on Electron Devices,00189383,,Dec2013,60,12,4206,7.0,92520639,10.1109/TED.2013.2285154,IEEE,Article,ELECTRIC properties of silicon nanowires; SPECTRUM analysis; LORENTZIAN function; IONIZATION energy; ELECTRON capture; ELECTRIC current measurement,Current measurement; Deep-levels; field-effect transistor (FET); generation-recombination (G-R) noise; Gold; low-frequency noise (LFN); Nickel; Noise; Silicon; silicon nanowire (SiNW); Spectroscopy; Temperature measurement,"We report detailed characterization of electrically-active deep-levels in doped Si nanowires (SiNWs) grown using catalyst-assisted vapor–liquid-solid (VLS) technique. Temperature-dependent low-frequency noise (LFN) spectroscopy was used to reveal the presence of generation-recombination related Lorentzian-type peaks along with 1/f-type noise in these NWs. In Ni-catalyzed SiNWs, the correlated LFN spectroscopy detected electrically active deep-levels with ionization energies of 0.42 eV for the n-type and 0.22 eV for the p-type SiNWs, respectively. In Au-catalyzed n- and p-type SiNWs, the energies of the deep-levels were estimated to be 0.44 and 0.38 eV, respectively. These values are in good agreement with the known ionization energies of deep-levels introduced by Ni and Au in Si. Associated trap concentrations and hole and electron capture cross sections were also estimated. This paper clearly indicated the presence of electrically active deep-levels associated with unintentional incorporation of catalyst atoms in the VLS-grown SiNWs. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Electron Devices is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=92520639&site=ehost-live
414,Domain structure and multiferroic properties of epitaxial hexagonal ErMnO3 films.,Qiliang Li,Journal of Alloys & Compounds,09258388,,Apr2020,821,,N.PAG,1.0,141632063,10.1016/j.jallcom.2019.153529,Elsevier B.V.,Article,EPITAXY; PULSED laser deposition; MAGNETIC measurements; THIN films; RAMAN spectroscopy,Ferroelectric domains; Hexagonal ErMnO3 film; Multiferroicity; Raman spectrum,"Epitaxial ErMnO 3 thin films were grown on Pt-coated Al 2 O 3 substrate by pulsed laser deposition. Their structure, multiferroicity, and ferroelectric domain properties have been comprehensively characterized and studied. The XRD measurement indicated an excellent epitaxy of out-of-plane ErMnO 3 (0001)//Pt(111)//Al 2 O 3 (0001) and in-plane ErMnO 3 [1000]//Pt[11 2 ‾ ]//Al 2 O 3 [11 2 ‾ 0] structures. The as-deposited ErMnO 3 films exhibited spontaneous ferroelectric domains with reversible polarization. A significant remnant polarization of 1.3 μC/cm2 and an active peak at 662 cm−1 in Raman spectra were found, further showing a high quality of the ErMnO 3 thin films. Moreover, the magnetic measurements indicated that the thin film has an excellent anisotropic magnetic property with a Neel temperature at ≈53 K. • The films were grown with highly hexagonal epitaxy with out-of-plane ErMnO 3 (0001)//Pt(111)//Al 2 O 3 (0001) and in-plane ErMnO 3 [1000]//Pt[11 2 ]//Al 2 O 3 [11 2 0]. • The as-deposited ErMnO 3 films clearly exhibited spontaneous ferroelectric domains, which can be polarized reversibly with excellent retention performance. • A remanent polarization value of 1.3 μC/cm2 and a significant Raman-active peak at 662 cm−1 were observed. • The films exhibited a anisotropic magnetic properties with a Neel temperature of 53 K. [ABSTRACT FROM AUTHOR] Copyright of Journal of Alloys & Compounds is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141632063&site=ehost-live
415,Electrical transport and low-frequency noise in chemical vapor deposited single-layer MoS2 devices.,Qiliang Li,Nanotechnology,09574484,,4/18/2014,25,15,155702,7.0,95012273,10.1088/0957-4484/25/15/155702,IOP Publishing,Article,MOLYBDENUM disulfide; CHEMICAL vapor deposition; ELECTRONIC noise; CHARGE exchange; FIELD-effect transistors; PASSIVATION,,"We have studied temperature-dependent (77–300 K) electrical characteristics and low-frequency noise (LFN) in chemical vapor deposited (CVD) single-layer molybdenum disulfide (MoS2) based back-gated field-effect transistors (FETs). Electrical characterization and LFN measurements were conducted on MoS2 FETs with Al2O3 top-surface passivation. We also studied the effect of top-surface passivation etching on the electrical characteristics of the device. Significant decrease in channel current and transconductance was observed in these devices after the Al2O3 passivation etching. For passivated devices, the two-terminal resistance variation with temperature showed a good fit to the activation energy model, whereas for the etched devices the trend indicated a hopping transport mechanism. A significant increase in the normalized drain current noise power spectral density (PSD) was observed after the etching of the top passivation layer. The observed channel current noise was explained using a standard unified model incorporating carrier number fluctuation and correlated surface mobility fluctuation mechanisms. Detailed analysis of the gate-referred noise voltage PSD indicated the presence of different trapping states in passivated devices when compared to the etched devices. Etched devices showed weak temperature dependence of the channel current noise, whereas passivated devices exhibited near-linear temperature dependence. [ABSTRACT FROM AUTHOR] Copyright of Nanotechnology is the property of IOP Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95012273&site=ehost-live
416,Gate assisted Kelvin test structure to measure the electron and hole flows at the same nanowire contacts.,Qiliang Li,Applied Physics Letters,00036951,,9/29/2014,105,13,1,4.0,98710878,10.1063/1.4897008,American Institute of Physics,Article,SILICON nanowires; LOGIC circuits; ELECTRONS; FIELD-effect transistors; MICROFABRICATION; NANOELECTRONICS,,"A gate assisted Kelvin test structure based on Si nanowire field effect transistors has been designed and fabricated for the characterization of the transistor source/drain contacts. Because the Si nanowire field effect transistors exhibit ambipolar characteristics with electron current slightly lower than the hole current, we can select the type of carriers (electrons or holes) flowing through the same contacts and adjust the current by the applied gate voltage. In this way, we are able to measure the characteristics of the same contact with either pure electron or hole flow. In addition, we found that the nanowire contacts behave very differently depending on the current flow directions. This indicates that the source and drain contact resistance can be dramatically different. Such a gate assisted Kelvin Test structure will lead to future metrology and applications in nanoelectronics. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98710878&site=ehost-live
417,Hole doping induced half-metallic itinerant ferromagnetism and giant magnetoresistance in CrI3 monolayer.,Qiliang Li,Applied Surface Science,01694332,,Jan2021,535,,N.PAG,1.0,146612864,10.1016/j.apsusc.2020.147693,Elsevier B.V.,Article,MONTE Carlo method; GIANT magnetoresistance; FERROMAGNETISM; CURIE temperature; MAGNETIC storage; MONOMOLECULAR films,Carrier doping; Giant magnetoresistance; Half metal; Itinerant ferromagnetism; Two dimensional monolayer,"• The magnetoresistance over 106% is achieved via hole doping in 1L-CrI 3 by NEGF. • Hole doping renders 1L-CrI 3 half-metallic and nearly 100% spin-polarization. • Hole doping significantly enhances ferromagnetic stability and Curie temperature. The exploit of magnetic devices with high magnetoresistance is vital for the development of magnetic sensing and data storage technologies. Here, using density functional calculations combined with Monte Carlo simulations, we explore the magnetic properties and spin-dependent transport of CrI 3 monolayer under an electrostatic hole doping. Extraordinarily, the magnetoresistance can be controlled over 106% within a certain doping density range. The hole doping can render CrI 3 monolayer half-metallic and nearly 100% spin-polarization at Fermi energy level can be achieved. Moreover, the hole doping can significantly enhance the stability of itinerant ferromagnetism. The Heisenberg exchange parameters can be significantly improved and meanwhile, the Curie temperature can be boosted to room temperature via a doping density of 8.49 × 1014 cm−2. This study reveals that the carrier doping engineering can enable two-dimensional CrI 3 as a remarkable material for developing practical and high-performance spintronic nanodevices. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146612864&site=ehost-live
418,Maritime vessel emission monitoring by an UAV gas sensor system.,Qiliang Li,Ocean Engineering,00298018,,Dec2020,218,,N.PAG,1.0,147604796,10.1016/j.oceaneng.2020.108206,Elsevier B.V.,Article,"NAVIGATION; ATMOSPHERIC turbulence; TRACKING algorithms; DRONE aircraft; MOBILE robots; DETECTORS; Search, Detection, Navigation, Guidance, Aeronautical, and Nautical System and Instrument Manufacturing",Gas detection; Joint tracking; Maritime monitoring; UAV (Unmanned aerial vehicle); Vessel emission,"Monitoring the gas emission of maritime vessels is very challenging as the fields are almost inaccessible and the emission is very susceptible to the surrounding environment. In ultra-large-scale maritime scenes, mobile robots capable of gas detection and tracking will most likely fall into atmospheric turbulence. In this work, an enhanced tracking algorithm using our unmanned aerial vehicle (UAV) gas sensor system is proposed for maritime vessel emission monitoring. A global prediction is acquired by modelling the emission of individual vessel, while another local gradient direction can be calculated from the real-time onboard sensor measurements. By a probabilistic framework, the global prediction and the gradient detection are fused to generate a more reliable navigation for the UAV. In order to verify the proposed algorithm, real monitoring experiments using the developed UAV gas sensor system are presented and SO 2 , NO x contents in vessel emissions are conducted. A comparison between the UAV-based detections and the off-line sampled data is preformed, and the results indicate that the proposed algorithm has the advantage in exactly guiding the UAV towards the vessel emission. • Aim to utilizing UAV for monitoring the gas emissions from maritime vessels.. • A vector tracking algorithm for UAV to detect and track the vessel emissions.. • A series of comprehensive on-the-spot experiments have been performed.. • The proposed UAV system and algorithm is applicable well to maritime situation.. [ABSTRACT FROM AUTHOR] Copyright of Ocean Engineering is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147604796&site=ehost-live
419,Non-volatile memory with self-assembled ferrocene charge trapping layer.,Qiliang Li,Applied Physics Letters,00036951,,7/29/2013,103,5,53102,4.0,89546936,10.1063/1.4817009,American Institute of Physics,Article,"NONVOLATILE memory; FERROCENE; ELECTRIC charge; CAPACITORS; X-ray photoelectron spectroscopy; METALLIC oxides; Electronic components, navigational and communications equipment and supplies merchant wholesalers; All Other Miscellaneous Electrical Equipment and Component Manufacturing; Capacitor, Resistor, Coil, Transformer, and Other Inductor Manufacturing; Other Electronic Parts and Equipment Merchant Wholesalers",,"A metal/oxide/molecule/oxide/Si capacitor structure containing redox-active ferrocene molecules has been fabricated for non-volatile memory application. Cyclic voltammetry and X-ray photoelectron spectroscopy were used to measure the molecules in the structure, showing that the molecules attach on SiO2/Si and the molecules are functional after device fabrication. These solid-state molecular memory devices have fast charge-storage speed and can endure more than 109 program/erase cycles. This excellent performance is derived from the intrinsic properties of the redox-active molecules and the hybrid Si-molecular device structure. These molecular devices are very attractive for future high-level non-volatile memory applications. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89546936&site=ehost-live
420,Novel Te doping in Y2O3–Al2O3 system phosphor.,Qiliang Li,Journal of Alloys & Compounds,09258388,,Apr2020,821,,N.PAG,1.0,141632021,10.1016/j.jallcom.2019.153474,Elsevier B.V.,Article,YTTRIUM aluminum garnet; PHOSPHORS; DENSITY of states; OPTICAL properties; GALLIUM antimonide; YAMS; TELLURIUM; POWDERS; Potato Farming,First-principles; Luminescence; Te doping; Y3Al5O12; Y4Al2O9,"Te doped Yttrium-aluminum (Y 2 O 3 –Al 2 O 3) system powders were synthesized by solid-state method. The structure and optical properties were investigated by XRD, SEM, XPS and PL. XRD analysis indicated that the samples transform from YAM, YAP to YAG phase as the sintering temperature rises. The samples sintered below 1200 °C have a broad emission centered at 604 nm with a lifetime up to 13.33 μs under room temperature. While in the mix phase of YAP and YAG, a sharp and intense emission peak centered at 715 nm was monitored with a lifetime up to approximate 2.76 ms. Moreover, the band structure and density of states for Te doped YAGwere investigated by first-principle calculations. [ABSTRACT FROM AUTHOR] Copyright of Journal of Alloys & Compounds is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141632021&site=ehost-live
421,Novel Two-Dimensional Mechano-Electric Generators and Sensors Based on Transition Metal Dichalcogenides.,Qiliang Li,Scientific Reports,20452322,,8/7/2015,,,12854,1.0,108797757,10.1038/srep12854,Springer Nature,Article,"ELECTROMECHANICAL devices; ELECTRIC generators; TRANSITION metals; CHALCOGENIDES; BAND gaps; All other building equipment contractors; Other Building Equipment Contractors; Motor and Generator Manufacturing; Electrical Apparatus and Equipment, Wiring Supplies, and Related Equipment Merchant Wholesalers; Electrical wiring and construction supplies merchant wholesalers; Totalizing Fluid Meter and Counting Device Manufacturing; Automatic Environmental Control Manufacturing for Residential, Commercial, and Appliance Use; Other Electronic Component Manufacturing; Other Communications Equipment Manufacturing; Semiconductor and other electronic component manufacturing",,"Transition metal dichalcogenides (TMDCs), such as MoS2 and WSe2, provide two-dimensional atomic crystals with semiconductor band gap. In this work, we present a design of new mechano-electric generators and sensors based on transition metal dichalcogenide nanoribbon PN junctions and heterojunctions. The mechano-electric conversion was simulated by using a first-principle calculation. The output voltage of MoS2 nanoribbon PN junction increases with strain, reaching 0.036 V at 1% strain and 0.31 V at 8% strain, much larger than the reported results. Our study indicates that the length, width and layer number of TMDC nanoribbon PN junctions have an interesting but different impact on the voltage output. Also, the results indicate that doping position and concentration only cause a small fluctuation in the output voltage. These results have been compared with the mechano-electric conversion of TMDC heterojunctions. Such novel mechano-electric generators and sensors are very attractive for applications in future self-powered, wearable electronics and systems. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108797757&site=ehost-live
422,"Phase transition, effective mass and carrier mobility of MoS2 monolayer under tensile strain.",Qiliang Li,Applied Surface Science,01694332,,Jan2015,325,,27,6.0,100234033,10.1016/j.apsusc.2014.11.079,Elsevier B.V.,Article,PHASE transitions; MOLYBDENUM compounds; MONOMOLECULAR films; TENSILE strength; COMPUTATIONAL chemistry; ELECTRON mobility; DEFORMATION potential,Mobility enhancement; MoS 2 monolayer; Phase transition; Strain effect; Two-dimensional materials,"We report a computational study on the impact of tensile strain on MoS 2 monolayer. The transition between direct and indirect bandgap structure and the transition between semiconductor and metal phases in the monolayer have been investigated with tensile strain along all direction configurations with both x -axis and y -axis components ɛ xy ( ɛ x and ɛ y ). Electron effective mass and the hole effective mass are isotropic for biaxial strain ɛ xy = ɛ x = ɛ y and anisotropic for ɛ xy with ɛ x ≠ ɛ y . The carrier effective mass behaves differently along different directions in response to the tensile strain. In addition, the impact of strain on carrier mobility has been studied by using the deformation potential theory. The electron mobility increases over 10 times with the biaxial strain: ɛ x = ɛ y = 9.5%. Also, the mobility decreases monotonically with the increasing temperature as μ ∼ T −1 . These results are very important for future nanotechnology based on two-dimensional materials. [ABSTRACT FROM AUTHOR] Copyright of Applied Surface Science is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100234033&site=ehost-live
423,Polarization tunability in multiferroic DyMn2O5: Influence of Y and Eu co-doping and 3d-4f exchange.,Qiliang Li,Solid State Communications,00381098,,Feb2020,307,,N.PAG,1.0,142250119,10.1016/j.ssc.2019.113809,Elsevier B.V.,Article,SPECIFIC heat; DATA warehousing; MANGANITE; MAGNETIC fields; FERROELECTRICITY; RARE earth metals; All Other Metal Ore Mining,DyMn2O5; Ferrielectricity; Manganite; Multiferroicity,"Coupling effects among spin, charge, and lattice in a strongly correlated system are critical for next generation spintronic and data storage devices. However, the complex effects are elusive and difficult to distinguish their contributions to polarization modulation. Here we tailored the polarization by co-doping of non-magnetic Y and Eu at A-sites in DyMn 2 O 5. The structure, specific heat, magnetism, and ferroelectricity of the polycrystalline Dy 1-x (Eu 0.24 Y 0.76) x Mn 2 O 5 ceramics were comprehensively explored. Interestingly, the co-doping does not cause lattice distortion of DyMn 2 O 5 , and all the ceramics are orthorhombic structures, while the independent Dy3+ spin order and the Dy3+-Mn3+ coupling can be suppressed. With increasing the co-doping content x , the spins related properties associated with the Dy3+-Mn4+-Dy3+ sub-lattice are progressively inhibited, while they keep less disturbance in the Mn3+-Mn4+-Mn3+ block. Moreover, the spin coupling of Dy3+-Mn3+ ions is stronger again the magnetic field than that of Dy3+-Mn3+. Our results enhance the understanding of ferrielectricity in DyMn 2 O 5 , and provide a method for controlling the polarization in the multiferroic manganite coexisting 3 d and 4 f elements. [ABSTRACT FROM AUTHOR] Copyright of Solid State Communications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142250119&site=ehost-live
424,Polarization tunable and enhanced photovoltaic properties in tetragonal-like BiFeO3 epitaxial films with graphene top electrode.,Qiliang Li,Journal of Alloys & Compounds,09258388,,Nov2019,811,,N.PAG,1.0,138815563,10.1016/j.jallcom.2019.152013,Elsevier B.V.,Article,EPITAXY; OPEN-circuit voltage; PULSED laser deposition; PHOTOVOLTAIC effect; ENERGY conversion; FERROELECTRIC materials; SCHOTTKY barrier,Graphene; Photovoltaic effect; Polarization; Tetragonal-like BiFeO3,"Ferroelectric photovoltaic materials have attracted intensive interest due to their intriguing above-bandgap photovoltage, while the low photocurrent limits their further device applications. In this work, both enhanced and tunable photovoltaic effect (PVE) are demonstrated in sandwiched structure of Graphene/tetragonal-like BiFeO 3 /Ca 0.96 Ce 0.04 MnO 3 (Graphene/T(-like) BFO/CCMO). The epitaxial BFO film is grown on the CCMO buffered LaAlO 3 substrate by pulsed laser deposition and the mechanically exfoliated graphene is transferred directly onto the BFO film as top electrode. The optimized open circuit voltage (V oc) and short circuit current density (J sc) are measured to be −0.88 V and 2.56 mA/cm2, respectively. Moreover, the photovoltaic response can be modulated by controllable ferroelectric polarization, whereby both the V oc and J sc show piezoresponse-like hysteresis behaviors against voltage. The notably enhanced PVE is likely a result of the combination effects of large polarization in the T(-like) BFO film, partially unscreened depolarization field, high transmittance and conductivity of the graphene top electrode. This work clearly indicates the potential of Graphene/ferroelectric photovoltaic devices for memory and energy conversion applications. • New nanocapacitor consists tetragonal-like BFO and exfoliated ultrathin graphene. • Open circuit voltage and short circuit current density are −0.88 V and 2.56 mA/cm2. • The photovoltaic response can be well modulated by ferroelectric polarization. • The mechanism relies on the polarization modulated Schottky barrier change. • The ultrathin graphene further enhances the photovoltaic response. [ABSTRACT FROM AUTHOR] Copyright of Journal of Alloys & Compounds is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138815563&site=ehost-live
425,Ultraviolet/ozone treatment to reduce metal-graphene contact resistance.,Qiliang Li,Applied Physics Letters,00036951,,5/6/2013,102,18,183110,5.0,87545140,10.1063/1.4804643,American Institute of Physics,Article,ELECTRIC properties of graphene; METAL fabrication; CHEMICAL vapor deposition; PHOTOLITHOGRAPHY; METHYL methacrylate; PHOTORESIST materials; RAMAN spectroscopy; Other plate work and fabricated structural product manufacturing,,"We report reduced contact resistance of single-layer graphene devices by using ultraviolet ozone treatment to modify the metal/graphene contact interface. The devices were fabricated from mechanically transferred, chemical vapor deposition grown single layer graphene. Ultraviolet ozone treatment of graphene in the contact regions as defined by photolithography and prior to metal deposition was found to reduce interface contamination originating from incomplete removal of poly(methyl-methacrylate) and photoresist. Our control experiment shows that exposure times up to 10 min did not introduce significant disorder in the graphene as characterized by Raman spectroscopy. By using the described approach, contact resistance of less than 200 Ω μm was achieved for 25 min ultraviolet ozone treatment, while not significantly altering the electrical properties of the graphene channel region of devices. [ABSTRACT FROM AUTHOR] Copyright of Applied Physics Letters is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=87545140&site=ehost-live
426,An agent-based system with temporal data mining for monitoring financial stability on insurance markets.,Jessica Lin,Expert Systems with Applications,09574174,,Jun2019,123,,270,13.0,134739097,10.1016/j.eswa.2019.01.049,Elsevier B.V.,Article,"DATA mining; MULTIAGENT systems; INSURANCE companies; EXPERT systems; ECONOMIC forecasting; Other Direct Insurance (except Life, Health, and Medical) Carriers; Direct Health and Medical Insurance Carriers; Direct group life, health and medical insurance carriers; Insurance Agencies and Brokerages",Agents; Anomaly; Automobile insurance; Crisis; Cycle; Motif,"Highlights • Knowledge discovery on agent-based simulations predicts market instability. • Cycles on insurance markets can be forecast. • Aggressive price-undercutting by some insurers creates a winners curse. • Regulators should monitor pricing patterns of insurers. Abstract We describe an expert system to monitor the stability of insurance markets. It consists of two components: an agent-based simulation component and a temporal data mining component. Like other financial markets, insurance markets experience destabilizing cycles and suffer episodic crises. The expert system assists market regulators by monitoring the financial position of individual insurers and of the overall market, and by forecasting cycles and impending insolvencies. The agent-based simulation component runs a forward simulation allowing for interaction among insurers in a competitive market, and between insurers and customers. The temporal data mining component extracts useful information for market regulators from the simulations. A prototype of the system is applied to the automobile insurance market. We show how the system may be used to forecast cycles, investigate stability, and analyze insurers' herding behavior on the market. A practical policy conclusion is that regulators should monitor individual insurers' pricing pattern because aggressive price undercutting creates a ""winner's curse"", with subsequent losses and market instability. [ABSTRACT FROM AUTHOR] Copyright of Expert Systems with Applications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134739097&site=ehost-live
427,CPM: A general feature dependency pattern mining framework for contrast multivariate time series.,Jessica Lin,Pattern Recognition,00313203,,Apr2021,112,,N.PAG,1.0,148407299,10.1016/j.patcog.2020.107711,Elsevier B.V.,Article,TIME series analysis; DISTRIBUTION (Probability theory); SEQUENTIAL pattern mining; PROCESS optimization; SCALABILITY; INTERVENTION (Federal government); GAUSSIAN distribution,Contrast pattern; Controlled experiment; Driving behavior; Feature dependency; Multivariate time series,"• Unsupervised framework to mine contrast patterns in controlled experiment. • Customizable regularization techniques. • Efficient optimization algorithm easily adapt to various models under the framework. • Highly interpretable results in real world controlled experiments. With recent advances in sensor technology, multivariate time series data are becoming extremely large with sophisticated but insightful inter-variable dependency patterns. Mining contrast dependency patterns in controlled experiments can help quantify the differences between control and experimental time series, however, overwhelms practitioners' capability. Existing methods suffer from determining whether the differences are caused by the intervention or by different states. We propose a novel Contrast Pattern Mining (CPM) framework to find the intervention-related differences by jointly determining and characterizing the dynamic states in both time series via multivariate Gaussian distributions. Under the CPM framework, we not only propose a new covariance-based contrast pattern model, but also integrate our previous proposed partial correlation-based model as a special case. An efficient generic algorithm is developed to optimize various CPM models by adjusting one of the sub-routines. Comprehensive experiments are conducted to analyze the effectiveness, scalability, utility, and interpretability of the proposed framework. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148407299&site=ehost-live
428,Efficient Discovery of Unusual Patterns in Time Series.,Jessica Lin,New Generation Computing,02883635,,2007,25,1,61,33.0,23971598,,Springer Nature,Article,"TIME Series Processor (Computer program language); ANOMALY detection (Computer security); MARKOV processes; DATABASE management; COMPUTER algorithms; RESEARCH & development; Data Processing, Hosting, and Related Services; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology",Anomaly Detection; Markov Model Feature Extraction; Novelty Detection; Suffix Tree; Time Series,"The problem of finding a specified pattern in a time series database (i.e., query by content) has received much attention and is now a relatively mature field. In contrast, the important problem of enumerating all surprising or interesting patterns has received far less attention. This problem requires a meaningful definition of ‘surprise’, and an efficient search technique. All previous attempts at finding surprising patterns in time series use a very limited notion of surprise, and/or do not scale to massive datasets. To overcome these limitations we propose a novel technique that defines a pattern surprising if the frequency of its occurrence differs substantially from that expected by chance, given some previously seen data. This notion has the advantage of not requiring the user to explicitly define what is a surprising pattern, which may be hard, or perhaps impossible, to elicit from a domain expert. Instead, the user gives the algorithm a collection of previously observed ‘normal’ data. Our algorithm uses a suffix tree to efficiently encode the frequency of all observed patterns and allows a Markov model to predict the expected frequency of previously unobserved patterns. Once the suffix tree has been constructed, a measure of surprise for all the patterns in a new database can be determined in time and space linear in the size of the database. We demonstrate the utility of our approach with an extensive experimental evaluation. [ABSTRACT FROM AUTHOR] Copyright of New Generation Computing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=23971598&site=ehost-live
429,Teaching as a Design Process: A Framework for Design-based Research in Engineering Education.,Craig Lorie,Proceedings of the ASEE Annual Conference & Exposition,21535868,,2015,,,1,11.0,116026269,,ASEE,Article,TEACHING; ENGINEERING education; HUMAN ecology; DESIGN education; ENGINEERING models,,"The article provides information on teaching aspects related to design process and design based developmental research in engineering education. Topics discussed include characterization of human environment under constraints, methodologies of design research, and activities to demonstrate teaching practice.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116026269&site=ehost-live
431,Assessment of Level-3 Gridded Global Precipitation Mission (GPM) Products Over Oceans.,Viviana Maggioni,Remote Sensing,20724292,,Feb2019,11,3,255,1.0,134843738,10.3390/rs11030255,MDPI,Article,METEOROLOGICAL precipitation measurement; METEOROLOGICAL satellites; ATMOSPHERIC effects on remote sensing; RADAR meteorology; SATELLITE-based remote sensing; STANDARD deviations; ERROR analysis in mathematics; GLOBAL Precipitation Climatology Project,error analysis; precipitation; satellite remote sensing; triple collocation,"The performance of Level-3 gridded Global Precipitation Mission (GPM)-based precipitation products (IMERG, Integrated Multi-satellite Retrievals for GPM) is assessed against two references over oceans: the OceanRAIN dataset, derived from oceanic shipboard disdrometers, and a satellite-based radar product (the Level-3 Dual-frequency Precipitation Radar, 3DPRD). Daily IMERG products (early, late, final) and microwave-only (MW) and Infrared-only (IR) precipitation components are evaluated at four different spatial resolutions (0.5°, 1°, 2°, and 3°) during a 3-year study period (March 2014–February 2017). Their performance is assessed based on both categorical and continuous performance metrics, including correlation coefficient, probability of detection, success ratio, bias, and root mean square error (RMSE). A triple collocation analysis (TCA) is also presented to further investigate the performance of these satellite-based products. Overall, the IMERG products show an underestimation with respect to OceanRAIN. Rain events in OceanRAIN are correctly detected by all IMERG products ~80% of the times. IR estimates show relatively large errors and low correlations with OceanRAIN compared to the other products. On the other hand, the MW component performs better than other products in terms of both categorical and continuous statistics. TCA reveals that 3DPRD performs consistently better than OceanRAIN in terms of RMSE and coefficient of determination at all spatial resolutions. This work is part of a larger effort to validate GPM products over nontraditional regions such as oceans. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134843738&site=ehost-live
433,Building an Online Learning Module for Satellite Remote Sensing Applications in Hydrologic Science.,Viviana Maggioni,Remote Sensing,20724292,,Sep2020,12,18,3009,1.0,146537753,10.3390/rs12183009,MDPI,Letter,REMOTE sensing; LEARNING modules; ONLINE education; REMOTE-sensing images; HYDROLOGIC cycle; ENVIRONMENTAL sciences; MULTISPECTRAL imaging,active learning; authentic task; constructive alignment; e-learning; higher education; remote sensing,"This article presents an online teaching tool that introduces students to basic concepts of remote sensing and its applications in hydrology. The learning module is intended for junior/senior undergraduate students or junior graduate students with no (or little) prior experience in remote sensing, but with some basic background of environmental science, hydrology, statistics, and programming. This e-learning environment offers background content on the fundamentals of remote sensing, but also integrates a set of existing online tools for visualization and analysis of satellite observations. Specifically, students are introduced to a variety of satellite products and techniques that can be used to monitor and analyze changes in the hydrological cycle. At completion of the module, students are able to visualize remote sensing data (both in terms of time series and spatial maps), detect temporal trends, interpret satellite images, and assess errors and uncertainties in a remote sensing product. Students are given the opportunity to check their understanding as they progress through the module and also tackle complex real-life problems using remote sensing observations that professionals and scientists commonly use in practice. The learning tool is implemented in HydroLearn, an open-source, online platform for instructors to find and share learning modules and collaborate on developing teaching resources in hydrology and water resources. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146537753&site=ehost-live
434,Centroidal Voronoi tessellation based methods for optimal rain gauge location prediction.,Viviana Maggioni,Journal of Hydrology,00221694,,May2020,584,,N.PAG,1.0,142766578,10.1016/j.jhydrol.2020.124651,Elsevier B.V.,Article,CENTROIDAL Voronoi tessellations; PRECIPITATION gauges; RAIN gauges; FORECASTING; PRECIPITATION variability; PATTERNMAKING; GAUGE field theory; NORTHERN Italy; OKLAHOMA; Other Measuring and Controlling Device Manufacturing,CVT; Decorrelation; Optimal placement; Rain gauges,"• An automated strategy to find optimal precipitation gauge locations. • Allow users customizing the level of confidence when placing gauges. • Potentially flexible to any precipitation dataset. With more satellite and model precipitation data becoming available, new analytical methods are needed that can take advantage of emerging data patterns to make well informed predictions in many hydrological applications. We propose a new strategy where we extract precipitation variability patterns and use correlation map to build the resulting density map that serves as an input to centroidal Voronoi tessellation construction that optimizes placement of precipitation gauges. We provide results of numerical experiments based on the data from the Alto-Adige region in Northern Italy and Oklahoma and compare them against actual gauge locations. This method provides an automated way for choosing new gauge locations and can be generalized to include physical constraints and to tackle other types of resource allocation problems. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142766578&site=ehost-live
435,Characteristics and Diurnal Cycle of GPM Rainfall Estimates over the Central Amazon Region.,Viviana Maggioni,Remote Sensing,20724292,,Jul2016,8,7,544,20.0,117069858,10.3390/rs8070544,MDPI,Article,DIURNAL variations of rainfall; ARTIFICIAL satellites; RAINFALL; HAZARD mitigation; METEOROLOGICAL precipitation measurement; RADAR meteorology; Space Research and Technology,GoAmazon; GPM; GPROF; IMERG; radar rainfall estimates; satellite rainfall estimates; uncertainty quantification,"Studies that investigate and evaluate the quality, limitations and uncertainties of satellite rainfall estimates are fundamental to assure the correct and successful use of these products in applications, such as climate studies, hydrological modeling and natural hazard monitoring. Over regions of the globe that lack in situ observations, such studies are only possible through intensive field measurement campaigns, which provide a range of high quality ground measurements, e.g., CHUVA (Cloud processes of tHe main precipitation systems in Brazil: A contribUtion to cloud resolVing modeling and to the GlobAl Precipitation Measurement) and GoAmazon (Observations and Modeling of the Green Ocean Amazon) over the Brazilian Amazon during 2014/2015. This study aims to assess the characteristics of Global Precipitation Measurement (GPM) satellite-based precipitation estimates in representing the diurnal cycle over the Brazilian Amazon. The Integrated Multi-satellitE Retrievals for Global Precipitation Measurement (IMERG) and the Goddard Profiling Algorithm--Version 2014 (GPROF2014) algorithms are evaluated against ground-based radar observations. Specifically, the S-band weather radar from the Amazon Protection National System (SIPAM), is first validated against the X-band CHUVA radar and then used as a reference to evaluate GPM precipitation. Results showed satisfactory agreement between S-band SIPAM radar and both IMERG and GPROF2014 algorithms. However, during the wet season, IMERG, which uses the GPROF2014 rainfall retrieval from the GPM Microwave Imager (GMI) sensor, significantly overestimates the frequency of heavy rainfall volumes around 00:00-04:00 UTC and 15:00-18:00 UTC. This overestimation is particularly evident over the Negro, Solimões and Amazon rivers due to the poorly-calibrated algorithm over water surfaces. On the other hand, during the dry season, the IMERG product underestimates mean precipitation in comparison to the S-band SIPAM radar, mainly due to the fact that isolated convective rain cells in the afternoon are not detected by the satellite precipitation algorithm. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117069858&site=ehost-live
436,Comparing global passive microwave freeze/thaw records: Investigating differences between Ka- and L-band products.,Viviana Maggioni,Remote Sensing of Environment,00344257,,Sep2020,247,,N.PAG,1.0,144459988,10.1016/j.rse.2020.111936,Elsevier B.V.,Article,MICROWAVE radiometry; MICROWAVES; ARID regions; THAWING; SHRUBLANDS; SOIL moisture measurement; COLD regions,Climate; Cryosphere; Freeze-thaw earth system data record (FT-ESDR); Freeze/thaw; Microwave radiometry; Passive microwave; Remote sensing; Satellite applications; Soil moisture active passive (SMAP),"The NASA L-Band Soil Moisture Active Passive (SMAP) satellite mission launched in 2015 has produced soil moisture and freeze thaw (FT) products at a global scale. While the use of L-band (1.41 GHz) passive microwave radiometry (P-MW) has proven useful in detecting changes in the surface FT state, these classifications have not been comprehensively assessed against similar existing FT products, such as the global FT record from the Special Sensor Microwave/Imager (SSM/I, Ka-band, 37.0 GHz) as part of the FT Earth System Data Record (FT-ESDR). In order to fill in this gap, this study investigates regions in which FT classifications diverge and identifies potential sources of classification variability. The SMAP and SSM/I FT records are compared over an extended period covering multiple seasonal cycles from April 2015 through December 2017. The spatially and temporally varying relationship between these products is examined in relation to climate (Köppen-Geiger climate classes and air temperature), MODIS (MoDerate Resolution Imaging Spectrometer) land cover, and topography (using Global Multi-resolution Terrain Elevation Data). SMAP and SSM/I FT product agreement proportion (Ap) was corrected for seasonality and then separated by land cover classes and compared to the global Ap mean. The agreement between these products vary most notably during freeze and thaw onset and in areas near abundant surface water, snow and ice, and wetlands. Relative to other vegetation types, reduced agreement between FT products is also observed over grasslands, sparsely vegetated lands, as well as mixed and evergreen forests. Distinct seasonal differences in FT classification agreement were also detected between products over cold arid regions and between continental and temperate classes. Similarly, as topographic complexity increases, a decreasing trend in agreement between L- and Ka-band FT products is observed. While reiterating challenges in FT classifications identified by prior studies, this work also contributes new insights by providing detailed geospatial and seasonal analyses into the factors contributing to FT product divergence. • We compare ~3 years of global microwave derived freeze/thaw (FT) records. • Variations in defined frozen extent are observed between L- and Ka-bands. • FT records from FT-ESDR and SMAP had classification agreement of 83.5%. • Product agreement is substantially diminished in shoulder seasons. • Distinct variability identified across climate, topography, and landcover. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing of Environment is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144459988&site=ehost-live
437,Complementing near-real time satellite rainfall products with satellite soil moisture-derived rainfall through a Bayesian Inversion approach.,Viviana Maggioni,Journal of Hydrology,00221694,,Jun2019,573,,341,11.0,139236837,10.1016/j.jhydrol.2019.03.038,Elsevier B.V.,Article,SOIL moisture; RAINFALL; FLOOD forecasting; ARTIFICIAL satellites; PRECIPITATION probabilities; WATER levels; ITALY; Space Research and Technology,Predictive uncertainty; Rainfall; Soil moisture; Water resource management,"• A Bayesian approach has been used for merging multiple satellite rainfall products. • We created a superior product that can be efficiently run in near-real time. • Soil moisture can provide useful information for improving satellite rainfall. This work investigates the potential of using the Bayesian-based Model Conditional Processor (MCP) for complementing satellite precipitation products with a rainfall dataset derived from satellite soil moisture observations. MCP – which is a Bayesian Inversion approach – was originally developed for predictive uncertainty estimates of water level and discharge to support real-time flood forecasting. It is applied here for the first time to precipitation to provide its probability distribution conditional on multiple satellite precipitation estimates derived from TRMM Multi-Satellite Precipitation Analysis real-time product v.7.0 (3B42RT) and the soil moisture-based rainfall product SM2RAIN-CCI. In MCP, 3B42RT and SM2RAIN-CCI represent a priori information (predictors) about the ""true"" precipitation (predictand) and are used to provide its real-time a posteriori probabilistic estimate by means of the Bayes theorem. MCP is tested across Italy during a 6-year period (2010–2015) at daily/0.25 deg temporal/spatial scale. Results demonstrate that the proposed methodology provides rainfall estimates that are superior to both 3B42RT (as well as its successor IMERG-early run) and SM2RAIN-CCI in terms of both median bias, random errors and categorical scores. The study confirms that satellite soil moisture-derived rainfall can provide valuable information for improving state-of-the-art satellite precipitation products, thus making them more attractive for water resource management and large scale flood forecasting applications. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139236837&site=ehost-live
438,Development and Evaluation of Ensemble Consensus Precipitation Estimates over High Mountain Asia.,Viviana Maggioni,Journal of Hydrometeorology,1525755X,,Sep2022,23,9,1469,18.0,159348296,10.1175/JHM-D-21-0196.1,American Meteorological Society,Article,ATMOSPHERIC circulation; HYDROLOGIC cycle; COMMUNITIES; RAINFALL; ASIA,Asia; Climate variability; Climatology; Ensembles; Forcing; Hydrologic cycle; Precipitation; Rainfall; Snowfall,"Precipitation estimates are highly uncertain in complex regions such as High Mountain Asia (HMA), where ground measurements are very difficult to obtain and atmospheric dynamics poorly understood. Though gridded products derived from satellite-based observations and/or reanalysis can provide temporally and spatially distributed estimates of precipitation, there are significant inconsistencies in these products. As such, to date, there is little agreement in the community on the best and most accurate gridded precipitation product in HMA, which is likely area dependent because of HMA's strong heterogeneities and complex orography. Targeting these gaps, this article presents the development of a consensus ensemble precipitation product using three gridded precipitation datasets [the Integrated Multi-satellitE Retrievals for Global Precipitation Measurement (IMERG), the Climate Hazards Group Infrared Precipitation with Station data (CHIRPS), and the ECMWF reanalysis ERA5] with a localized probability matched mean (LPM) approach. We evaluate the performance of the LPM estimate along with a simple ensemble mean (EM) estimate to overcome the differences and disparities of the three selected constituent products on long-term averages and trends in HMA. Our analysis demonstrates that LPM reduces the high biases embedded in the ensemble members and provides more realistic spatial patterns compared to EM. LPM is also a good alternative for merging data products with different spatiotemporal resolutions. By filtering disparities among the individual ensemble members, LPM overcomes the problem of a certain product performing well only in a particular area and provides a consensus estimate with plausible temporal trends. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrometeorology is the property of American Meteorological Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159348296&site=ehost-live
439,Investigating the Error Propagation from Satellite-Based Input Precipitation to Output Water Quality Indicators Simulated by a Hydrologic Model.,Viviana Maggioni,Remote Sensing,20724292,,Nov2020,12,22,3728,1.0,147276947,10.3390/rs12223728,MDPI,Article,WATER quality; TOTAL suspended solids; RAIN gauges; HYDROLOGIC models; WATER temperature; HYDROLOGY; WASHINGTON (D.C.); Other Measuring and Controlling Device Manufacturing,CMORPH; HSPF; modeling; PERSIANN; satellite-based precipitation products; TMPA; water quality,"This study investigated the propagation of errors in input satellite-based precipitation products (SPPs) on streamflow and water quality indicators simulated by a hydrological model in the Occoquan Watershed, located in the suburban Washington, D.C. area. A dense rain gauge network was used as reference to evaluate three SPPs which are based on different retrieval algorithms. A Hydrologic Simulation Program-FORTRAN (HSPF) hydrology and water quality model was forced with the three SPPs to simulate output of streamflow (Q), total suspended solids (TSS), stream temperature (TW), and dissolved oxygen (DO). Results indicate that the HSPF model may have a dampening effect on the precipitation-to-streamflow error. The bias error propagation of all three SPPs showed a positive dependency on basin scale for streamflow and TSS, but not for TW and DO. On a seasonal basis, bias error propagation varied by product, with larger values generally found in fall and winter. This study demonstrated that the spatiotemporal variability of SPPs, along with their algorithms to estimate precipitation, have an influence on water quality simulations in a hydrologic model. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147276947&site=ehost-live
440,"Modeling Satellite Precipitation Errors Over Mountainous Terrain: The Influence of Gauge Density, Seasonality, and Temporal Resolution.",Viviana Maggioni,IEEE Transactions on Geoscience & Remote Sensing,01962892,,Jul2017,55,7,4130,11.0,124146582,10.1109/TGRS.2017.2688998,IEEE,Article,METEOROLOGICAL precipitation; NATURAL satellites; HYDROLOGY; METEOROLOGICAL observations; RAINFALL intensity duration frequencies,Algorithm design and analysis; Complex terrain; Correlation; Estimation; precipitation; Rain; satellite observations; Satellites; Spatial resolution,"This paper contributes to the predictive understanding of satellite precipitation estimation errors over complex terrain, which is fundamental to the development of error models for improving hydrological applications. This paper focuses on the Trentino-Alto Adige region of the eastern Italian Alps. Rainfall observations over a 10-year period (2000–2009) from a dense rain gauge network in the region are used as reference precipitation. A number of satellite precipitation error properties (probability of detection, false alarm rates, missed events, spatial correlation of the error, and hit biases) are investigated in terms of seasonality, satellite algorithm, rainfall intensity, gauge density, and temporal resolution dependencies. These error parameters are typically used in error models (e.g., SREM2D) and provide the basis for enhancing error scheme development. Three widely used satellite-based precipitation products are employed: 1) the Climate Prediction Center morphing product; 2) the precipitation estimation from remotely sensed imagery using artificial neural networks; and 3) the Tropical Rainfall Measuring Mission multisatellite precipitation analysis 3B42 near-real-time product. The three products show similar performances, with larger errors during the warm season, characterized by convective storms, and less variability in the cold season, characterized by more organized stratiform systems. Lower biases are depicted at the daily scale with respect to the 3-hourly resolution. The SREM2D error model has the ability to correct the satellite precipitation products, even though attention is needed for potential systematic errors when applying the calibrated model to independent periods or regions. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Geoscience & Remote Sensing is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124146582&site=ehost-live
441,On the performance of satellite precipitation products in riverine flood modeling: A review.,Viviana Maggioni,Journal of Hydrology,00221694,,Mar2018,558,,214,11.0,128126433,10.1016/j.jhydrol.2018.01.039,Elsevier B.V.,Article,FLOODS; NATURAL satellites; METEOROLOGICAL precipitation; RAINFALL; HYDROLOGIC models; FLOOD forecasting; MATHEMATICAL models,Floods; Hydrologic modeling; Satellite precipitation,"This work is meant to summarize lessons learned on using satellite precipitation products for riverine flood modeling and to propose future directions in this field of research. Firstly, the most common satellite precipitation products (SPPs) during the Tropical Rainfall Measuring Mission (TRMM) and Global Precipitation Mission (GPM) eras are reviewed. Secondly, we discuss the main errors and uncertainty sources in these datasets that have the potential to affect streamflow and runoff model simulations. Thirdly, past studies that focused on using SPPs for predicting streamflow and runoff are analyzed. As the impact of floods depends not only on the characteristics of the flood itself, but also on the characteristics of the region (population density, land use, geophysical and climatic factors), a regional analysis is required to assess the performance of hydrologic models in monitoring and predicting floods. The performance of SPP-forced hydrological models was shown to largely depend on several factors, including precipitation type, seasonality, hydrological model formulation, topography. Across several basins around the world, the bias in SPPs was recognized as a major issue and bias correction methods of different complexity were shown to significantly reduce streamflow errors. Model re-calibration was also raised as a viable option to improve SPP-forced streamflow simulations, but caution is necessary when recalibrating models with SPP, which may result in unrealistic parameter values. From a general standpoint, there is significant potential for using satellite observations in flood forecasting, but the performance of SPP in hydrological modeling is still inadequate for operational purposes. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128126433&site=ehost-live
442,Propagation of satellite precipitation uncertainties through a distributed hydrologic model: A case study in the Tocantins–Araguaia basin in Brazil.,Viviana Maggioni,Journal of Hydrology,00221694,,Aug2015,527,,943,15.0,108322177,10.1016/j.jhydrol.2015.05.042,Elsevier B.V.,Article,METEOROLOGICAL precipitation; HYDROLOGIC models; STREAMFLOW; RAINFALL; ARAGUAIA River (Brazil),Satellite rainfall; Streamflow ensemble; Tropical basin; Uncertainties precipitation,"Summary This study investigates the applicability of error corrections to satellite-based precipitation products in streamflow simulations. A three-year time series (2008–2011) is considered across 19 sub-basins of the Tocantins–Araguaia basin (764,000 km 2 ), located in the center-north region of Brazil. A raingauge network (24 h accumulation) of approximately 300 collection points (∼1 gauge every 2500 km 2 ) is used as reference for evaluating the following four satellite rainfall products: the Tropical Rainfall Measuring Mission real-time 3B42 product (3B42RT), the Climate Prediction Center morphing technique (CMORPH), the Global Satellite Mapping of Precipitation (GSMaP), and the NOAA Hydroestimator (HYDRO-E). Ensemble streamflow simulations, for both dry and rainy seasons, are obtained by forcing the Distributed Hydrological Model developed by the Brazilian National Institute for Space Research (MHD–INPE) with the satellite rainfall products, corrected using a two-dimensional stochastic satellite rainfall error model (SREM2D). The ensemble simulations are evaluated using streamflow output derived by forcing the model with reference rainfall gauge data. SREM2D is able to correct for errors in the satellite precipitation data by pushing the modeled streamflow ensemble closer to the reference river discharge, when compared to the simulations forced with uncorrected rainfall input. Ensemble streamflow error statistics (MAE and RMSE) show a decreasing trend as a function of the catchment area for all satellite products, but the rainfall-to-streamflow error propagation does not show any dependence on the basin size. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108322177&site=ehost-live
443,The Global Satellite Precipitation Constellation: Current Status and Future Requirements.,Viviana Maggioni,Bulletin of the American Meteorological Society,00030007,,Oct2021,102,10,E1844,18.0,153454292,10.1175/BAMS-D-20-0299.1,American Meteorological Society,Article,SPATIAL resolution; PRECIPITATION variability; METEOROLOGICAL satellites; CONSTELLATIONS; RADIOMETERS,Instrumentation/sensors; Microwave observations; Precipitation; Rainfall; Satellite observations; Snowfall,"To address the need to map precipitation on a global scale, a collection of satellites carrying passive microwave (PMW) radiometers has grown over the last 20 years to form a constellation of about 10–12 sensors at any one time. Over the same period, a broad range of science and user communities has become increasingly dependent on the precipitation products provided by these sensors. The constellation presently consists of both conical and cross-track-scanning precipitation-capable multichannel instruments, many of which are beyond their operational and design lifetime but continue to operate through the cooperation of the responsible agencies. The Group on Earth Observations and the Coordinating Group for Meteorological Satellites (CGMS), among other groups, have raised the issue of how a robust, future precipitation constellation should be constructed. The key issues of current and future requirements for the mapping of global precipitation from satellite sensors can be summarized as providing 1) sufficiently fine spatial resolutions to capture precipitation-scale systems and reduce the beam-filling effects of the observations; 2) a wide channel diversity for each sensor to cover the range of precipitation types, characteristics, and intensities observed across the globe; 3) an observation interval that provides temporal sampling commensurate with the variability of precipitation; and 4) precipitation radars and radiometers in low-inclination orbit to provide a consistent calibration source, as demonstrated by the first two spaceborne radar–radiometer combinations on the Tropical Rainfall Measuring Mission (TRMM) and Global Precipitation Measurement (GPM) mission Core Observatory. These issues are critical in determining the direction of future constellation requirements while preserving the continuity of the existing constellation necessary for long-term climate-scale studies. [ABSTRACT FROM AUTHOR] Copyright of Bulletin of the American Meteorological Society is the property of American Meteorological Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153454292&site=ehost-live
444,The impact of weather condition and social activity on COVID-19 transmission in the United States.,Viviana Maggioni,Journal of Environmental Management,03014797,,Jan2022:Part A,302,,N.PAG,1.0,153848748,10.1016/j.jenvman.2021.114085,Academic Press Inc.,Article,COVID-19; COVID-19 pandemic; WEATHER; SOCIAL history; HUMIDITY; UNITED States,COVID-19 transmission; Machine learning; Random forest regression model; Social activity factor; Weather condition,"The coronavirus disease 2019 (COVID-19) has been first reported in December 2019 and rapidly spread worldwide. As other severe acute respiratory syndromes, it is a widely discussed topic whether seasonality affects the COVID-19 infection spreading. This study presents two different approaches to analyse the impact of social activity factors and weather variables on daily COVID-19 cases at county level over the Continental U.S. (CONUS). The first one is a traditional statistical method, i.e., Pearson correlation coefficient, whereas the second one is a machine learning algorithm, i.e., random forest regression model. The Pearson correlation is analysed to roughly test the relationship between COVID-19 cases and the weather variables or the social activity factor (i.e. social distance index). The random forest regression model investigates the feasibility of estimating the number of county-level daily confirmed COVID-19 cases by using different combinations of eight factors (county population, county population density, county social distance index, air temperature, specific humidity, shortwave radiation, precipitation, and wind speed). Results show that the number of daily confirmed COVID-19 cases is weakly correlated with the social distance index, air temperature and specific humidity through the Pearson correlation method. The random forest model shows that the estimation of COVID-19 cases is more accurate with adding weather variables as input data. Specifically, the most important factors for estimating daily COVID-19 cases are the population and population density, followed by the social distance index and the five weather variables, with temperature and specific humidity being more critical than shortwave radiation, wind speed, and precipitation. The validation process shows that the general values of correlation coefficients between the daily COVID-19 cases estimated by the random forest model and the observed ones are around 0.85. • This study investigates the impact of social activity factors and weather variables on COVID-19 transmission in the U.S. • The random forest model shows the estimation of COVID-19 cases is more accurate with adding weather variables as input data. • The random forest model estimated number of COVID-19 cases are highly correlated with the observation data. [ABSTRACT FROM AUTHOR] Copyright of Journal of Environmental Management is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153848748&site=ehost-live
445,The impact of weather condition and social activity on COVID-19 transmission in the United States.,Viviana Maggioni,Journal of Environmental Management,03014797,,Jan2022:Part B,302,,N.PAG,1.0,153848842,10.1016/j.jenvman.2021.114085,Academic Press Inc.,Article,COVID-19; COVID-19 pandemic; WEATHER; SOCIAL history; HUMIDITY; UNITED States,COVID-19 transmission; Machine learning; Random forest regression model; Social activity factor; Weather condition,"The coronavirus disease 2019 (COVID-19) has been first reported in December 2019 and rapidly spread worldwide. As other severe acute respiratory syndromes, it is a widely discussed topic whether seasonality affects the COVID-19 infection spreading. This study presents two different approaches to analyse the impact of social activity factors and weather variables on daily COVID-19 cases at county level over the Continental U.S. (CONUS). The first one is a traditional statistical method, i.e., Pearson correlation coefficient, whereas the second one is a machine learning algorithm, i.e., random forest regression model. The Pearson correlation is analysed to roughly test the relationship between COVID-19 cases and the weather variables or the social activity factor (i.e. social distance index). The random forest regression model investigates the feasibility of estimating the number of county-level daily confirmed COVID-19 cases by using different combinations of eight factors (county population, county population density, county social distance index, air temperature, specific humidity, shortwave radiation, precipitation, and wind speed). Results show that the number of daily confirmed COVID-19 cases is weakly correlated with the social distance index, air temperature and specific humidity through the Pearson correlation method. The random forest model shows that the estimation of COVID-19 cases is more accurate with adding weather variables as input data. Specifically, the most important factors for estimating daily COVID-19 cases are the population and population density, followed by the social distance index and the five weather variables, with temperature and specific humidity being more critical than shortwave radiation, wind speed, and precipitation. The validation process shows that the general values of correlation coefficients between the daily COVID-19 cases estimated by the random forest model and the observed ones are around 0.85. • This study investigates the impact of social activity factors and weather variables on COVID-19 transmission in the U.S. • The random forest model shows the estimation of COVID-19 cases is more accurate with adding weather variables as input data. • The random forest model estimated number of COVID-19 cases are highly correlated with the observation data. [ABSTRACT FROM AUTHOR] Copyright of Journal of Environmental Management is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153848842&site=ehost-live
446,The Joint Assimilation of Remotely Sensed Leaf Area Index and Surface Soil Moisture into a Land Surface Model.,Viviana Maggioni,Remote Sensing,20724292,,Feb2022,14,3,437,1.0,155266389,10.3390/rs14030437,MDPI,Article,LEAF area index; SOIL moisture; SURFACE area; STANDARD deviations,data assimilation; Ensemble Kalman Filter; evapotranspiration; GLASS; net ecosystem exchange; SMAP,"This work tests the hypothesis that jointly assimilating satellite observations of leaf area index and surface soil moisture into a land surface model improves the estimation of land vegetation and water variables. An Ensemble Kalman Filter is used to test this hypothesis across the Contiguous United States from April 2015 to December 2018. The performance of the proposed methodology is assessed for several modeled vegetation and water variables (evapotranspiration, net ecosystem exchange, and soil moisture) in terms of random errors and anomaly correlation coefficients against a set of independent validation datasets (i.e., Global Land Evaporation Amsterdam Model, FLUXCOM, and International Soil Moisture Network). The results show that the assimilation of the leaf area index mostly improves the estimation of evapotranspiration and net ecosystem exchange, whereas the assimilation of surface soil moisture alone improves surface soil moisture content, especially in the western US, in terms of both root mean squared error and anomaly correlation coefficient. The joint assimilation of vegetation and soil moisture information combines the results of individual vegetation and soil moisture assimilations and reduces errors (and increases correlations with the reference datasets) in evapotranspiration, net ecosystem exchange, and surface soil moisture simulated by the land surface model. However, because soil moisture satellite observations only provide information on the water content in the top 5 cm of the soil column, the impact of the proposed data assimilation technique on root zone soil moisture is limited. This work moves one step forward in the direction of improving our estimation and understanding of land surface interactions using a multivariate data assimilation approach, which can be particularly useful in regions of the world where ground observations are sparse or missing altogether. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155266389&site=ehost-live
447,Towards hyper-resolution land-surface modeling of surface and root zone soil moisture.,Viviana Maggioni,Journal of Hydrology,00221694,,Mar2021,594,,N.PAG,1.0,148984348,10.1016/j.jhydrol.2020.125945,Elsevier B.V.,Article,SOIL moisture; METEOROLOGICAL precipitation; WIND speed; WATER management; ATMOSPHERIC temperature; WATER supply; OKLAHOMA; Water Supply and Irrigation Systems,Hyper-resolution; Land surface modeling; Soil moisture,"• A new framework for modeling hyper-resolution soil moisture is developed. • Finer resolution forcing data improve modeled surface and root-zone soil moisture. • The resolution of input precipitation plays a critical role in improving soil moisture. The goal of this work is to estimate surface and root zone soil moisture at resolutions that are useful for decision making and water resources management. A 500-m atmospheric forcing dataset is developed from the 12.5-km NLDAS-2 (North America Land Data Assimilation System) products across Oklahoma, where high-quality observations are available for validation purposes. A land surface model is then forced with three combinations of input variables to simulate surface and root zone soil moisture: 1) NLDAS-2 atmospheric forcings at their original resolution; 2) downscaled NLDAS-2 atmospheric variables (i.e., near-surface air temperature and humidity, wind speed and direction, incident longwave and shortwave radiation, pressure) and original resolution NLDAS-2 precipitation; and 3) downscaled NLDAS-2 atmospheric variables and precipitation. Results show that the third simulation is able to bring modeled standard-normal deviates of both surface and root zone soil moisture closer to in-situ observations, whereas the second simulation only shows slight improvements with respect to one forced with original resolution NLDAS-2 data. This is particularly evident for negative values of standard-normal deviates, which correspond to drier than usual cases, due to the improved ability of the downscaled precipitation to detect missed events and no-rain cases. In summary, finer resolution forcings have the potential to improve simulations of soil moisture and the resolution of precipitation plays a critical role in improving time series of soil moisture standard-normal deviates. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148984348&site=ehost-live
448,"Using remote sensing and modeling techniques to investigate the annual parasite incidence of malaria in Loreto, Peru.",Viviana Maggioni,Advances in Water Resources,03091708,,Oct2017,108,,423,16.0,125374606,10.1016/j.advwatres.2016.11.009,Elsevier B.V.,Article,MALARIA; PLASMODIUM; DISEASE incidence; REMOTE sensing; LORETO (Peru : Dept.),Climate; Environment; Malaria; Modeling; Peru; Remote sensing,"Between 2001 and 2010 significant progress was made towards reducing the number of malaria cases in Peru; however, the country saw an increase between 2011 and 2015. This work attempts to uncover the associations among various climatic and environmental variables and the annual malaria parasite incidence in the Peruvian region of Loreto. A Multilevel Mixed-effects Poisson Regression model is employed, focusing on the 2009–2013 period, when trends in malaria incidence shifted from decreasing to increasing. The results indicate that variations in elevation (β = 0.78; 95% confidence interval (CI), 0.75–0.81), soil moisture (β = 0.0021; 95% CI, 0.0019–0.0022), rainfall (β = 0.59; 95% CI, 0.56–0.61), and normalized difference vegetation index (β = 2.13; 95% CI, 1.83–2.43) is associated with higher annual parasite incidence, whereas an increase in temperature (β = -0.0043; 95% CI, − 0.0044-− 0.0041) is associated with a lower annual parasite incidence. The results from this study are particularly useful for healthcare workers in Loreto and have the potential of being integrated within malaria elimination plans. [ABSTRACT FROM AUTHOR] Copyright of Advances in Water Resources is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125374606&site=ehost-live
449,Using Satellite Error Modeling to Improve GPM-Level 3 Rainfall Estimates over the Central Amazon Region.,Viviana Maggioni,Remote Sensing,20724292,,Feb2018,10,2,336,14.0,128347569,10.3390/rs10020336,MDPI,Article,MEASUREMENT errors; METEOROLOGICAL precipitation; REMOTE sensing; RAINFALL; SPATIAL distribution (Quantum optics),Amazon; error model; global precipitation measurement; IMERG; PUSH; validation,"This study aims to assess the characteristics and uncertainty of Integrated Multisatellite Retrievals for Global Precipitation Measurement (GPM) (IMERG) Level 3 rainfall estimates and to improve those estimates using an error model over the central Amazon region. The S-band Amazon Protection National System (SIPAM) radar is used as reference and the Precipitation Uncertainties for Satellite Hydrology (PUSH) framework is adopted to characterize uncertainties associated with the satellite precipitation product. PUSH is calibrated and validated for the study region and takes into account factors like seasonality and surface type (i.e., land and river). Results demonstrated that the PUSH model is suitable for characterizing errors in the IMERG algorithm when compared with S-band SIPAM radar estimates. PUSH could efficiently predict the satellite rainfall error distribution in terms of spatial and intensity distribution. However, an underestimation (overestimation) of light satellite rain rates was observed during the dry (wet) period, mainly over rivers. Although the estimated error showed a lower standard deviation than the observed error, the correlation between satellite and radar rainfall was high and the systematic error was well captured along the Negro, Solimões, and Amazon rivers, especially during the wet season. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128347569&site=ehost-live
450,A Derived Optimal Linear Interpolation approach for merging multiple satellite soil moisture-based rainfall products with IMERG early run.,Viviana Maggioni,Geophysical Research Abstracts,10297006,,2019,21,,1,1.0,140488939,,Copernicus Gesellschaft mbH,Article,"SOIL moisture; EARTH system science; SOIL moisture measurement; MOISTURE measurement; WEATHER forecasting; RAINFALL; HYDROLOGIC cycle; SEAWATER salinity; All Other Professional, Scientific, and Technical Services",,"As a natural feature of the Earth's weather system, rainfall is the main driver of the hydrological cycle. Rainfall plays an essential role in many applications including climate monitoring, extreme weather prediction and weather forecasting. On a global scale, ground-monitoring networks do not provide sufficient coverage and satellite rainfall products are often the only source of rainfall that guarantee a continuous temporal coverage. However, the indirect and the instantaneous nature of the measurement makes satellite rainfall products prone to errors (Kucera et al., 2013). Thanks to the strong connection between soil moisture and precipitation, capable to track accumulated precipitation estimates (rather than instantaneous), soil moisture can be successfully used to enhance the quality of satellite rainfall observations (Crow et al., 2011; Pellarin et al., 2013; Brocca et al. 2014). The SMOS+rainfall project of the European Space Agency (ESA), started in 2015 and concluded in 2017, has demonstrated the capability of the SMOS soil moisture product to enhance satellite rainfall information over land and has raised many interesting research questions related to the potential improvement that can be obtained by a combination of different soil moisture sensors. Here, we propose the use of a new near real time purely observational rainfall dataset derived from the combination of the Integrated Multi-Satellite Retrievals for GPM (IMERG early run) with multiple satellite rainfall products obtained from the inversion of the soil moisture retrievals derived from: 1) the Soil Moisture Active and Passive (SMAP) mission, 2) the Advanced Scatterometer (ASCAT) and 3) the Soil Moisture and Ocean Salinity (SMOS) mission via SM2RAIN (Brocca et al. 2014).The weighting method (Hobeichi et al. 2018) is based on a technique that provides an analytically optimal linear combination of rainfall products and accounts for both the performance differences and error covariance between the participating products. We examine the performance of the weighting approach in India, United States, Australia and Europe showing that the simultaneous use of soil moisture products is able to increase the quality of IMERG early run product and its performance for hydrological applications.Brocca et al., 2014. Soil as a natural rain gauge: estimating global rainfall from satellite soil moisture data. J. Geophy. Res. 119 (9), 5128–5141.Crow et al., 2011. Correcting rainfall using satellite-based surface soil moisture retrievals: the soil moisture analysis rainfall tool (SMART). Water Resour. Res. 47, W08521.Kucera et al. 2013. Precipitation from space: advancing earth system science. Bull. Am. Meteorol. Soc. 94, 365–375.Pellarin et al. 2013. A simple and effective method for correcting soil moisture and precipitation estimates using AMSR-E measurements. Remote Sens. Environ. 136, 28–36.Hobeichi et al. 2018. Derived Optimal Linear Combination Evapotranspiration (DOLCE): a global gridded synthesis ET estimate, Hydrol. Earth Syst. Sci., 22, 1317-1336. [ABSTRACT FROM AUTHOR] Copyright of Geophysical Research Abstracts is the property of Copernicus Gesellschaft mbH and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140488939&site=ehost-live
451,Applying a precipitation error model to numerical weather predictions for probabilistic flood forecasts.,Viviana Maggioni,Journal of Hydrology,00221694,,Jul2021,598,,N.PAG,1.0,150933051,10.1016/j.jhydrol.2021.126374,Elsevier B.V.,Article,"NUMERICAL weather forecasting; FLOOD forecasting; WEATHER forecasting; STOCHASTIC models; BRAZIL; All Other Professional, Scientific, and Technical Services",Ensemble flood forecasting; Ensemble prediction system; Numerical weather prediction; Stochastic error model,"• A stochastic error model was applied to numerical weather predictions. • Rainfall ensemble forecasting fields were generated using the error model. • The error model proved to be efficient to remove forecasts biases. • Error model flood forecasts performed similarly to expensive consecrated techniques. This work investigates the use of a stochastic error model (the 2-Dimensional Satellite Rainfall Error Model-SREM2D) to generate an ensemble of rainfall fields, based on the forecasts from the Eta regional weather forecast model. To evaluate the usefulness of this approach against traditional techniques, streamflow probabilistic forecasts from a distributed hydrological model forced with two sources of rainfall data are compared in the Tocantins-Araguaia basin in Brazil. The first dataset is an empirical rainfall ensemble produced by the SREM2D model applied to the Eta model, and the second is a state-of-the-art rainfall ensemble produced by the ECMWF model. Results show the potential of the stochastic error model to generate precipitation ensemble fields from a regional numerical weather forecasting model removing around 60% and 12% of the systematic and random error, respectively. Moreover, SREM2D is proven to be an efficient technique that involves a low computational cost when compared to the more sophisticated ensemble techniques used by the ECMWF model. [ABSTRACT FROM AUTHOR] Copyright of Journal of Hydrology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150933051&site=ehost-live
452,Data Assimilation of Terrestrial Water Storage Observations to Estimate Precipitation Fluxes: A Synthetic Experiment.,Viviana Maggioni,Remote Sensing,20724292,,Mar2021,13,6,1223,1.0,149574591,10.3390/rs13061223,MDPI,Article,WATER storage; SOIL moisture; FLUX (Energy); STREAMFLOW; RUNOFF,data assimilation; GRACE; precipitation; TWS,"The Gravity Recovery and Climate Experiment (GRACE) mission and its Follow-On (GRACE-FO) mission provide unprecedented observations of terrestrial water storage (TWS) dynamics at basin to continental scales. Established GRACE data assimilation techniques directly adjust the simulated water storage components to improve the estimation of groundwater, streamflow, and snow water equivalent. Such techniques artificially add/subtract water to/from prognostic variables, thus upsetting the simulated water balance. To overcome this limitation, we propose and test an alternative assimilation scheme in which precipitation fluxes are adjusted to achieve the desired changes in simulated TWS. Using a synthetic data assimilation experiment, we show that the scheme improves performance skill in precipitation estimates in general, but that it is more robust for snowfall than for rainfall, and it fails in certain regions with strong horizontal gradients in precipitation. The results demonstrate that assimilation of TWS observations can help correct (adjust) the model's precipitation forcing and, in turn, enhance model estimates of TWS, snow mass, soil moisture, runoff, and evaporation. A key limitation of the approach is the assumption that all errors in TWS originate from errors in precipitation. Nevertheless, the proposed approach produces more consistent improvements in simulated runoff than the established GRACE data assimilation techniques. [ABSTRACT FROM AUTHOR] Copyright of Remote Sensing is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149574591&site=ehost-live
453,Estimating uncertainties associated with quasi-global satellite infrared-based retrievals over land.,Viviana Maggioni,Geophysical Research Abstracts,10297006,,2019,21,,1,1.0,140485959,,Copernicus Gesellschaft mbH,Article,PRECIPITATION variability; HYDROLOGIC cycle; CLIMATE change; FARM management; ORBITS of artificial satellites; ARTIFICIAL satellites; PRECIPITATION (Chemistry); Space Research and Technology; Support activities for crop production; Farm Management Services,,"An accurate characterization of the global hydrologic cycle is essential not only to study and forecast climate variations, but also for extreme event mitigation and agricultural planning. Since precipitation is the major driving force of the hydrological cycle, current and future satellite missions with a focus on precipitation are critical to estimate hydrological variables globally. Error estimates associated with satellite precipitation retrievals are crucial to allow inferences about the reliability of such products in their operational applications. However, evaluating satellite precipitation error characteristics is challenging because of the inherent temporal and spatial variability of precipitation, measurement errors, and sampling uncertainties, especially at fine temporal and spatial resolutions.This study proposes to use a stochastic error model – PUSH (Probability Uncertainty in Satellite Hydrology) – for estimating uncertainties associated with fine resolution satellite precipitation products. The framework is tested on the daily IMERG (Integrated Multi-satellitE Retrievals for GPM) infrared-only (IR) precipitation component using a satellite-based radar product (the Level-3 Dual-frequency Precipitation Radar, 3DPRD) as reference. PUSH decomposes the error into four components and employs different modeling approaches for each case: correct no-precipitation detection; missed precipitation; false alarm; hit bias. PUSH is calibrated globally over land for different climatological regions. The calibrated parameters are validated using an independent period to verify whether they can be applied to estimate uncertainties associated with future IR retrievals without degrading the model performance. The four error components are then investigated as a function of climate region to study their spatial variability. [ABSTRACT FROM AUTHOR] Copyright of Geophysical Research Abstracts is the property of Copernicus Gesellschaft mbH and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140485959&site=ehost-live
454,Investigating the GPM Dual‐frequency Precipitation Radar signatures of low‐level precipitation enhancement.,Viviana Maggioni,Quarterly Journal of the Royal Meteorological Society,00359009,,Oct2019,145,724,3161,14.0,139587645,10.1002/qj.3611,"John Wiley & Sons, Inc.",Article,RADAR meteorology; METEOROLOGICAL precipitation; RADAR; LANDSLIDES; NATURAL disasters; MICROPHYSICS; RAINFALL,collision‐coalescence; DPR; GPM; microphysics; precipitation enhancement; radar,"High‐intensity precipitation represents a threat for several regions of the world because of the related risk of natural disasters (e.g. floods and landslides). This work focuses on low‐level precipitation enhancement that occurs in the cloud warm layer and has been observed in relation to collision‐coalescence (CC) leading to flash floods and extreme rainfall events in tropical and temperate latitudes. Specifically, signatures of precipitation enhancement (referred to as CC‐dominant precipitation) are investigated in the observations from the Global Precipitation Measurement (GPM) core mission Dual‐frequency Precipitation Radar (DPR) over the central/eastern Contiguous United States (CONUS) during June 2014–May 2018. A classification scheme for CC‐dominant precipitation, developed for dual‐polarization S‐band radar measurements and applied in a previous work to X‐band radar observations in complex terrain, is used as a benchmark. The scheme is here applied to the GPM ground validation dataset that matches ground‐based radar observations across CONUS to space‐borne DPR retrievals. The occurrence of CC‐dominant precipitation is documented and the corresponding signatures of CC‐dominant precipitation at Ku‐ and Ka‐band are studied. CC‐dominant profiles show distinguishing features when compared to profiles not dominated by CC, e.g. characteristic vertical slopes of reflectivity at Ku‐ and Ka‐band in the liquid layer, lower freezing‐level height, and shallower ice layer, which are linked to environmental conditions driving the peculiar CC microphysics. This work aims at improving satellite quantitative precipitation estimation, particularly GPM retrievals, by targeting CC development in precipitation columns. [ABSTRACT FROM AUTHOR] Copyright of Quarterly Journal of the Royal Meteorological Society is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139587645&site=ehost-live
465,An optimal stopping approach to managing travel-time uncertainty for time-sensitive customer pickup.,Elise Miller-Hooks,Transportation Research: Part B,01912615,,Aug2017,102,,22,16.0,123894813,10.1016/j.trb.2017.04.017,Elsevier B.V.,Article,RIDESHARING services; OPTIMAL stopping (Mathematical statistics); VEHICLE routing problem; TAXICABS; STOCHASTIC models; Taxi Service; All Other Support Activities for Transportation,Dial-a-ride; On-line routing; Optimal stopping; Stochastic travel times,"In dynamic vehicle routing, it is common to respond to real-time information with immediate updates to routes and fleet management. However, even if routes are updated continuously, in practice, some decisions once made are difficult to reverse. At times, it may thus be valuable to wait for additional information before acting on a decision. We use the theory of optimal stopping to determine the optimal timing of a recourse action when vehicles are likely to miss customer deadlines due to travel-time stochasticities and backup services are available. The factors involved in making this decision – that is, the likelihood that the primary vehicle will arrive late, the location of the backup vehicle, and value of waiting for additional travel-time information – each change dynamically over time. We develop a recourse model that accounts for this complexity. We formulate the optimal recourse policy as a stochastic dynamic program. Properties of the optimal policy are derived analytically, and its solution is approximated with a binomial lattice method used in the pricing of American options. Finally, we develop a two-stage stochastic optimization approach to show how the opportunity to take recourse dynamically might be integrated into a priori scheduling and routing. The framework is demonstrated for a stochastic dial-a-ride application in which taxis serve as backup to ridesharing vehicles. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part B is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123894813&site=ehost-live
466,Assessing hospital system resilience to disaster events involving physical damage and Demand Surge.,Elise Miller-Hooks,Socio-Economic Planning Sciences,00380121,,Jun2020,70,,N.PAG,1.0,142890640,10.1016/j.seps.2019.07.005,Elsevier B.V.,Article,HOSPITALS; DISASTER resilience; DISCRETE event simulation; MEDICAL care; EMERGENCY medical services; Municipal police services; Emergency and Other Relief Services; General (except paediatric) hospitals; General Medical and Surgical Hospitals,Collaboration; Disaster preparedness; Discrete event simulation; Healthcare resilience; Hospital operations in MCI; Interhospital coalition; Queueing networks,"This paper investigates the effectiveness of formalized collaboration strategies through which patients can be transferred and resources, including staff, equipment and supplies, can be shared across hospitals in response to a disaster incident involving mass casualties and area-wide damage. Inflicted damage can affect hospital infrastructure and its supporting lifelines, thus impacting capacity and capability or, ultimately, services that are provided. Using a discrete event simulation framework and underlying open queuing network conceptualization involving patient flows through 9 critical units of each hospital, impacts on critical resources, physical spaces and demand are modeled and the hospital system's resilience to these hazard events is evaluated. Findings from numerical experiments on a case study involving multiple hospitals spaced over a large metropolitan region replicating a system similar to the Johns Hopkins Hospital System show the potential of strategies involving not only transfers and resource sharing, but also joint capacity enhancement alternatives to improve post-disaster emergency health care service delivery through joint action. • Investigates potential benefits of hospital coalitions in disaster. • Assess potential value of patient transfers and resource sharing. • Considers joint capacity enhancement alternatives. • Discrete event simulation conceptualization of hospital system. • Quantifies hospital system resilience to pandemic, MCI and disaster events with damage. [ABSTRACT FROM AUTHOR] Copyright of Socio-Economic Planning Sciences is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142890640&site=ehost-live
467,Assessing strategies for protecting transportation infrastructure from an uncertain climate future.,Elise Miller-Hooks,Transportation Research Part A: Policy & Practice,09658564,,Nov2017,105,,27,15.0,125235850,10.1016/j.tra.2017.08.010,Elsevier B.V.,Article,FINANCING of transportation; INFRASTRUCTURE (Economics); CLIMATE change; METROPOLITAN areas; INVESTMENTS; Miscellaneous Financial Investment Activities; Investment Advice; All Other Support Activities for Transportation; Other support activities for transportation,Climate change resilience; Critical infrastructure; Expected value of perfect information; Flooding; Protective infrastructure investment planning; Transportation protection; Value of stochastic solution,"This paper investigates the importance of explicitly considering the stochastic nature of future climate impact predictions and predictive accuracy for optimal investment planning in the protection of coastal and inland transportation infrastructure against climate impacts. Such impacts include sea level rise, coastal and riverine flooding resulting from more frequent and intense precipitation events, storms, storm surges and other extreme events. For this purpose, numerical experiments utilizing stochastic optimization based methodologies were conducted on a case study of the Washington, D.C. Greater Metropolitan area proximate to the Potomac River under varying climatic predictions. Results from the numerical experiments suggest a 54% reduction in added costs due to the implementation of chosen protective infrastructure investments. They also indicate a reduction in added costs (capital investment and added delays) on the order of 19% when the investments are chosen to hedge against probable future flooding events as compared with planning for the 50th percentile SLR prediction with associated weather events. A potential gain of nearly 27% in reduced costs through improved predictive accuracy in climatic forecasts is also noted, suggesting significant value in more accurate forecasts. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research Part A: Policy & Practice is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125235850&site=ehost-live
468,Constructs in infrastructure resilience framing – from components to community services and the built and human infrastructures on which they rely.,Elise Miller-Hooks,IISE Transactions,24725854,,Apr2022,,,1,14.0,156476409,10.1080/24725854.2022.2070801,Taylor & Francis Ltd,Article,,built environment; community services; critical lifelines; human infrastructure; Resilience; socio-technical system,"Abstract This article describes five constructs for framing infrastructure resilience estimation. These constructs range from the consideration of a single component to a community service provided through a set of buildings whose functionality relies on interdependent supporting lifelines. A key aim is to explore how the construct that is adopted affects resilience understanding. It discusses the value of reframing the resilience computation around services that are provided by built environments rather than around the built systems themselves. The built environment would provide little in the way of services if not for human involvement and other needed resources. A construct for framing resilience is expanded to incorporate the role of humans as infrastructure, as well as permanent and consumable limiting resources, in creating service capacity. Taking a service-based viewpoint induces a change in perspective with rippling impact. It affects the choice of metrics for measuring resilience, adaptation strategies to include in assessment, baselines for comparison, and elements of the built environment to incorporate in the evaluation. It necessitates consideration of socio-technical concerns. It also brings hidden issues of inequity to the foreground. This article suggests that underlying many resilience studies is an implicit construct for framing resilience, and explores how the construct affects and enables resilience understanding. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156476409&site=ehost-live
469,"Co-opetition in enhancing global port network resiliency: A multi-leader, common-follower game theoretic approach.",Elise Miller-Hooks,Transportation Research: Part B,01912615,,Feb2018,108,,281,18.0,127441948,10.1016/j.trb.2018.01.004,Elsevier B.V.,Article,"COOPETITION; GAME theory; DECISION making; MARITIME shipping; ECONOMIC demand; MARKET share; Marine shipping agencies; Other Support Activities for Water Transportation; Deep sea, coastal and Great Lakes water transportation (except by ferries); Navigational Services to Shipping",Co-opetition; Complementarity optimization; Freight transportation protection; Game theory; Multi-leader common-follower; Port resiliency; Protective infrastructure investment planning,"Ports are key elements of global supply chains, providing connection between land- and maritime-based transportation modes. They operate in cooperative, but competitive, co-opetitive , environments wherein individual port throughput is linked through an underlying transshipment network. Short-term port performance and long-term market share can be significantly impacted by a disaster event; thus, ports plan to invest in capacity expansion and protective measures to increase their reliability or resiliency in times of disruption. To account for the co-opetition among ports, a bi-level multiplayer game theoretic approach is used, wherein each individual port takes protective investment decisions while anticipating the response of the common market-clearing shipping assignment problem in the impacted network. This lower-level assignment is modeled as a cost minimization problem, which allows for consideration of gains and losses from other ports decisions through changes in port and service capacities and port cargo handling times. Linear properties of the lower-level formulation permit reformulation of the individual port bi-level optimization problems as single-level problems by replacing the common lower-level by its equivalent Karush Kuhn Tucker (KKT) conditions. Simultaneous consideration of individual port optimization problems creates a multi-leader, common-follower problem, i.e. an unrestricted game, that is modeled as an Equilibrium Problem with Equilibrium Constraints (EPEC). Equilibria solutions are sought by use of a diagonalization technique. Solutions of unrestricted, semi-restricted and restricted games are analyzed and compared for a hypothetical application from the literature involving ports in East Asia and Europe. The proposed co-opetitive approach was found to lead to increased served total demand, significantly increased market share for many ports and improved services for shippers. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part B is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127441948&site=ehost-live
470,Enhancing resilience through port coalitions in maritime freight networks.,Elise Miller-Hooks,Transportation Research Part A: Policy & Practice,09658564,,Mar2022,157,,1,23.0,155459287,10.1016/j.tra.2022.01.015,Elsevier B.V.,Article,"HARBORS; COALITIONS; DECISION making in investments; FREIGHT & freightage; PORT districts; INDUSTRIAL capacity; SHIPPING rates; Specialized Freight (except Used Goods) Trucking, Local; Other freight transportation arrangement; Freight Transportation Arrangement; Port and Harbor Operations; Other Heavy and Civil Engineering Construction; Regulation and Administration of Transportation Programs",EPEC; Global supply chains; Maritime systems; Port networks; Resilience; Shipping,"• Coalition strategies with capacity sharing/cross-port investment among ports are tested. • Provide equilibrium problem with equilibrium constraints conceptualization of problem. • Give evidence that coalitions can improve system resilience and demand fulfillment rates. • Found positive returns on investment for individual and network of ports via coalition. • Support policies that increase market share through increased reliability of service routes. Reliable port services are key to maritime freight transport system performance. These systems are vulnerable to disasters of anthropogenic or natural cause, which can significantly impact port capacity, handling times and overall system performance. To improve resilience of individual ports, strategies involving capacity sharing and protective cross-port investments through coalition formation are proposed. This collaborative port protection and investment approach to improve individual and system-level port resilience is formulated as an Equilibrium Problem with Equilibrium Constraints. That is, the program is bi-level with multiple players in the upper level and a common liner shipping problem in the lower level. Its solution is obtained at a Nash equilibrium wherein no port stakeholder can achieve better performance by unilaterally changing its investment plan. A Stackelberg equilibrium between upper and lower levels infers that best investment decisions are made given competition between ports and the market's response to improvements. The benefits of regional coalitions in this co-opetitive (competitive and collaborative) environment in terms of port and system resilience, port- and system-level demand fulfilment rates and return on investment are investigated from multiple perspectives, including the perspectives of shippers, port owners and the larger shipping network. With insights gained through study of the proposed coalition policies, this work aims to facilitate port authorities in making decisions on port capacity expansion, infrastructure investment and forming strategic partnerships. Shipping companies may also take into consideration the ability of a port to provide service under disruption events when choosing which ports to include in their service loops. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research Part A: Policy & Practice is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155459287&site=ehost-live
471,Maritime port network resiliency and reliability through co-opetition.,Elise Miller-Hooks,Transportation Research: Part E,13665545,,May2020,137,,N.PAG,1.0,142890650,10.1016/j.tre.2020.101916,Elsevier B.V.,Article,"COOPETITION; HARBOR management; FREIGHT & freightage; HARBORS; TRADE routes; INTERMODAL freight terminals; RELIABILITY in engineering; Other freight transportation arrangement; Freight Transportation Arrangement; Specialized Freight (except Used Goods) Trucking, Local; Other Heavy and Civil Engineering Construction; Port and Harbor Operations",Co-opetitive games; Maritime network; Ports; Protective infrastructure investment; Reliability; Resiliency; Stackelberg game; Stochastic optimization,"• Models for assessing and improving resiliency and reliability of port networks. • Stochastic co-opetitive formulations as stochastic Nash and Stackelberg games. • Accounting for market interactions, multiple hazards, investments, co-opetition. • Risk-averse (worst-case) and risk-neutral (expected value) strategies. • Stochastic, centralized methods as benchmark. Local and global economies are for many nations highly dependent on the import and export of goods. These goods are shipped through global intermodal (IM) freight land-water transportation systems that rely on truck, rail and maritime networks and their IM terminals. These terminals are crucial to creating and maintaining efficient international trade routes. This paper considers port reliability and resilience, as well as the role of ports in supporting a larger resilient maritime system. Specifically, stochastic, bi-level, game theoretic optimization models for assessing and improving the resiliency and reliability of the global port network are presented. Proposed models are devised for a set of independent ports with interacting investment problems for competitive, but potentially cooperative (co-opetitive) environments. Uncertainties in traversal times and port throughput capacities are accounted for by adopting a stochastic optimization method using expected or max-min functions to simultaneously hedge against the consequences of multiple possible future port-related disaster events. Alternative centralized, but stochastic formulations are also provided. This stochastic, co-opetitive methodology and alternative centralized methods fill an important gap in the maritime resiliency literature. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part E is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142890650&site=ehost-live
472,Optimal time-differentiated pricing for a competitive mixed traditional and crowdsourced event parking market.,Elise Miller-Hooks,Transportation Research: Part C,0968090X,,Nov2021,132,,N.PAG,1.0,153528935,10.1016/j.trc.2021.103409,Elsevier B.V.,Article,PARKING garages; PARKING facilities; PROXIMITY spaces; SET functions; FUNCTION spaces; Parking Lots and Garages,Crowdsourcing; Emerging markets; Equilibrium; Event management; Multi-player game; Pricing,"• Mathematically conceptualizes a crowdsourced parking market mechanism. • Exploits a multi-period, stochastic EPEC formulation. • Reservation-time based pricing is enabled through price differentiation for late comers. • Uses a diagonalization method with embedded gradient ascent approach for solution. • Supply- and demand-side uncertainties are explicitly modeled. An event-based parking pricing problem, the Crowdsourced Event Parking Market Pricing Problem, is proposed wherein parking lot owners and others who are willing to rent out privately owned spaces compete to attract drivers who are looking for available parking spaces. Each parking location owner's problem is modeled as a bi-level program, where the upper-level parking garages, individuals and consolidators (the players) compete for customers, setting their prices to maximize revenue given the response of the lower-level followers. The lower-level followers choose their parking locations based on the utilities they derive from the spaces, which is a function of the proximity of the spaces to their destinations, parking fees and crowdedness. Prices are set as a function of the time of reservation. Reservation-time based pricing enables price differentiation for late comers either in the form of lower prices to attract last-minute customers when excess spaces are anticipated or higher-pricing if few spaces are expected to remain empty. A multi-period, stochastic Equilibrium Problem with Equilibrium Constraints (EPEC) formulation of the Crowdsourced Event Parking Market Pricing Problem is presented, and a diagonalization method with embedded gradient ascent approach for solution of individual player Mathematical Programs with Equilibrium Constraints (MPECs) is proposed for its solution. Both supply- and demand-side uncertainties are explicitly modeled. Solutions provide competitive parking prices set by reservation time for each parking facility, whether the facility involves a large parking garage or single parking space owner. Numerical experiments were conducted to illustrate the proposed concepts and assess the potential impact of crowdsourced parking spaces on the parking market. The results show that social welfare increases by more than 5% when crowdsourced parking locations account for 7% of the parking market. Results from additional numerical experiments show that ignoring stochasticity results in revenue loss for all parking owners. The developed techniques aim to facilitate existing and new parking facility owners to participate in crowdsourced event-parking markets. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153528935&site=ehost-live
473,Optimal transportation and shoreline infrastructure investment planning under a stochastic climate future.,Elise Miller-Hooks,Transportation Research: Part B,01912615,,Jun2017,100,,156,19.0,123043555,10.1016/j.trb.2016.12.023,Elsevier B.V.,Article,INFRASTRUCTURE (Economics); INVESTMENT policy; STOCHASTIC processes; INTEGER programming; GENETIC algorithms,Climate change; Multi-stage stochastic programming; Multi-temporal and multi-scale; Noisy genetic algorithms; Resilience; Sea level rise,"This paper studies the problem of optimal long-term transportation investment planning to protect from and mitigate impacts of climate change on roadway performance. The problem of choosing the extent, specific system components, and timing of these investments over a long time horizon (e.g., 40–60 years) is modeled as a multi-stage, stochastic, bi-level, mixed-integer program wherein cost-effective investment decisions are taken in the upper level. The effects of possible episodic precipitation events on experienced travel delays are estimated from solution of a lower-level, traffic equilibrium problem. The episodic events and longer-term sea level changes exist on different time scales, making their integration a crucial element in model development. The optimal investment strategy is obtained at a Stackelberg equilibrium that is reached upon solution to the bilevel program. A recursive noisy genetic algorithm (rNGA), designed to address large-scale applications, is proposed for this purpose. The rNGA seeks the optimal combination of investment decisions to take now given only probabilistic information on the predicted sea level rise trend for a long planning horizon and associated likely extreme climatic events (in terms of their frequencies and intensities) that might arise over that planning period. The proposed solution method enables the evaluation of decisions concerning where, when and to what level to make infrastructure investments. The proposed rNGA has broad applicability to more general multi-stage, stochastic, bilevel, nonconvex, mixed integer programs that arise in many applications. The proposed solution methodology is demonstrated on an example representing a portion of the Washington, D.C. Greater Metropolitan area adjacent to the Potomac River. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part B is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=123043555&site=ehost-live
474,Quantifying the resilience of an urban traffic-electric power coupled system.,Elise Miller-Hooks,Reliability Engineering & System Safety,09518320,,Jul2017,163,,79,16.0,121938725,10.1016/j.ress.2017.01.026,Elsevier B.V.,Article,CITY traffic; TRANSPORTATION; ELECTRIC power; STOCHASTIC programming; RISK assessment; All Other Support Activities for Transportation; Other support activities for transportation,Critical infrastructure interdependencies; Failure scenarios; Risk management; Traffic-electric coupled system; Transportation resilience quantification; Uncertainty analysis,"Transportation system resilience has been the subject of several recent studies. To assess the resilience of a transportation network, however, it is essential to model its interactions with and reliance on other lifelines. Prior works might consider these interactions implicitly, perhaps in the form of hazard impact scenarios wherein services from a second lifeline (e.g. power) are precluded due to a hazard event. In this paper, a bi-level, mixed-integer, stochastic program is presented for quantifying the resilience of a coupled traffic-power network under a host of potential natural or anthropogenic hazard-impact scenarios. A two-layer network representation is employed that includes details of both systems. Interdependencies between the urban traffic and electric power distribution systems are captured through linking variables and logical constraints. The modeling approach was applied on a case study developed on a portion of the signalized traffic-power distribution system in southern Minneapolis. The results of the case study show the importance of explicitly considering interdependencies between critical infrastructures in transportation resilience estimation. The results also provide insights on lifeline performance from an alternate power perspective. [ABSTRACT FROM AUTHOR] Copyright of Reliability Engineering & System Safety is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121938725&site=ehost-live
475,Transit system resilience: Quantifying the impacts of disruptions on diverse populations.,Elise Miller-Hooks,Reliability Engineering & System Safety,09518320,,Nov2019,191,,N.PAG,1.0,138833730,10.1016/j.ress.2019.106561,Elsevier B.V.,Article,FAULT trees (Reliability engineering); SOCIOTECHNICAL systems; SOCIAL systems; PUBLIC transit; POPULATION; TELECOMMUNICATION systems; LOCAL transit access; Other Heavy and Civil Engineering Construction; Urban transit systems; Mixed Mode Transit Systems; Bus and Other Motor Vehicle Transit Systems; Other Urban Transit Systems; Satellite Telecommunications,BDDs; Coupled systems; Fault tree; Interdependent lifelines; Reliability; Resilience; Transit,"• Conceptualizes transit network resilience in terms of a socio-technical system. • Explicitly considers diverse populations of passengers in resilience measurement. • Develops efficient data structure for evaluating system performance. • Makes explicit mechanisms of interdependency arising from social systems. • Assesses component criticality and revisits methods of comparison. The resilience of a community to disruptions in a public transportation system depends not only on the technical system's ability to maintain service levels, but also on the ability of individuals to cope with and adapt to disruptions. This paper proposes a reliability-based methodology for conceptualizing and evaluating resilience of a socio-technical system in the context of diverse populations of passengers who experience the system differently, but share its resources. Considering a transit network as a socio-technical system induces coupling with other technical systems, such as communication networks and other transportation modes, which support user adaptability in disruption. Extending concepts from fault trees and binary decision diagrams (BDDs), a multi-valued dependency graph framework is developed and used to quantify the resilience of the system. Various notions of resilience are explored. Techniques for identifying which mechanisms, whether technically or socially driven, are critical to the resilience of each population are discussed. Sensitivity to various factors through the study of improvement potential provides practical insight into policies and technical improvements that can increase system or user resilience. [ABSTRACT FROM AUTHOR] Copyright of Reliability Engineering & System Safety is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138833730&site=ehost-live
482,"Methanol, ethanol and hydrogen sensing using metal oxide and metal (TiO2–Pt) composite nanoclusters on GaN nanowires: a new route towards tailoring the selectivity of nanowire/nanocluster chemical sensors.",Rao V. Mulpuri,Nanotechnology,09574484,,5/4/2012,23,17,1,1.0,98022211,10.1088/0957-4484/23/17/175501,IOP Publishing,Article,CHEMICAL detectors; SEMICONDUCTOR nanowires; TITANIUM dioxide; GALLIUM nitride; SPUTTERING (Physics); BENZENE compounds; ULTRAVIOLET radiation; HEAT of adsorption; Other Basic Inorganic Chemical Manufacturing; All other basic inorganic chemical manufacturing,,"We demonstrate a new method for tailoring the selectivity of chemical sensors using semiconductor nanowires (NWs) decorated with metal and metal oxide multicomponent nanoclusters (NCs). Here we present the change of selectivity of titanium dioxide (TiO2) nanocluster-coated gallium nitride (GaN) nanowire sensor devices on the addition of platinum (Pt) nanoclusters. The hybrid sensor devices were developed by fabricating two-terminal devices using individual GaN NWs followed by the deposition of TiO2 and/or Pt nanoclusters (NCs) using the sputtering technique. This paper present the sensing characteristics of GaN/(TiO2–Pt) nanowire–nanocluster (NWNC) hybrids and GaN/(Pt) NWNC hybrids, and compare their selectivity with that of the previously reported GaN/TiO2 sensors. The GaN/TiO2 NWNC hybrids showed remarkable selectivity to benzene and related aromatic compounds, with no measurable response for other analytes. Addition of Pt NCs to GaN/TiO2 sensors dramatically altered their sensing behavior, making them sensitive only to methanol, ethanol and hydrogen, but not to any other chemicals we tested. The GaN/(TiO2–Pt) hybrids were able to detect ethanol and methanol concentrations as low as 100 nmol mol−1 (ppb) in air in approximately 100 s, and hydrogen concentrations from 1 µmol mol−1 (ppm) to 1% in nitrogen in less than 60 s. However, GaN/Pt NWNC hybrids showed limited sensitivity only towards hydrogen and not towards any alcohols. All these hybrid sensors worked at room temperature and are photomodulated, i.e. they responded to analytes only in the presence of ultraviolet (UV) light. We propose a qualitative explanation based on the heat of adsorption, ionization energy and solvent polarity to explain the observed selectivity of the different hybrids. These results are significant from the standpoint of applications requiring room-temperature hydrogen sensing and sensitive alcohol monitoring. These results demonstrate the tremendous potential for tailoring the selectivity of the hybrid nanosensors for a multitude of environmental and industrial sensing applications. [ABSTRACT FROM AUTHOR] Copyright of Nanotechnology is the property of IOP Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98022211&site=ehost-live
483,A class of distributed event-triggered average consensus algorithms for multi-agent systems.,Cameron Nowzari,International Journal of Control,00207179,,Feb 2022,95,2,502,14.0,154955760,10.1080/00207179.2020.1799244,Taylor & Francis Ltd,Article,DISTRIBUTED algorithms; MULTIAGENT systems; WIRELESS sensor networks; LYAPUNOV functions; ALGORITHMS; CONSENSUS (Social sciences),clock synchronisation; distributed coordination; Event-triggered control; multi-agent consensus; varying performance needs,"This paper proposes a class of distributed event-triggered algorithms that solve the average consensus problem in multi-agent systems. By designing events such that a specifically chosen Lyapunov function is monotonically decreasing, event-triggered algorithms succeed in reducing communications among agents while still ensuring that the entire system converges to the desired state. However, depending on the chosen Lyapunov function the transient behaviours can be very different. Moreover, performance requirements also vary from application to application. Consequently, we are instead interested in considering a class of Lyapunov functions such that each Lyapunov function produces a different event-triggered coordination algorithm to solve the multi-agent average consensus problem. The proposed class of algorithms all guarantee exponential convergence of the resulting system and exclusion of Zeno behaviours. This allows us to easily implement different algorithms that all guarantee correctness to meet varying performance needs. We show that our findings can be applied to the practical clock synchronisation problem in wireless sensor networks (WSNs) and further corroborate their effectiveness with simulation results. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Control is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154955760&site=ehost-live
484,Event-triggered communication and control of networked systems for multi-agent consensus.,Cameron Nowzari,Automatica,00051098,,Jul2019,105,,1,27.0,137030433,10.1016/j.automatica.2019.03.009,Elsevier B.V.,Article,MULTIAGENT systems; CONSENSUS (Social sciences); TOPOLOGY,Distributed coordination; Event-triggered control; Multi-agent consensus; Networked systems,"This article provides an introduction to event-triggered coordination for multi-agent average consensus. We provide a comprehensive account of the motivations behind the use of event-triggered strategies for consensus, the methods for algorithm synthesis, the technical challenges involved in establishing desirable properties of the resulting implementations, and their applications in distributed control. We pay special attention to the assumptions on the capabilities of the network agents and the resulting features of the algorithm execution, including the interconnection topology, the evaluation of triggers, and the role of imperfect information. The issues raised in our discussion transcend the specific consensus problem and are indeed characteristic of cooperative algorithms for networked systems that solve other coordination tasks. As our discussion progresses, we make these connections clear, highlighting general challenges and tools to address them widespread in the event-triggered control of networked systems. [ABSTRACT FROM AUTHOR] Copyright of Automatica is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137030433&site=ehost-live
485,Robust Economic Model Predictive Control of Continuous-Time Epidemic Processes.,Cameron Nowzari,IEEE Transactions on Automatic Control,00189286,,Mar2020,65,3,1116,16.0,143314014,10.1109/TAC.2019.2919136,IEEE,Article,ECONOMIC models; PREDICTION models; STOCHASTIC systems; STOCHASTIC processes; STOCHASTIC convergence; PREDICTIVE control systems,Convergence; Diseases; Epidemic processes; model predictive control; Modeling; networked systems; Nickel; Predictive control; Stochastic processes,"In this paper, we develop a robust economic model predictive controller for the containment of stochastic susceptible-exposed-infected-vigilant (SEIV) epidemic processes, which drives the process to extinction quickly, while minimizing the rate at which control resources are used. The study we present here is significant in that it addresses the problem of efficiently controlling general stochastic epidemic systems without relying on mean-field approximation, which is an important issue in the theory of stochastic epidemic processes. This enables us to provide rigorous convergence guarantees on the stochastic epidemic model itself, improving over the mean-field type convergence results of most prior work. There are two primary technical difficulties addressed in treating this problem: 1) constructing a means of tractably approximating the evolution of the process so that the designed approximation is robust to the modeling error introduced by the applied moment closure and 2) guaranteeing that the designed controller causes the closed-loop system to drive the SEIV process to extinction quickly. As an application, we use the developed framework for optimizing the use of quarantines in containing an SEIV epidemic outbreak. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143314014&site=ehost-live
486,The impact of catastrophic collisions and collision avoidance on a swarming behavior.,Cameron Nowzari,Robotics & Autonomous Systems,09218890,,Jun2021,140,,N.PAG,1.0,149887105,10.1016/j.robot.2021.103754,Elsevier B.V.,Article,OBSTACLE avoidance (Robotics); IMPACT (Mechanics); GOAL (Psychology),,"Swarms of autonomous agents are useful in many applications due to their ability to accomplish tasks in a decentralized manner, making them more robust to failures. Due to the difficulty in running experiments with large numbers of hardware agents, researchers often make simplifying assumptions and remove constraints that might be present in a real swarm deployment. While simplifying away some constraints is tolerable, we feel that two in particular have been overlooked: one, that agents in a swarm take up physical space, and two, that agents might be damaged in collisions. Many existing works assume agents have negligible size or pass through each other with no added penalty. It seems possible to ignore these constraints using collision avoidance, but we show using an illustrative example that this is easier said than done. In particular, we show that collision avoidance can interfere with the intended swarming behavior and significant parameter tuning is necessary to ensure the behavior emerges as best as possible while collisions are avoided. We compare four different collision avoidance algorithms, two of which we consider to be the best decentralized collision avoidance algorithms available. Despite putting significant effort into tuning each algorithm to perform at its best, we believe our results show that further research is necessary to develop swarming behaviors that can achieve their goal while avoiding collisions with agents of non-negligible volume. • A scalar metric for assessing the quality of a rotating ring formation. • Simulation of destructive collisions with 'respawning' to mimic real platforms. • Investigation of how 4 collision avoidance algorithms interfere with a swarm behavior. • Extensive tuning of each collision avoidance algorithm for fair comparisons. • Heat maps of the parameter space to illustrate the overall tuning difficulty. [ABSTRACT FROM AUTHOR] Copyright of Robotics & Autonomous Systems is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149887105&site=ehost-live
487,"An experimental comparison of edge, edge-pair, and prime path criteria.",Jeff Offutt,Science of Computer Programming,01676423,,Jan2018,152,,99,17.0,126478868,10.1016/j.scico.2017.10.003,Elsevier B.V.,Article,"MUTATION testing of computer software; ASSIMILATION theory (Cognitive learning theory); GRAPH theory; QUANTITATIVE research; COMPUTER software; Software publishers (except video game publishers); Computer and software stores; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers",Edge pair coverage; Mutation testing; Prime path coverage; Structural testing,"Background Many criteria have been proposed to generate test inputs. Criteria are usually compared in terms of subsumption: if a criterion C1 subsumes C2, it is guaranteed that every test set that satisfies C1 will also satisfy C2. An implication of this notion of subsumption is that C1-adequate tests tend to find more faults than C2-adequate tests, but C1-adequate tests tend to be larger. Thus, while useful, the idea of subsumption does not elaborate on some practical properties of expensive criteria as, for instance, how many more faults a C1-adequate test set will find? More generally, what is the return on investment for choosing more expensive criteria? Method To provide a more accurate idea of the fault finding ability and cost of several criteria, we set out to compare three structural graph coverage criteria: edge coverage (EC), edge-pair coverage (EPC), and prime path coverage (PPC). PPC and EPC subsume EC. To compare these criteria we examined 189 functions from 39 C programs, used mutants as a proxy for faults, and performed a statistical analysis of the results. Result The three criteria are very similar in terms of effectiveness when all mutants are taken into account: PPC killed 98% of the mutants, EPC 97%, and EC 94%. However, the difference between the criteria is emphasized with minimal mutant sets: PPC killed 75% of the mutants, EPC killed 67%, and EC killed only 57%. As for the cost of these criteria, we found that there is not much difference in terms of the number of TRs. We expected PPC to have the most TRs, so we were surprised to find that, on average, the number of TRs for EPC was highest. Conclusion PPC can detect more faults, specially in programs that have complicated control flows, but at higher cost. Thus, a practical tester can make an informed cost versus benefit decision. A better understanding of which structures in the programs contribute to the expense might help to choose when to use PPC. [ABSTRACT FROM AUTHOR] Copyright of Science of Computer Programming is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=126478868&site=ehost-live
488,Categorization of Common. Coupling and Its Application to the Maintainability of the Linux Kernel.,Jeff Offutt,IEEE Transactions on Software Engineering,00985589,,Oct2004,30,10,694,13.0,14654398,,IEEE,Article,OPEN source software; SYSTEMS software; COMPUTER operating systems; DATA transmission systems; DIGITAL communications; Software publishers (except video game publishers); Software Publishers,common coupling; definition-use analysis; dependencies; kernel-based software; Linux; Modularity; open-source software,"Data coupling between modules, especially common coupling, has long been considered a source of concern in software design, but the issue is somewhat more complicated for products that are comprised of kernel modules together with optional nonkernel modules. This paper presents a refined categorization of common coupling based on definitions and uses between kernel and nonkernel modules and applies the categorization to a case study. Common coupling is usually avoided when possible because of the potential for introducing risky dependencies among software modules. The relative risk of these dependencies is strongly related to the specific definition-use relationships. In a previous paper, we presented results from a longitudinal analysis of multiple versions of the open-source operating system Linux. This paper applies the new common coupling categorization to version 2.4.20 of Linux, counting the number of instances of common coupling between each of the 26 kernel modules and all the other nonkernel modules. We also categorize each coupling in terms of the definition-use relationships. Results show that the Linux kernel contains a large number of common couplings of all types, raising a concern about the long-term maintainability of Linux. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Software Engineering is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=14654398&site=ehost-live
489,"Evaluating the role of professional development on elementary teachers’ knowledge, comfort, and beliefs related to teaching computer science to students with high-incidence disabilities.",Jeff Offutt,Journal of Research on Technology in Education,15391523,,Jun2022,,,1,17.0,157864318,10.1080/15391523.2022.2089408,Taylor & Francis Ltd,Article,,Computer science education; professional development; special education; teach perceptions; teacher beliefs; universal design for learning,"Abstract This article reports results from the implementation of a model of professional development (PD) to help K-5 teachers develop the knowledge and skills to teach Computer Science (CS) in classrooms of diverse students, including students with high-incidence disabilities. This article describes our Inclusive CS model of PD, how we made the PD model available to teachers during a pandemic and presents quantitative and qualitative results about the impact of the PD on teachers’ knowledge, comfort, and beliefs related to teaching computer science to students. Results indicate that the teachers’ knowledge, comfort, beliefs and perceptions about teaching CS to students with disabilities significantly improved. Teachers’ knowledge and understanding of Universal Design for Learning for supporting students in learning about CS also improved. [ABSTRACT FROM AUTHOR] Copyright of Journal of Research on Technology in Education is the property of Routledge and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157864318&site=ehost-live
490,Modeling presentation layers of web applications for testing.,Jeff Offutt,Software & Systems Modeling,16191366,,Apr2010,9,2,257,24.0,48999313,10.1007/s10270-009-0125-4,Springer Nature,Article,"WEB-based user interfaces; COMPUTER software development; HTML (Document markup language); COMPUTER software; SOFTWARE engineering; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers; Software publishers (except video game publishers); Computer and software stores; Custom Computer Programming Services; Computer systems design and related services (except video game design and development)",Test criteria; Web applications; Web modeling,"Web software applications have become complex, sophisticated programs that are based on novel computing technologies. Their most essential characteristic is that they represent a different kind of software deployment—most of the software is never delivered to customers’ computers, but remains on servers, allowing customers to run the software across the web. Although powerful, this deployment model brings new challenges to developers and testers. Checking static HTML links is no longer sufficient; web applications must be evaluated as complex software products. This paper focuses on three aspects of web applications that are unique to this type of deployment: (1) an extremely loose form of coupling that features distributed integration, (2) the ability that users have to directly change the potential flow of execution, and (3) the dynamic creation of HTML forms. Taken together, these aspects allow the potential control flow to vary with each execution, thus the possible control flows cannot be determined statically, prohibiting several standard analysis techniques that are fundamental to many software engineering activities. This paper presents a new way to model web applications, based on software couplings that are new to web applications, dynamic flow of control, distributed integration, and partial dynamic web application development. This model is based on the notion of atomic sections, which allow analysis tools to build the analog of a control flow graph for web applications. The atomic section model has numerous applications in web applications; this paper applies the model to the problem of testing web applications. [ABSTRACT FROM AUTHOR] Copyright of Software & Systems Modeling is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=48999313&site=ehost-live
491,Mutation at the multi-class and system levels,Jeff Offutt,Science of Computer Programming,01676423,,Apr2013,78,4,364,24.0,85250477,10.1016/j.scico.2012.02.005,Elsevier B.V.,Article,"MUTATION testing of computer software; PROGRAMMING languages; COMPUTER network protocols; WEB services; SYSTEM integration; COMPUTER software; Software publishers (except video game publishers); Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers; Software Publishers; Data Processing, Hosting, and Related Services; Computer Systems Design Services",Flexible weak mutation; Mutation; Mutation process; System testing; Testing,"Abstract: Mutation analysis has been applied to many testing problems, including numerous programming languages, specifications, network protocols, web services, and security policies. Program mutation, where mutation analysis is applied to programs, has been applied to the unit level (functions and methods), integration of pairs of functions, and individual classes. However, program mutation has not been applied to the problem of integration testing of multiple classes or entire software programs; thus, there is no system level mutation. This paper introduces a project on the problem of integration testing of multiple classes (multi-class) and system level mutation testing. The technical differences between using mutation to test single classes and multiple classes are explored, and new system level mutation operators are defined. A new execution style for detecting killed mutants, flexible weak mutation, is introduced. A mutation tool, Bacterio, still under construction, is also described. [Copyright &y& Elsevier] Copyright of Science of Computer Programming is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=85250477&site=ehost-live
492,Putting the Engineering into Software Engineering Education.,Jeff Offutt,IEEE Software,07407459,,Jan2013,30,1,96,1.0,84675195,10.1109/MS.2013.12,IEEE,Article,"SOFTWARE engineering education; COMPUTER science education; ENGINEERING education; TEACHING methods; EDUCATIONAL objectives; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Computer engineering education; Engineering education; scientific discipline; Software engineering; software engineering education; teaching paradigm,"Based on over 20 years of teaching and research experience, the author provides his assessment of software engineering education. He then builds on the analysis to provide recommendations on how we need to diverge from computer science to increase our impact, gain credibility, and ultimately ensure the success and recognition of our young discipline. A key behind the author's message is that we need to become a true engineering discipline. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Software is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=84675195&site=ehost-live
493,SE 2014: Curriculum Guidelines for Undergraduate Degree Programs in Software Engineering.,Jeff Offutt,Computer (00189162),00189162,,Nov2015,48,11,106,4.0,111647090,10.1109/MC.2015.345,IEEE,Article,GUIDELINES; CURRICULUM; UNDERGRADUATES; SOFTWARE engineering education; HIGHER education; ACADEMIC degrees,Computer science education; degree; Education courses; SE2014; software; Software engineering; standards,"Revised curriculum guidelines help university faculty create or update undergraduate software engineering programs. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=111647090&site=ehost-live
494,Testing concurrent user behavior of synchronous web applications with Petri nets.,Jeff Offutt,Software & Systems Modeling,16191366,,Apr2019,18,2,913,24.0,135694871,10.1007/s10270-018-0655-8,Springer Nature,Article,PETRI nets; WEB-based user interfaces; SOFTWARE development tools; DESIGN techniques; Custom Computer Programming Services; Computer systems design and related services (except video game design and development),Model-based testing; Petri nets; Test criteria; Web applications,"Web applications are now used in every aspect of our lives to manage work, provide products and services, read email, and provide entertainment. The software technologies used to build web applications provide features that help designers provide flexible functionality, but that are challenging to model and test. In particular, the network-based request-response model of programming means that web applications are inherently ""stateless"" and implicitly concurrent. They are stateless because a new network connection is made for each request (for example, when a user clicks a submit button). Thus, the server does not, by default, recognize multiple requests from the same user. Web applications are also concurrent because multiple users can use the same web application at the same time, creating contention for the same resources. Unfortunately, most web application testing does not adequately evaluate these aspects of web applications, leaving many software faults in deployed web applications. Part of this problem is because most traditional software modeling tools (such as UML) do not have built-in support for the stateless and concurrent aspects of web applications. This research project uses a novel model that is based on Petri nets to describe certain aspects of the behavior of web applications. This paper makes several contributions. We present a novel technique to design tests from this model that explicitly tests concurrency in web applications. We present novel coverage criteria that are defined on the Petri net model. We present results from an empirical study of 18 web applications with 343 components and 30,186 lines of code, followed by a case study on a large industrial web application. The tests found significantly more faults than traditional requirements-based tests, with fewer tests. [ABSTRACT FROM AUTHOR] Copyright of Software & Systems Modeling is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135694871&site=ehost-live
495,Testing Web applications by modeling with FSMs.,Jeff Offutt,Software & Systems Modeling,16191366,,Jul2005,4,3,326,20.0,17551496,10.1007/s10270-004-0077-7,Springer Nature,Article,"WEB services; APPLICATION software; TESTING; SOFTWARE engineering; COMPUTER systems; COMPUTER simulation; Software Publishers; Software publishers (except video game publishers); Custom Computer Programming Services; Computer systems design and related services (except video game design and development); Computer Systems Design Services; Data Processing, Hosting, and Related Services",Finite state machines; System testing; Testing of Web applications,"Researchers and practitioners are still trying to find effective ways to model and test Web applications. This paper proposes a system-level testing technique that combines test generation based on finite state machines with constraints. We use a hierarchical approach to model potentially large Web applications. The approach builds hierarchies of Finite State Machines (FSMs) that model subsystems of the Web applications, and then generates test requirements as subsequences of states in the FSMs. These subsequences are then combined and refined to form complete executable tests. The constraints are used to select a reduced set of inputs with the goal of reducing the state space explosion otherwise inherent in using FSMs. The paper illustrates the technique with a running example of a Web-based course student information system and introduces a prototype implementation to support the technique. [ABSTRACT FROM AUTHOR] Copyright of Software & Systems Modeling is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=17551496&site=ehost-live
496,21 Years of Distributed Denial-of Service: Current State of Affairs.,Eric Osterweil,Computer (00189162),00189162,,Jul2020,53,7,88,5.0,144376186,10.1109/MC.2020.2983711,IEEE,Article,,Aggregates; Computer crime; Denial-of-service attack; Internet; Protocols; Servers,"The Internet's features and capacity have evolved, but is the nature of its security noticeably better? We examine the fundamental nature of distributed denial-of-service (DDoS) and the state of the union of our defenses in today's DDoS wars. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144376186&site=ehost-live
496,21 Years of Distributed Denial-of Service: Current State of Affairs.,Angelos Stavrou,Computer (00189162),00189162,,Jul2020,53,7,88,5.0,144376186,10.1109/MC.2020.2983711,IEEE,Article,,Aggregates; Computer crime; Denial-of-service attack; Internet; Protocols; Servers,"The Internet's features and capacity have evolved, but is the nature of its security noticeably better? We examine the fundamental nature of distributed denial-of-service (DDoS) and the state of the union of our defenses in today's DDoS wars. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144376186&site=ehost-live
497,21 Years of Distributed Denial-of-Service: A Call to Action.,Eric Osterweil,Computer (00189162),00189162,,Aug2020,53,8,94,6.0,144933842,10.1109/MC.2020.2993330,IEEE,Article,"COMPUTER crimes; WEB services; INTERNET; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals; Data Processing, Hosting, and Related Services",Bandwidth; Computer crime; Denial-of-service attack; Protocols; Web and internet services,"We are falling behind in the war against distributed denial-of-service attacks. Unless we act now, the future of the Internet could be at stake. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144933842&site=ehost-live
497,21 Years of Distributed Denial-of-Service: A Call to Action.,Angelos Stavrou,Computer (00189162),00189162,,Aug2020,53,8,94,6.0,144933842,10.1109/MC.2020.2993330,IEEE,Article,"COMPUTER crimes; WEB services; INTERNET; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals; Data Processing, Hosting, and Related Services",Bandwidth; Computer crime; Denial-of-service attack; Protocols; Web and internet services,"We are falling behind in the war against distributed denial-of-service attacks. Unless we act now, the future of the Internet could be at stake. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144933842&site=ehost-live
498,Modeling and mitigating human annotation errors to design efficient stream processing systems with human-in-the-loop machine learning.,Hemant Purohit,International Journal of Human-Computer Studies,10715819,,Apr2022,160,,N.PAG,1.0,154896355,10.1016/j.ijhcs.2022.102772,Academic Press Inc.,Article,HUMAN error; MACHINE learning; WEB analytics; MEMORY; ACTIVE learning; SOCIAL media,Active learning; Annotation schedule; Human-AI collaboration; Human-centered computing; Memory decay,"• Study of human annotation task in hybrid stream processing systems. • Presenting a generic human error framework of serial ordering-based mistakes and slips. • Verifying of the proposed human error framework through extensive experiments. • Presenting a novel method for human error-mitigation in an active learning paradigm. • Validating the novel method through simulation-based experiments. High-quality human annotations are necessary for creating effective machine learning-driven stream processing systems. We study hybrid stream processing systems based on a Human-In-The-Loop Machine Learning (HITL-ML) paradigm, in which one or many human annotators and an automatic classifier (trained at least partially by the human annotators) label an incoming stream of instances. This is typical of many near-real-time social media analytics and web applications, including annotating social media posts during emergencies by digital volunteer groups. From a practical perspective, low-quality human annotations result in wrong labels for retraining automated classifiers and indirectly contribute to the creation of inaccurate classifiers. Considering human annotation as a psychological process allows us to address these limitations. We show that human annotation quality is dependent on the ordering of instances shown to annotators and can be improved by local changes in the instance sequence/order provided to the annotators, yielding a more accurate annotation of the stream. We adapt a theoretically-motivated human error framework of mistakes and slips for the human annotation task to study the effect of ordering instances (i.e., an ""annotation schedule""). Further, we propose an error-avoidance approach to the active learning paradigm for stream processing applications robust to these likely human errors (in the form of slips) when deciding a human annotation schedule. We support the human error framework using crowdsourcing experiments and evaluate the proposed algorithm against standard baselines for active learning via extensive experimentation on classification tasks of filtering relevant social media posts during natural disasters. According to these experiments, considering the order in which data instances are presented to a human annotator leads to increased accuracy for machine learning and awareness of the potential properties of human memory for the class concept, which may affect annotation for automated classifiers. Our results allow the design of hybrid stream processing systems based on the HITL-ML paradigm, which requires the same amount of human annotations, but that has fewer human annotation errors. Automated systems that help reduce human annotation errors could benefit several web stream processing applications, including social media analytics and news filtering. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Human-Computer Studies is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154896355&site=ehost-live
499,"Sex, Lies, and Stereotypes: Gendered Implications of Fake News for Women in Politics.",Hemant Purohit,Public Integrity,10999922,,Sep/Oct2019,21,5,491,12.0,138322711,10.1080/10999922.2019.1626695,Taylor & Francis Ltd,Article,"UNITED States presidential election, 2016; WOMEN leaders; CLINTON, Hillary Rodham, 1947-; TRUMP, Donald, 1946-",fake news; gender bias; women in politics,"This analysis examines the literature on gendered media coverage of women candidates for higher office, and considers how biases in the treatment of candidates based on gender may be evident in or exacerbated by the promulgation of fake news. Using the 2016 Presidential election cycle in the United States as a case study, two fake news stories are investigated, which, like most fake news stories at the time, exhibited coverage in favor of the candidacy of Donald Trump and demonized or denigrated his opponent, Hillary Clinton. Findings suggest that the Pizzagate and Hillary Health Scare stories evince gendered narratives supporting stereotypes of women as unfit for leadership positions, and either villainize or trivialize women, depending on their perceived degree of power. Using a dataset of news articles and tweets from the months surrounding the 2016 election, evidence is offered of more negative coverage of the female versus male contender, in keeping with the findings of the literature, though the presence of potentially confounding factors, including personality and party, is acknowledged. [ABSTRACT FROM AUTHOR] Copyright of Public Integrity is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138322711&site=ehost-live
500,Extremes of locally stationary Gaussian and chi fields on manifolds.,Wanli Qiao,Stochastic Processes & Their Applications,03044149,,Mar2021,133,,166,27.0,148365508,10.1016/j.spa.2020.11.006,Elsevier B.V.,Article,VORONOI polygons; STATIONARY processes; PROBABILITY theory,Chi-fields; Excursion probabilities; Gaussian fields; Local stationarity; Positive reach; Voronoi diagrams,"Depending on a parameter h ∈ (0 , 1 ] , let { X h (t) , t ∈ M h } be a class of centered Gaussian fields indexed by compact manifolds M h with positive reach. For locally stationary Gaussian fields X h , we study the asymptotic excursion probabilities of X h on M h. Two cases are considered: (i) h is fixed and (ii) h → 0. These results are also extended to obtain the limit behaviors of the extremes of locally stationary χ -fields on manifolds. [ABSTRACT FROM AUTHOR] Copyright of Stochastic Processes & Their Applications is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148365508&site=ehost-live
501,From mutations to mechanisms and dysfunction via computation and mining of protein energy landscapes.,Wanli Qiao,BMC Genomics,14712164,,9/24/2018 Supplement 7,19,,1,13.0,131918149,10.1186/s12864-018-5024-z,BioMed Central,Article,PROTEINS; MOLECULES; EQUILIBRIUM; DYNAMICS; GENETIC mutation,Basins; Energy landscape; Equilibrium dynamics; Landscape mining; Landscape reconstruction; Pathogenic mutations; Protein dysfunction; Saddles,"Background: The protein energy landscape underscores the inherent nature of proteins as dynamic molecules interconverting between structures with varying energies. Reconstructing a protein's energy landscape holds the key to characterizing a protein's equilibrium conformational dynamics and its relationship to function. Many pathogenic mutations in protein sequences alter the equilibrium dynamics that regulates molecular interactions and thus protein function. In principle, reconstructing energy landscapes of a protein's healthy and diseased variants is a central step to understanding how mutations impact dynamics, biological mechanisms, and function. Results: Recent computational advances are yielding detailed, sample-based representations of protein energy landscapes. In this paper, we propose and describe two novel methods that leverage computed, sample-based representations of landscapes to reconstruct them and extract from them informative local structures that reveal the underlying organization of an energy landscape. Such structures constitute landscape features that, as we demonstrate here, can be utilized to detect alterations of landscapes upon mutation. Conclusions: The proposed methods detect altered protein energy landscape features in response to sequence mutations. By doing so, the methods allow formulating hypotheses on the impact of mutations on specific biological activities of a protein. This work demonstrates that the availability of energy landscapes of healthy and diseased variants of a protein opens up new avenues to harness the quantitative information embedded in landscapes to summarize mechanisms via which mutations alter protein dynamics to percolate to dysfunction. [ABSTRACT FROM AUTHOR] Copyright of BMC Genomics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131918149&site=ehost-live
501,From mutations to mechanisms and dysfunction via computation and mining of protein energy landscapes.,Amarda Shehu,BMC Genomics,14712164,,9/24/2018 Supplement 7,19,,1,13.0,131918149,10.1186/s12864-018-5024-z,BioMed Central,Article,PROTEINS; MOLECULES; EQUILIBRIUM; DYNAMICS; GENETIC mutation,Basins; Energy landscape; Equilibrium dynamics; Landscape mining; Landscape reconstruction; Pathogenic mutations; Protein dysfunction; Saddles,"Background: The protein energy landscape underscores the inherent nature of proteins as dynamic molecules interconverting between structures with varying energies. Reconstructing a protein's energy landscape holds the key to characterizing a protein's equilibrium conformational dynamics and its relationship to function. Many pathogenic mutations in protein sequences alter the equilibrium dynamics that regulates molecular interactions and thus protein function. In principle, reconstructing energy landscapes of a protein's healthy and diseased variants is a central step to understanding how mutations impact dynamics, biological mechanisms, and function. Results: Recent computational advances are yielding detailed, sample-based representations of protein energy landscapes. In this paper, we propose and describe two novel methods that leverage computed, sample-based representations of landscapes to reconstruct them and extract from them informative local structures that reveal the underlying organization of an energy landscape. Such structures constitute landscape features that, as we demonstrate here, can be utilized to detect alterations of landscapes upon mutation. Conclusions: The proposed methods detect altered protein energy landscape features in response to sequence mutations. By doing so, the methods allow formulating hypotheses on the impact of mutations on specific biological activities of a protein. This work demonstrates that the availability of energy landscapes of healthy and diseased variants of a protein opens up new avenues to harness the quantitative information embedded in landscapes to summarize mechanisms via which mutations alter protein dynamics to percolate to dysfunction. [ABSTRACT FROM AUTHOR] Copyright of BMC Genomics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131918149&site=ehost-live
502,"Protocol for a quasi-experimental, 950 county study examining implementation outcomes and mechanisms of Stepping Up, a national policy effort to improve mental health and substance use services for justice-involved individuals.",Niloofar Ramezani,Implementation Science,17485908,,3/29/2021,16,1,1,14.0,149527386,10.1186/s13012-021-01095-2,BioMed Central,journal article,"MENTAL health policy; SUBSTANCE abuse; COMMUNITY mental health services; GOVERNMENT policy; MENTAL health services; MENTAL health; CRIMINAL justice system; Residential Mental Health and Substance Abuse Facilities; Other Individual and Family Services; Outpatient Mental Health and Substance Abuse Centers; Other Justice, Public Order, and Safety Activities; Offices of Mental Health Practitioners (except Physicians); Psychiatric and Substance Abuse Hospitals; Administration of Public Health Programs",Community; Implementation; Jail; Mechanism; Mental health; Outcome; Substance use,"<bold>Background: </bold>The criminal justice system is the largest provider of mental health services in the USA. Many jurisdictions are interested in reducing the use of the justice system for mental health problems. The national Stepping Up Initiative helps agencies within counties work together more effectively to reduce the number of individuals with mental illness in jails and to improve access to mental health services in the community. This study will compare Stepping Up counties to matched comparison counties over time to (1) examine the effectiveness of Stepping Up and (2) test hypothesized implementation mechanisms to inform multi-agency implementation efforts more broadly.<bold>Methods: </bold>The study will survey 950 counties at baseline, 18 months, and 36 months in a quasi-experimental design comparing implementation mechanisms and outcomes between 475 Stepping Up counties and 475 matched comparison counties. Surveys will be sent to up to four respondents per county including administrators of jail, probation, community mental health services, and community substance use treatment services (3800 total respondents). We will examine whether Stepping Up counties show faster improvements in implementation outcomes (number of justice-involved clients receiving behavioral health services, number of behavioral health evidence-based practices and policies [EBPPs] available to justice-involved individuals, and resources for behavioral health EBPP for justice-involved individuals) than do matched comparison counties. We will also evaluate whether engagement of hypothesized mechanisms explains differences in implementation outcomes. Implementation target mechanisms include (1) use of and capacity for performance monitoring, (2) use and functioning of interagency teams, (3) common goals and mission across agencies, and (4) system integration (i.e., building an integrated system of care rather than adding one program or training). Finally, we will characterize implementation processes and critical incidents using survey responses and qualitative interviews.<bold>Discussion: </bold>There are few rigorous, prospective studies examining implementation mechanisms and their relationship with behavioral health implementation outcomes in justice and associated community behavioral health settings. There is also limited understanding of implementation mechanisms that occur across systems with multiple goals. This study will describe implementation outcomes of Stepping Up and will elucidate target mechanisms that are effective in multi-goal, multi-agency systems. [ABSTRACT FROM AUTHOR] Copyright of Implementation Science is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149527386&site=ehost-live
503,Protocol for the Mason: Health Starts Here prospective cohort study of young adult college students.,Niloofar Ramezani,BMC Public Health,14712458,,5/12/2021,21,1,1,15.0,150301669,10.1186/s12889-021-10969-5,BioMed Central,journal article,COVID-19 pandemic; MENTAL health of college students; SOCIOECONOMIC status; GENDER identity; PERIODIC health examinations,,"<bold>Background: </bold>Young adulthood is a period of increasing independence for the 40% of young adults enrolled in U.S. colleges. Previous research indicates differences in how students' health behaviors develop and vary by gender, race, ethnicity, and socioeconomic status. George Mason University is a state institution that enrolls a highly diverse student population, making it an ideal setting to launch a longitudinal cohort study using multiple research methods to evaluate the effects of health behaviors on physical and psychological functioning, especially during the COVID-19 pandemic.<bold>Results: </bold>Mason: Health Starts Here was developed as a longitudinal cohort study of successive waves of first year students that aims to improve understanding of the natural history and determinants of young adults' physical health, mental health, and their role in college completion. The study recruits first year students who are 18 to 24 years old and able to read and understand English. All incoming first year students are recruited through various methods to participate in a longitudinal cohort for 4 years. Data collection occurs in fall and spring semesters, with online surveys conducted in both semesters and in-person clinic visits conducted in the fall. Students receive physical examinations during clinic visits and provide biospecimens (blood and saliva).<bold>Conclusions: </bold>The study will produce new knowledge to help understand the development of health-related behaviors during young adulthood. A long-term goal of the cohort study is to support the design of effective, low-cost interventions to encourage young adults' consistent performance of healthful behaviors, improve their mental health, and improve academic performance. [ABSTRACT FROM AUTHOR] Copyright of BMC Public Health is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150301669&site=ehost-live
504,Substance use and firearm access among college freshmen.,Niloofar Ramezani,Journal of American College Health,07448481,,Apr2022,,,1,5.0,156826695,10.1080/07448481.2022.2068959,Taylor & Francis Ltd,Article,,Gun access; heavy episodic drinking; university,"Abstract Objective Participants Methods Results Conclusions Examine the proportion of students with rapid firearm access and associations with recent alcohol and marijuana use.Cross-sectional data from college freshmen (<italic>n</italic> = 183) in 2020 who participated in the <italic>Mason: Health Starts Here</italic> study.Using logistic regression, associations were examined between past 30-day substance use and access to firearms within 15-min.More than 10% of students could rapidly access a firearm, 53% of whom were current binge drinkers, compared to 13% of those who could not rapidly access firearms. Non-Hispanic White students (AOR = 4.1, 95%CI = 1.3,12.7) and past 30-day binge drinkers (AOR = 6.4, 95%CI = 2.1,19.7) had greater odds of having rapid firearm access. Age, sex, and past 30-day marijuana use were not associated with rapid access.A notable proportion of students had rapid firearm access, which was strongly associated with recent binge drinking. Campus prevention programs should consider how their alcohol and firearm policies could be enhanced to prevent violence/self-harm. [ABSTRACT FROM AUTHOR] Copyright of Journal of American College Health is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156826695&site=ehost-live
505,"The relationship between community public health, behavioral health service accessibility, and mass incarceration.",Niloofar Ramezani,BMC Health Services Research,14726963,,7/29/2022,22,1,1,11.0,158273191,10.1186/s12913-022-08306-6,BioMed Central,Article,,Access to care; Behavioral health; Community public health; Community service accessibility; Jail population; Mass incarceration,"<bold>Background: </bold>The relationship between healthcare service accessibility in the community and incarceration is an important, yet not widely understood, phenomenon. Community behavioral health and the criminal legal systems are treated separately, which creates a competing demand to confront mass incarceration and expand available services. As a result, the relationship between behavioral health services, demographics and community factors, and incarceration rate has not been well addressed. Understanding potential drivers of incarceration, including access to community-based services, is necessary to reduce entry into the legal system and decrease recidivism. This study identifies county-level demographic, socioeconomic, healthcare services availability/accessibility, and criminal legal characteristics that predict per capita jail population across the U.S. More than 10 million individuals pass through U.S. jails each year, increasing the urgency of addressing this challenge.<bold>Methods: </bold>The selection of variables for our model proceeded in stages. The study commenced by identifying potential descriptors and then using machine learning techniques to select non-collinear variables to predict county jail population per capita. Beta regression was then applied to nationally available data from all 3,141 U.S. counties to identify factors predicting county jail population size. Data sources include the Vera Institute's incarceration database, Robert Wood Johnson Foundation's County Health Rankings and Roadmaps, Uniform Crime Report, and the U.S. Census.<bold>Results: </bold>Fewer per capita psychiatrists (z-score = -2.16; p = .031), lower percent of drug treatment paid by Medicaid (-3.66; p < .001), higher per capita healthcare costs (5.71; p < .001), higher number of physically unhealthy days in a month (8.6; p < .001), lower high school graduation rate (-4.05; p < .001), smaller county size (-2.66, p = .008; -2.71, p = .007; medium and large versus small counties, respectively), and more police officers per capita (8.74; p < .001) were associated with higher per capita jail population. Controlling for other factors, violent crime rate did not predict incarceration rate.<bold>Conclusions: </bold>Counties with smaller populations, larger percentages of individuals that did not graduate high school, that have more health-related issues, and provide fewer community treatment services are more likely to have higher jail population per capita. Increasing access to services, including mental health providers, and improving the affordability of drug treatment and healthcare may help reduce incarceration rates. [ABSTRACT FROM AUTHOR] Copyright of BMC Health Services Research is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158273191&site=ehost-live
506,"What Matters More in Explaining Drug Court Graduation and Rearrest: Program Features, Individual Characteristics, or Some Combination.",Niloofar Ramezani,International Journal of Offender Therapy & Comparative Criminology,0306624X,,Apr2022,,,1,,156459114,10.1177/0306624x221086558,Sage Publications Inc.,Article,,drug courts; hierarchical linear modeling; program graduation; programming; rearrest; risk need assessments,"This study examines the program- and individual-level factors that impact the success of drug court clients in terms of: (1) graduation; and (2) not being arrested while participating in the court program. The data consist of 848 individuals in nine drug courts. This paper discusses how different individual- and program-level factors impact the success of drug court participants. The findings suggest that individual- and program-level factors are both important in predicting program graduation and arrest during drug court participation, while controlling for participant demographics. Clients’ education, drug/alcohol usage, program staffing, and clinical standards impact program graduation while criminal history, drug/alcohol usage, number of program hours offered, program staffing, and use of rewards and sanctions predict in-program arrest. Models combining both program- and individual-level factors performed better than either alone, leading to recommendations that agencies should emphasize improving program quality while targeting clients’ needs to achieve greater success. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Offender Therapy & Comparative Criminology is the property of Sage Publications Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156459114&site=ehost-live
507,Colonic microbiome is altered in alcoholism.,Huzefa Rangwala,American Journal of Physiology: Gastrointestinal & Liver Physiology,01931857,,May2012,65,5,G966,14.0,95871562,10.1152/ajpgi.00380.2011,American Physiological Society,Article,,alcohol; alcoholic liver disease; colon; colonic microbiota; length heterogeneity polymerase chain reaction; pyrosequencing,"Several studies indicate the importance of colonic microbiota in metabolic and inflammatory disorders and importance of diet on microbiota composition. The effects of alcohol, one of the prominent components of diet, on colonic bacterial composition is largely unknown. Mounting evidence suggests that gut-derived bacterial endotoxins are cofactors for alcohol-induced tissue injury and organ failure like alcoholic liver disease (ALD) that only occur in a subset of alcoholics. We hypothesized that chronic alcohol consumption results in alterations of the gut microbiome in a subgroup of alcoholics, and this may be responsible for the observed inflammatory state and endotoxemia in alcoholics. Thus we interrogated the mucosa-associated colonic microbiome in 48 alcoholics with and without ALD as well as 18 healthy subjects. Colonic biopsy samples from subjects were analyzed for microbiota composition using length heterogeneity PCR fingerprinting and multitag pyrosequencing. A subgroup of alcoholics have an altered colonic microbiome (dysbiosis). The alcoholics with dysbiosis had lower median abundances of Bacteroidetes and higher ones of Proteobacteria. The observed alterations appear to correlate with high levels of serum endotoxin in a subset of the samples. Network topology analysis indicated that alcohol use is correlated with decreased connectivity of the microbial network, and this alteration is seen even after an extended period of sobriety. We show that the colonic mucosaassociated bacterial microbiome is altered in a subset of alcoholics. The altered microbiota composition is persistent and correlates with endotoxemia in a subgroup of alcoholics. [ABSTRACT FROM AUTHOR] Copyright of American Journal of Physiology: Gastrointestinal & Liver Physiology is the property of American Physiological Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95871562&site=ehost-live
508,Evaluation of short read metagenomic assembly.,Huzefa Rangwala,BMC Genomics,14712164,,2011 Supplement 2,12,Suppl 2,1,13.0,64164241,10.1186/1471-2164-12-S2-S8,BioMed Central,Article,GENOMES; ORGANISMS; NUCLEOTIDE sequence; ERROR rates; GENETICS,,"Background: Metagenomic assembly is a challenging problem due to the presence of genetic material from multiple organisms. The problem becomes even more difficult when short reads produced by next generation sequencing technologies are used. Although whole genome assemblers are not designed to assemble metagenomic samples, they are being used for metagenomics due to the lack of assemblers capable of dealing with metagenomic samples. We present an evaluation of assembly of simulated short-read metagenomic samples using a state-of-art de Bruijn graph based assembler. Results: We assembled simulated metagenomic reads from datasets of various complexities using a state-of-art de Bruijn graph based parallel assembler. We have also studied the effect of k-mer size used in de Bruijn graph on metagenomic assembly and developed a clustering solution to pool the contigs obtained from different assembly runs, which allowed us to obtain longer contigs. We have also assessed the degree of chimericity of the assembled contigs using an entropy/impurity metric and compared the metagenomic assemblies to assemblies of isolated individual source genomes. Conclusions: Our results show that accuracy of the assembled contigs was better than expected for the metagenomic samples with a few dominant organisms and was especially poor in samples containing many closely related strains. Clustering contigs from different k-mer parameter of the de Bruijn graph allowed us to obtain longer contigs, however the clustering resulted in accumulation of erroneous contigs thus increasing the error rate in clustered contigs. [ABSTRACT FROM AUTHOR] Copyright of BMC Genomics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=64164241&site=ehost-live
509,IDMIL: an alignment-free Interpretable Deep Multiple Instance Learning (MIL) for predicting disease from whole-metagenomic data.,Huzefa Rangwala,Bioinformatics,13674803,,2020 Supplement,36,,i39,9.0,144525612,10.1093/bioinformatics/btaa477,Oxford University Press / USA,Article,CONVOLUTIONAL neural networks; ENVIRONMENTAL forensics; SUPERVISED learning; FORENSIC sciences; HUMAN body; FEATURE extraction; Testing Laboratories,,"Motivation The human body hosts more microbial organisms than human cells. Analysis of this microbial diversity provides key insight into the role played by these microorganisms on human health. Metagenomics is the collective DNA sequencing of coexisting microbial organisms in an environmental sample or a host. This has several applications in precision medicine, agriculture, environmental science and forensics. State-of-the-art predictive models for phenotype predictions from metagenomic data rely on alignments, assembly, extensive pruning, taxonomic profiling and reference sequence databases. These processes are time consuming and they do not consider novel microbial sequences when aligned with the reference genome, limiting the potential of whole metagenomics. We formulate the problem of predicting human disease from whole-metagenomic data using Multiple Instance Learning (MIL), a popular supervised learning paradigm. Our proposed alignment-free approach provides higher accuracy in prediction by harnessing the capability of deep convolutional neural network (CNN) within a MIL framework and provides interpretability via neural attention mechanism. Results The MIL formulation combined with the hierarchical feature extraction capability of deep-CNN provides significantly better predictive performance compared to popular existing approaches. The attention mechanism allows for the identification of groups of sequences that are likely to be correlated to diseases providing the much-needed interpretation. Our proposed approach does not rely on alignment, assembly and reference sequence databases; making it fast and scalable for large-scale metagenomic data. We evaluate our method on well-known large-scale metagenomic studies and show that our proposed approach outperforms comparative state-of-the-art methods for disease prediction. Availability and implementation https://github.com/mrahma23/IDMIL. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144525612&site=ehost-live
510,Mining manufacturing data for discovery of high productivity process characteristics,Huzefa Rangwala,Journal of Biotechnology,01681656,,Jun2010,147,4-Mar,186,12.0,51291934,10.1016/j.jbiotec.2010.04.005,Elsevier B.V.,Article,DATA mining; BIOLOGICAL products; MANUFACTURING industries; MULTIVARIATE analysis; BIOCHEMICAL engineering; CELL culture; SUPPORT vector machines; REGRESSION analysis; Biological Product (except Diagnostic) Manufacturing,Bioprocess; Cell culture; Data mining; Manufacturing; Multivariate data analysis; Support vector machine,"Abstract: Modern manufacturing facilities for bioproducts are highly automated with advanced process monitoring and data archiving systems. The time dynamics of hundreds of process parameters and outcome variables over a large number of production runs are archived in the data warehouse. This vast amount of data is a vital resource to comprehend the complex characteristics of bioprocesses and enhance production robustness. Cell culture process data from 108 ‘trains’ comprising production as well as inoculum bioreactors from Genentech''s manufacturing facility were investigated. Each run constitutes over one-hundred on-line and off-line temporal parameters. A kernel-based approach combined with a maximum margin-based support vector regression algorithm was used to integrate all the process parameters and develop predictive models for a key cell culture performance parameter. The model was also used to identify and rank process parameters according to their relevance in predicting process outcome. Evaluation of cell culture stage-specific models indicates that production performance can be reliably predicted days prior to harvest. Strong associations between several temporal parameters at various manufacturing stages and final process outcome were uncovered. This model-based data mining represents an important step forward in establishing a process data-driven knowledge discovery in bioprocesses. Implementation of this methodology on the manufacturing floor can facilitate a real-time decision making process and thereby improve the robustness of large scale bioprocesses. [Copyright &y& Elsevier] Copyright of Journal of Biotechnology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=51291934&site=ehost-live
511,Modulation of the Metabiome by Rifaximin in Patients with Cirrhosis and Minimal Hepatic Encephalopathy.,Huzefa Rangwala,PLoS ONE,19326203,,Apr2013,8,4,1,11.0,87677218,10.1371/journal.pone.0060042,Public Library of Science,Article,TREATMENT of cirrhosis of the liver; HEPATIC encephalopathy; ANTIBIOTICS; BRAIN function localization; SERUM; NEURAL circuitry; LIPID metabolism; MEDICAL statistics; HEALTH outcome assessment; Pharmaceuticals and pharmacy supplies merchant wholesalers; Drugs and Druggists' Sundries Merchant Wholesalers; Medicinal and Botanical Manufacturing; Pharmaceutical and medicine manufacturing; Biological Product (except Diagnostic) Manufacturing,Biochemistry; Biology; Cirrhosis; Clinical research design; Clinical trials; Cognition; Cognitive neurology; Cognitive neuroscience; Gastroenterology and hepatology; Genetics and Genomics; Host-pathogen interaction; Lipid metabolism; Liver diseases; Medicine; Metabolism; Microbiology; Neurological Disorders; Neurology; Neuroscience; Proteomics; Research Article; Spectrometric identification of proteins; Systems biology,"Hepatic encephalopathy (HE) represents a dysfunctional gut-liver-brain axis in cirrhosis which can negatively impact outcomes. This altered gut-brain relationship has been treated using gut-selective antibiotics such as rifaximin, that improve cognitive function in HE, especially its subclinical form, minimal HE (MHE). However, the precise mechanism of the action of rifaximin in MHE is unclear. We hypothesized that modulation of gut microbiota and their end-products by rifaximin would affect the gut-brain axis and improve cognitive performance in cirrhosis. Aim To perform a systems biology analysis of the microbiome, metabolome and cognitive change after rifaximin in MHE. Methods: Twenty cirrhotics with MHE underwent cognitive testing, endotoxin analysis, urine/serum metabolomics (GC and LC-MS) and fecal microbiome assessment (multi-tagged pyrosequencing) at baseline and 8 weeks post-rifaximin 550 mg BID. Changes in cognition, endotoxin, serum/urine metabolites (and microbiome were analyzed using recommended systems biology techniques. Specifically, correlation networks between microbiota and metabolome were analyzed before and after rifaximin. Results: There was a significant improvement in cognition(six of seven tests improved,p<0.01) and endotoxemia (0.55 to 0.48 Eu/ml, p = 0.02) after rifaximin. There was a significant increase in serum saturated (myristic, caprylic, palmitic, palmitoleic, oleic and eicosanoic) and unsaturated (linoleic, linolenic, gamma-linolenic and arachnidonic) fatty acids post-rifaximin. No significant microbial change apart from a modest decrease in Veillonellaceae and increase in Eubacteriaceae was observed. Rifaximin resulted in a significant reduction in network connectivity and clustering on the correlation networks. The networks centered on Enterobacteriaceae, Porphyromonadaceae and Bacteroidaceae indicated a shift from pathogenic to beneficial metabolite linkages and better cognition while those centered on autochthonous taxa remained similar. Conclusions: Rifaximin is associated with improved cognitive function and endotoxemia in MHE, which is accompanied by alteration of gut bacterial linkages with metabolites without significant change in microbial abundance. Trial Registration: ClinicalTrials.gov NCT01069133 [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=87677218&site=ehost-live
512,Predicting Student Performance Using Personalized Analytics.,Huzefa Rangwala,Computer (00189162),00189162,,Apr2016,49,4,61,9.0,114640262,10.1109/MC.2016.119,IEEE,Article,RATING of students; FACTORIZATION; GRADE repetition; GRADING of students; GRADE advancement,Big data; computing in education; data analysis; data mining; Data models; Data retention; Education; learning-management systems; LMSs; massive open online courses; matrix factorization; MOOCs; multilinear regression; Predictive models; Recommender systems; Servers,"To help solve the ongoing problem of student retention, new expected performance-prediction techniques are needed to facilitate degree planning and determine who might be at risk of failing or dropping a class. Personalized multiregression and matrix factorization approaches based on recommender systems, initially developed for e-commerce applications, accurately forecast students' grades in future courses as well as on in-class assessments. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=114640262&site=ehost-live
513,Real-Time Classification of Hand Motions Using Ultrasound Imaging of Forearm Muscles.,Siddhartha Sikdar,IEEE Transactions on Biomedical Engineering,00189294,,Aug2016,63,8,1687,12.0,116975131,10.1109/TBME.2015.2498124,IEEE,Article,ULTRASONIC imaging; ELECTROMYOGRAPHY; SIGNAL-to-noise ratio; SIGNAL processing; INFORMATION measurement,Image motion analysis; Imaging; Muscles; pattern classification; prosthetic control; Real-time systems; rehabilitation; Thumb; Ultrasonic imaging; ultrasound imaging; Wrist,"Surface electromyography (sEMG) has been the predominant method for sensing electrical activity for a number of applications involving muscle–computer interfaces, including myoelectric control of prostheses and rehabilitation robots. Ultrasound imaging for sensing mechanical deformation of functional muscle compartments can overcome several limitations of sEMG, including the inability to differentiate between deep contiguous muscle compartments, low signal-to-noise ratio, and lack of a robust graded signal. The objective of this study was to evaluate the feasibility of real-time graded control using a computationally efficient method to differentiate between complex hand motions based on ultrasound imaging of forearm muscles. Dynamic ultrasound images of the forearm muscles were obtained from six able-bodied volunteers and analyzed to map muscle activity based on the deformation of the contracting muscles during different hand motions. Each participant performed 15 different hand motions, including digit flexion, different grips (i.e., power grasp and pinch grip), and grips in combination with wrist pronation. During the training phase, we generated a database of activity patterns corresponding to different hand motions for each participant. During the testing phase, novel activity patterns were classified using a nearest neighbor classification algorithm based on that database. The average classification accuracy was 91%. Real-time image-based control of a virtual hand showed an average classification accuracy of 92%. Our results demonstrate the feasibility of using ultrasound imaging as a robust muscle–computer interface. Potential clinical applications include control of multiarticulated prosthetic hands, stroke rehabilitation, and fundamental investigations of motor control and biomechanics. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Biomedical Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116975131&site=ehost-live
514,Solid-Phase Microextraction and the Human Fecal VOC Metabolome.,Huzefa Rangwala,PLoS ONE,19326203,,2011,6,4,1,9.0,61167779,10.1371/journal.pone.0018471,Public Library of Science,Article,VOLATILE organic compounds; SOLID phase extraction; FIBERS; MASS spectrometry; METABOLITES; FECES,,"The diagnostic potential and health implications of volatile organic compounds (VOCs) present in human feces has begun to receive considerable attention. Headspace solid-phase microextraction (SPME) has greatly facilitated the isolation and analysis of VOCs from human feces. Pioneering human fecal VOC metabolomic investigations have utilized a single SPME fiber type for analyte extraction and analysis. However, we hypothesized that the multifarious nature of metabolites present in human feces dictates the use of several diverse SPME fiber coatings for more comprehensive metabolomic coverage. We report here an evaluation of eight different commercially available SPME fibers, in combination with both GC-MS and GCFID, and identify the 50/30 μm CAR-DVB-PDMS, 85 μm CAR-PDMS, 65 μm DVB-PDMS, 7 μm PDMS, and 60 μm PEG SPME fibers as a minimal set of fibers appropriate for human fecal VOC metabolomics, collectively isolating approximately 90% of the total metabolites obtained when using all eight fibers. We also evaluate the effect of extraction duration on metabolite isolation and illustrate that ex vivo enteric microbial fermentation has no effect on metabolite composition during prolonged extractions if the SPME is performed as described herein. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=61167779&site=ehost-live
517,A Sorting Network on Trees.,Dana Richards,Parallel Processing Letters,01296264,,Dec2019,29,4,N.PAG,12.0,140235465,10.1142/S0129626419500154,World Scientific Publishing Company,Article,"COMPLETE graphs; TOPOLOGY; TREES; HYPERCUBES; ALGORITHMS; Flower, Nursery Stock, and Florists' Supplies Merchant Wholesalers",complexity; Oblivious sorting; sorting network,"Sorting networks are a class of parallel oblivious sorting algorithms. Not only do they have interesting theoretical properties but they can be fabricated. A sorting network is a sequence of parallel compare-exchange operations using comparators which are grouped into stages. This underlying graph defines the topology of the network. The majority of results on sorting networks concern the unrestricted case where the underlying graph is the complete graph. Prior results are also known for paths, hypercubes, and meshes. In this paper we introduce a sorting network whose underlying topology is a tree and formalize the concept of sorting networks on a restricted graph topology by introducing a new parameter for graphs called its sorting number. The main result of the paper is a description of an O (min (n Δ 2 , n 2)) depth sorting network on a tree with maximum degree Δ. [ABSTRACT FROM AUTHOR] Copyright of Parallel Processing Letters is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140235465&site=ehost-live
518,Broadcasting from multiple originators,Dana Richards,Discrete Applied Mathematics,0166218X,,Jul2009,157,13,2886,6.0,41239697,10.1016/j.dam.2009.02.013,Elsevier B.V.,Article,GRAPH theory; BROADCASTING industry; FUNDAMENTAL groups (Mathematics); GRAPHIC methods; ASYMPTOTES; MATHEMATICAL analysis; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,Broadcast; Minimum broadcast graph; Multiple originator; Relaxed broadcast,"Abstract: We begin an investigation of broadcasting from multiple originators, a variant of broadcasting in which any vertices may be the originators of a message in a network of vertices. The requirement is that the message be distributed to all vertices in minimum time. A minimum -originator broadcast graph is a graph on vertices with the fewest edges such that any subset of vertices can broadcast in minimum time. is the number of edges in such a graph. In this paper, we present asymptotic upper and lower bounds on . We also present an exact result for the case when . We also give an upper bound on the number of edges in a relaxed version of this problem in which one additional time unit is allowed for the broadcast. [Copyright &y& Elsevier] Copyright of Discrete Applied Mathematics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=41239697&site=ehost-live
519,Let the games continue.,Dana Richards,Scientific American,00368733,,Oct2014,311,4,90,6.0,98530162,10.1038/scientificamerican1014-90,Scientific American,journal article,"RECREATIONAL mathematics; POLYOMINOES; CELLULAR automata; PARADOXES; TILING (Mathematics); GARDNER, Martin, 1914-2010; CONWAY, John H. (John Horton), 1937-",,"The article focuses on late author of ""Scientific American's"" Mathematical Games column, Martin Gardner. Topics include his influence and popularity within recreational mathematics, Gardner's early entries in the journal ""Scripta Mathematica"" and ""Scientific American"" on mathematical magic and machines that could solve basic problems, polyominoes and mathematician John Horton Conway's ""Game of Life,"" and Gardner's interest in Penrose tilings and Newcomb's Paradox.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=98530162&site=ehost-live
520,Martin Gardner (1914-2010).,Dana Richards,Science,00368075,,7/9/2010,329,5988,157,1.0,52518333,10.1126/science.1194002,American Association for the Advancement of Science,Obituary,"GARDNER, Martin, 1914-2010",,An obituary for science author Martin Gardner is presented.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=52518333&site=ehost-live
521,Minimum multiple originator broadcast graphs.,Dana Richards,Discrete Applied Mathematics,0166218X,,Jan2017 Part 3,216,,646,16.0,120016179,10.1016/j.dam.2016.06.015,Elsevier B.V.,Article,GRAPH theory; BROADCASTING industry; GEOMETRIC vertices; EDGES (Geometry); SET theory; APPLIED mathematics; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,Broadcast; Minimum broadcast graph; Multiple originator,"Broadcasting from multiple originators is a variant of broadcasting in which any k vertices may be the originators of a message in a network of n vertices. A minimum broadcast graph has the fewest possible edges while still allowing minimum time broadcasting from any set of k originators. We provide a census of all known such graphs. [ABSTRACT FROM AUTHOR] Copyright of Discrete Applied Mathematics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=120016179&site=ehost-live
550,GKMPAN: An Efficient Group Rekeying Scheme for Secure Multicast in Ad-Hoc Networks.,Sanjeev Setia,Journal of Computer Security,0926227X,,2006,14,4,301,25.0,22976131,10.3233/JCS-2006-14401,IOS Press,Article,COMPUTER networks; SENSOR networks; DETECTORS; NETWORK routers; PROBABILISTIC number theory; PROBABILISTIC automata; PHASE shift keying; COMPUTER network protocols; Computer Systems Design Services,ad hoc networks; Group key management; key updating; probabilistic key sharing; stateless,"We present GKMPAN, an efficient and scalable group rekeying protocol for secure multicast in ad hoc networks. Our protocol exploits the property of ad hoc networks that each member of a group is both a host and a router, and distributes the group key to member nodes via a secure hop-by-hop propagation scheme. A probabilistic scheme based on pre-deployed symmetric keys is used for implementing secure channels between members for group key distribution. GKMPAN also includes a novel distributed scheme for efficiently updating the pre-deployed keys. GKMPAN has three attractive properties. First, it is significantly more efficient than group rekeying schemes that were adapted from those proposed for wired networks. Second, GKMPAN has the property of partial statelessness; that is, a node can decode the current group key even if it has missed a certain number of previous group rekeying operations. This makes it very attractive for ad hoc networks where nodes may lose packets due to transmission link errors or temporary network partitions. Third, in GKMPAN the key server does not need any information about the topology of the ad hoc network or the geographic location of the members of the group. We study the security and performance of GKMPAN through detailed analysis and simulation; we have also implemented GKMPAN in a sensor network testbed. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computer Security is the property of IOS Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22976131&site=ehost-live
551,A Data-Driven Evolutionary Algorithm for Mapping Multibasin Protein Energy Landscapes.,Amarda Shehu,Journal of Computational Biology,10665277,,Sep2015,22,9,844,17.0,109251698,10.1089/cmb.2015.0107,"Mary Ann Liebert, Inc.",Article,"EVOLUTIONARY algorithms; DIMENSION reduction (Statistics); PROTEIN structure; MULTISCALE modeling; COMPUTATIONAL biology; Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",dimensionality reduction; evolutionary algorithm; multibasin energy landscape; multiscale modeling; protein structure modeling,"Evidence is emerging that many proteins involved in proteinopathies are dynamic molecules switching between stable and semistable structures to modulate their function. A detailed understanding of the relationship between structure and function in such molecules demands a comprehensive characterization of their conformation space. Currently, only stochastic optimization methods are capable of exploring conformation spaces to obtain sample-based representations of associated energy surfaces. These methods have to address the fundamental but challenging issue of balancing computational resources between exploration (obtaining a broad view of the space) and exploitation (going deep in the energy surface). We propose a novel algorithm that strikes an effective balance by employing concepts from evolutionary computation. The algorithm leverages deposited crystal structures of wildtype and variant sequences of a protein to define a reduced, low-dimensional search space from where to rapidly draw samples. A multiscale technique maps samples to local minima of the all-atom energy surface of a protein under investigation. Several novel algorithmic strategies are employed to avoid premature convergence to particular minima and obtain a broad view of a possibly multibasin energy surface. Analysis of applications on different proteins demonstrates the broad utility of the algorithm to map multibasin energy landscapes and advance modeling of multibasin proteins. In particular, applications on wildtype and variant sequences of proteins involved in proteinopathies demonstrate that the algorithm makes an important first step toward understanding the impact of sequence mutations on misfunction by providing the energy landscape as the intermediate explanatory link between protein sequence and function. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational Biology is the property of Mary Ann Liebert, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109251698&site=ehost-live
553,Balancing multiple objectives in conformation sampling to control decoy diversity in template-free protein structure prediction.,Ahmed Bin Zaman,BMC Bioinformatics,14712105,,4/25/2019,20,1,1,17.0,136097840,10.1186/s12859-019-2794-5,BioMed Central,Article,PROTEIN structure; AMINO acids; ATOMIC interactions; PROTEIN conformation; ALGORITHMS,Protein energy landscape; Stochastic optimization; Structural dynamics,"Background: Computational approaches for the determination of biologically-active/native three-dimensional structures of proteins with novel sequences have to handle several challenges. The (conformation) space of possible three-dimensional spatial arrangements of the chain of amino acids that constitute a protein molecule is vast and high-dimensional. Exploration of the conformation spaces is performed in a sampling-based manner and is biased by the internal energy that sums atomic interactions. Even state-of-the-art energy functions that quantify such interactions are inherently inaccurate and associate with protein conformation spaces overly rugged energy surfaces riddled with artifact local minima. The response to these challenges in template-free protein structure prediction is to generate large numbers of low-energy conformations (also referred to as decoys) as a way of increasing the likelihood of having a diverse decoy dataset that covers a sufficient number of local minima possibly housing near-native conformations. Results: In this paper we pursue a complementary approach and propose to directly control the diversity of generated decoys. Inspired by hard optimization problems in high-dimensional and non-linear variable spaces, we propose that conformation sampling for decoy generation is more naturally framed as a multi-objective optimization problem. We demonstrate that mechanisms inherent to evolutionary search techniques facilitate such framing and allow balancing multiple objectives in protein conformation sampling. We showcase here an operationalization of this idea via a novel evolutionary algorithm that has high exploration capability and is also able to access lower-energy regions of the energy landscape of a given protein with similar or better proximity to the known native structure than several state-of-the-art decoy generation algorithms. Conclusions: The presented results constitute a promising research direction in improving decoy generation for template-free protein structure prediction with regards to balancing of multiple conflicting objectives under an optimization framework. Future work will consider additional optimization objectives and variants of improvement and selection operators to apportion a fixed computational budget. Of particular interest are directions of research that attenuate dependence on protein energy models. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136097840&site=ehost-live
553,Balancing multiple objectives in conformation sampling to control decoy diversity in template-free protein structure prediction.,Amarda Shehu,BMC Bioinformatics,14712105,,4/25/2019,20,1,1,17.0,136097840,10.1186/s12859-019-2794-5,BioMed Central,Article,PROTEIN structure; AMINO acids; ATOMIC interactions; PROTEIN conformation; ALGORITHMS,Protein energy landscape; Stochastic optimization; Structural dynamics,"Background: Computational approaches for the determination of biologically-active/native three-dimensional structures of proteins with novel sequences have to handle several challenges. The (conformation) space of possible three-dimensional spatial arrangements of the chain of amino acids that constitute a protein molecule is vast and high-dimensional. Exploration of the conformation spaces is performed in a sampling-based manner and is biased by the internal energy that sums atomic interactions. Even state-of-the-art energy functions that quantify such interactions are inherently inaccurate and associate with protein conformation spaces overly rugged energy surfaces riddled with artifact local minima. The response to these challenges in template-free protein structure prediction is to generate large numbers of low-energy conformations (also referred to as decoys) as a way of increasing the likelihood of having a diverse decoy dataset that covers a sufficient number of local minima possibly housing near-native conformations. Results: In this paper we pursue a complementary approach and propose to directly control the diversity of generated decoys. Inspired by hard optimization problems in high-dimensional and non-linear variable spaces, we propose that conformation sampling for decoy generation is more naturally framed as a multi-objective optimization problem. We demonstrate that mechanisms inherent to evolutionary search techniques facilitate such framing and allow balancing multiple objectives in protein conformation sampling. We showcase here an operationalization of this idea via a novel evolutionary algorithm that has high exploration capability and is also able to access lower-energy regions of the energy landscape of a given protein with similar or better proximity to the known native structure than several state-of-the-art decoy generation algorithms. Conclusions: The presented results constitute a promising research direction in improving decoy generation for template-free protein structure prediction with regards to balancing of multiple conflicting objectives under an optimization framework. Future work will consider additional optimization objectives and variants of improvement and selection operators to apportion a fixed computational budget. Of particular interest are directions of research that attenuate dependence on protein energy models. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136097840&site=ehost-live
554,Building maps of protein structure spaces in template-free protein structure prediction.,Ahmed Bin Zaman,Journal of Bioinformatics & Computational Biology,02197200,,Dec2019,17,6,N.PAG,17.0,141546480,10.1142/S0219720019400134,World Scientific Publishing Company,Article,PROTEIN structure; SPACE frame structures; ALGORITHMS; AMINO acid sequence; TERTIARY structure,decoy generation; decoy quality; evolutionary algorithm; map of protein structure space; Template-free protein structure prediction,"An important goal in template-free protein structure prediction is how to control the quality of computed tertiary structures of a target amino-acid sequence. Despite great advances in algorithmic research, given the size, dimensionality, and inherent characteristics of the protein structure space, this task remains exceptionally challenging. It is current practice to aim to generate as many structures as can be afforded so as to increase the likelihood that some of them will reside near the sought but unknown biologically-active/native structure. When operating within a given computational budget, this is impractical and uninformed by any metrics of interest. In this paper, we propose instead to equip algorithms that generate tertiary structures, also known as decoy generation algorithms, with memory of the protein structure space that they explore. Specifically, we propose an evolving, granularity-controllable map of the protein structure space that makes use of low-dimensional representations of protein structures. Evaluations on diverse target sequences that include recent hard CASP targets show that drastic reductions in storage can be made without sacrificing decoy quality. The presented results make the case that integrating a map of the protein structure space is a promising mechanism to enhance decoy generation algorithms in template-free protein structure prediction. [ABSTRACT FROM AUTHOR] Copyright of Journal of Bioinformatics & Computational Biology is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141546480&site=ehost-live
554,Building maps of protein structure spaces in template-free protein structure prediction.,Amarda Shehu,Journal of Bioinformatics & Computational Biology,02197200,,Dec2019,17,6,N.PAG,17.0,141546480,10.1142/S0219720019400134,World Scientific Publishing Company,Article,PROTEIN structure; SPACE frame structures; ALGORITHMS; AMINO acid sequence; TERTIARY structure,decoy generation; decoy quality; evolutionary algorithm; map of protein structure space; Template-free protein structure prediction,"An important goal in template-free protein structure prediction is how to control the quality of computed tertiary structures of a target amino-acid sequence. Despite great advances in algorithmic research, given the size, dimensionality, and inherent characteristics of the protein structure space, this task remains exceptionally challenging. It is current practice to aim to generate as many structures as can be afforded so as to increase the likelihood that some of them will reside near the sought but unknown biologically-active/native structure. When operating within a given computational budget, this is impractical and uninformed by any metrics of interest. In this paper, we propose instead to equip algorithms that generate tertiary structures, also known as decoy generation algorithms, with memory of the protein structure space that they explore. Specifically, we propose an evolving, granularity-controllable map of the protein structure space that makes use of low-dimensional representations of protein structures. Evaluations on diverse target sequences that include recent hard CASP targets show that drastic reductions in storage can be made without sacrificing decoy quality. The presented results make the case that integrating a map of the protein structure space is a promising mechanism to enhance decoy generation algorithms in template-free protein structure prediction. [ABSTRACT FROM AUTHOR] Copyright of Journal of Bioinformatics & Computational Biology is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141546480&site=ehost-live
555,Computational Methods for Exploration and Analysis of Macromolecular Structure and Dynamics.,Amarda Shehu,PLoS Computational Biology,1553734X,,10/27/2015,11,10,1,3.0,110560854,10.1371/journal.pcbi.1004585,Public Library of Science,Editorial,COMPUTATIONAL mechanics; MACROMOLECULAR dynamics; COMPUTER simulation; MACROMOLECULES; MICROSCOPIC kinetics,Editorial,"The article presents author's views regarding the use and development of computational methods for exploring and analyzing macromolecular structure and dynamics. Topics discussed include benefits of computations in presenting a complete description of the processes of life, computer simulations as a bridge between the microscopic length and time scales, and the macroscopic world of the laboratory and role of macromolecules in countless biological processes.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=110560854&site=ehost-live
556,Computing the Structural Dynamics of RVFV L Protein Domain in Aqueous Glycerol Solutions.,Amarda Shehu,Biomolecules (2218-273X),2218273X,,Oct2021,11,10,1427,1.0,153220770,10.3390/biom11101427,MDPI,Article,STRUCTURAL dynamics; PROTEIN domains; CONFORMATIONAL analysis; AQUEOUS solutions; RIFT Valley fever; RADIAL distribution function; GLYCERIN; Soap and cleaning compound manufacturing; Soap and Other Detergent Manufacturing,aqueous glycerol; molecular dynamics; RVFV; structural dynamics,"Many biological and biotechnological processes are controlled by protein–protein and protein–solvent interactions. In order to understand, predict, and optimize such processes, it is important to understand how solvents affect protein structure during protein–solvent interactions. In this study, all-atom molecular dynamics are used to investigate the structural dynamics and energetic properties of a C-terminal domain of the Rift Valley Fever Virus L protein solvated in glycerol and aqueous glycerol solutions in different concentrations by molecular weight. The Generalized Amber Force Field is modified by including restrained electrostatic potential atomic charges for the glycerol molecules. The peptide is considered in detail by monitoring properties like the root-mean-squared deviation, root-mean-squared fluctuation, radius of gyration, hydrodynamic radius, end-to-end distance, solvent-accessible surface area, intra-potential energy, and solvent–peptide interaction energies for hundreds of nanoseconds. Secondary structure analysis is also performed to examine the extent of conformational drift for the individual helices and sheets. We predict that the peptide helices and sheets are maintained only when the modeling strategy considers the solvent with lower glycerol concentration. We also find that the solvent-peptide becomes more cohesive with decreasing glycerol concentrations. The density and radial distribution function of glycerol solvent calculated when modeled with the modified atomic charges show a very good agreement with experimental results and other simulations at 298.15 K. [ABSTRACT FROM AUTHOR] Copyright of Biomolecules (2218-273X) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=153220770&site=ehost-live
557,Data Size and Quality Matter: Generating Physically-Realistic Distance Maps of Protein Tertiary Structures.,Amarda Shehu,Biomolecules (2218-273X),2218273X,,Jul2022,12,7,N.PAG,22.0,158211289,10.3390/biom12070908,MDPI,Article,TERTIARY structure; PROTEIN structure; DATA quality; CYTOSKELETAL proteins; LATENT variables,disentanglement; generative model; multi-structure view; protein molecule; spatial pyramidal pooling; tertiary structure; training set configuration; variational autoencoder,"With the debut of AlphaFold2, we now can get a highly-accurate view of a reasonable equilibrium tertiary structure of a protein molecule. Yet, a single-structure view is insufficient and does not account for the high structural plasticity of protein molecules. Obtaining a multi-structure view of a protein molecule continues to be an outstanding challenge in computational structural biology. In tandem with methods formulated under the umbrella of stochastic optimization, we are now seeing rapid advances in the capabilities of methods based on deep learning. In recent work, we advance the capability of these models to learn from experimentally-available tertiary structures of protein molecules of varying lengths. In this work, we elucidate the important role of the composition of the training dataset on the neural network's ability to learn key local and distal patterns in tertiary structures. To make such patterns visible to the network, we utilize a contact map-based representation of protein tertiary structure. We show interesting relationships between data size, quality, and composition on the ability of latent variable models to learn key patterns of tertiary structure. In addition, we present a disentangled latent variable model which improves upon the state-of-the-art variable autoencoder-based model in key, physically-realistic structural patterns. We believe this work opens up further avenues of research on deep learning-based models for computing multi-structure views of protein molecules. [ABSTRACT FROM AUTHOR] Copyright of Biomolecules (2218-273X) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158211289&site=ehost-live
558,Editorial overview: Theory and simulation and their new friends.,Amarda Shehu,Current Opinion in Structural Biology,0959440X,,Apr2021,67,,iii,1.0,150069315,10.1016/j.sbi.2021.02.003,Elsevier B.V.,Article,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150069315&site=ehost-live
559,Elucidating the ensemble of functionally-relevant transitions in protein systems with a robotics-inspired method.,Amarda Shehu,BMC Structural Biology,14726807,,2013Suppl 1,13,,1,22.0,131875276,10.1186/1472-6807-13-S1-S8,BioMed Central,Article,,,"Background: Many proteins tune their biological function by transitioning between different functional states, effectively acting as dynamic molecular machines. Detailed structural characterization of transition trajectories is central to understanding the relationship between protein dynamics and function. Computational approaches that build on the Molecular Dynamics framework are in principle able to model transition trajectories at great detail but also at considerable computational cost. Methods that delay consideration of dynamics and focus instead on elucidating energetically-credible conformational paths connecting two functionally-relevant structures provide a complementary approach. Effective sampling-based path planning methods originating in robotics have been recently proposed to produce conformational paths. These methods largely model short peptides or address large proteins by simplifying conformational space. Methods: We propose a robotics-inspired method that connects two given structures of a protein by sampling conformational paths. The method focuses on small- to medium-size proteins, efficiently modeling structural deformations through the use of the molecular fragment replacement technique. In particular, the method grows a tree in conformational space rooted at the start structure, steering the tree to a goal region defined around the goal structure. We investigate various bias schemes over a progress coordinate for balance between coverage of conformational space and progress towards the goal. A geometric projection layer promotes path diversity. A reactive temperature scheme allows sampling of rare paths that cross energy barriers. Results and conclusions: Experiments are conducted on small- to medium-size proteins of length up to 214 amino acids and with multiple known functionally-relevant states, some of which are more than 13Å apart of each-other. Analysis reveals that the method effectively obtains conformational paths connecting structural states that are significantly different. A detailed analysis on the depth and breadth of the tree suggests that a soft global bias over the progress coordinate enhances sampling and results in higher path diversity. The explicit geometric projection layer that biases the exploration away from over-sampled regions further increases coverage, often improving proximity to the goal by forcing the exploration to find new paths. The reactive temperature scheme is shown effective in increasing path diversity, particularly in difficult structural transitions with known high-energy barriers. [ABSTRACT FROM AUTHOR] Copyright of BMC Structural Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131875276&site=ehost-live
560,Evolutionary-inspired probabilistic search for enhancing sampling of local minima in the protein energy surface.,Amarda Shehu,Proteome Science,14775956,,2012,10,Suppl 1,1,16.0,77940817,10.1186/1477-5956-10-S1-S5,BioMed Central,Article,"PROBABILITY theory; COMPUTATIONAL biology; CONFORMATIONAL analysis; MOLECULAR conformation; PROTEIN structure; PROTEIN-protein interactions; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences",,"Background: Despite computational challenges, elucidating conformations that a protein system assumes under physiologic conditions for the purpose of biological activity is a central problem in computational structural biology. While these conformations are associated with low energies in the energy surface that underlies the protein conformational space, few existing conformational search algorithms focus on explicitly sampling lowenergy local minima in the protein energy surface. Methods: This work proposes a novel probabilistic search framework, PLOW, that explicitly samples low-energy local minima in the protein energy surface. The framework combines algorithmic ingredients from evolutionary computation and computational structural biology to effectively explore the subspace of local minima. A greedy local search maps a conformation sampled in conformational space to a nearby local minimum. A perturbation move jumps out of a local minimum to obtain a new starting conformation for the greedy local search. The process repeats in an iterative fashion, resulting in a trajectory-based exploration of the subspace of local minima. Results and conclusions: The analysis of PLOW's performance shows that, by navigating only the subspace of local minima, PLOW is able to sample conformations near a protein's native structure, either more effectively or as well as state-of-the-art methods that focus on reproducing the native structure for a protein system. Analysis of the actual subspace of local minima shows that PLOW samples this subspace more effectively that a naive sampling approach. Additional theoretical analysis reveals that the perturbation function employed by PLOW is key to its ability to sample a diverse set of low-energy conformations. This analysis also suggests directions for further research and novel applications for the proposed framework. [ABSTRACT FROM AUTHOR] Copyright of Proteome Science is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=77940817&site=ehost-live
561,"Fewer Dimensions, More Structures for Improved Discrete Models of Dynamics of Free versus Antigen-Bound Antibody.",Amarda Shehu,Biomolecules (2218-273X),2218273X,,Jul2022,12,7,N.PAG,19.0,158211392,10.3390/biom12071011,MDPI,Article,MOLECULAR dynamics; MARKOV processes; IMMUNOGLOBULINS,antibody; antigen binding; Markov State Model; molecular dynamics; structure,"Over the past decade, Markov State Models (MSM) have emerged as powerful methodologies to build discrete models of dynamics over structures obtained from Molecular Dynamics trajectories. The identification of macrostates for the MSM is a central decision that impacts the quality of the MSM but depends on both the selected representation of a structure and the clustering algorithm utilized over the featurized structures. Motivated by a large molecular system in its free and bound state, this paper investigates two directions of research, further reducing the representation dimensionality in a non-parametric, data-driven manner and including more structures in the computation. Rigorous evaluation of the quality of obtained MSMs via various statistical tests in a comparative setting firmly shows that fewer dimensions and more structures result in a better MSM. Many interesting findings emerge from the best MSM, advancing our understanding of the relationship between antibody dynamics and antibody–antigen recognition. [ABSTRACT FROM AUTHOR] Copyright of Biomolecules (2218-273X) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158211392&site=ehost-live
562,From Extraction of Local Structures of Protein Energy Landscapes to Improved Decoy Selection in Template-Free Protein Structure Prediction.,Amarda Shehu,Molecules,14203049,,Jan2018,23,1,216,20.0,127754426,10.3390/molecules23010216,MDPI,Article,"PROTEIN structure; STRUCTURAL proteomics; PROTEIN domains; COMPUTATIONAL biology; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences",basins; conformational space; decoy selection; energy landscape; Pareto optimality; template-free protein structure prediction,"Due to the essential role that the three-dimensional conformation of a protein plays in regulating interactions with molecular partners, wet and dry laboratories seek biologically-active conformations of a protein to decode its function. Computational approaches are gaining prominence due to the labor and cost demands of wet laboratory investigations. Template-free methods can now compute thousands of conformations known as decoys, but selecting native conformations from the generated decoys remains challenging. Repeatedly, research has shown that the protein energy functions whose minima are sought in the generation of decoys are unreliable indicators of nativeness. The prevalent approach ignores energy altogether and clusters decoys by conformational similarity. Complementary recent efforts design protein-specific scoring functions or train machine learning models on labeled decoys. In this paper, we show that an informative consideration of energy can be carried out under the energy landscape view. Specifically, we leverage local structures known as basins in the energy landscape probed by a template-free method. We propose and compare various strategies of basin-based decoy selection that we demonstrate are superior to clustering-based strategies. The presented results point to further directions of research for improving decoy selection, including the ability to properly consider the multiplicity of native conformations of proteins. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127754426&site=ehost-live
563,Generative deep learning for macromolecular structure and dynamics.,Amarda Shehu,Current Opinion in Structural Biology,0959440X,,Apr2021,67,,170,8.0,150069307,10.1016/j.sbi.2020.11.012,Elsevier B.V.,Article,"MACROMOLECULAR dynamics; COMPUTATIONAL biology; MOLECULAR biology; DYNAMICAL systems; STRUCTURAL dynamics; DEEP learning; LEARNING communities; Research and development in the physical, engineering and life sciences; Research and Development in Biotechnology; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,"Much scientific enquiry across disciplines is founded upon a mechanistic treatment of dynamic systems that ties form to function. A highly visible instance of this is in molecular biology, where characterizing macromolecular structure and dynamics is central to a detailed, molecular-level understanding of biological processes in the living cell. The current computational paradigm utilizes optimization as the generative process for modeling both structure and structural dynamics. Computational biology researchers are now attempting to wield generative models employing deep neural networks as an alternative computational paradigm. In this review, we summarize such efforts. We highlight progress and shortcomings. More importantly, we expose challenges that macromolecular structure poses to deep generative models and take this opportunity to introduce the structural biology community to several recent advances in the deep learning community that promise a way forward. [ABSTRACT FROM AUTHOR] Copyright of Current Opinion in Structural Biology is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150069307&site=ehost-live
564,Guiding the Search for Native-like Protein Conformations with an Ab-initio Tree-based Exploration.,Amarda Shehu,International Journal of Robotics Research,02783649,,Jul2010,29,8,1106,22.0,60833518,10.1177/0278364910371527,"Sage Publications, Ltd.",Article,PROTEIN conformation; ROBOTICS; STATISTICAL sampling; AMINO acid sequence; MONTE Carlo method; DATA structures; PROTEIN engineering; Research and Development in Biotechnology; Marketing Research and Public Opinion Polling,discretization layers; energy landscape; guided exploration; native-like protein conformations; probabilistic sampling; projection space; robotics-inspired; tree-based search,"In this paper we propose a robotics-inspired method to enhance sampling of native-like conformations when employing only aminoacid sequence information for a protein at hand. Computing such conformations, essential to associating structural and functional information with gene sequences, is challenging due to the high-dimensionality and the rugged energy surface of the protein conformational space. The contribution of this paper is a novel two-layered method to enhance the sampling of geometrically distinct low-energy conformations at a coarse-grained level of detail. The method grows a tree in conformational space reconciling two goals: (i) guiding the tree towards lower energies; and (ii) not oversampling geometrically similar conformations. Discretizations of the energy surface and a low-dimensional projection space are employed to select more often for expansion low-energy conformations in under-explored regions of the conformational space. The tree is expanded with low-energy conformations through a Metropolis Monte Carlo framework that uses a move set of physical fragment configurations. Testing on sequences of eight small-to-medium structurally diverse proteins shows that the method rapidly samples native-like conformations in a few hours on a single CPU. Analysis shows that computed conformations are good candidates for further detailed energetic refinements by larger studies in protein engineering and design. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Robotics Research is the property of Sage Publications, Ltd. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=60833518&site=ehost-live
565,Modeling Structures and Motions of Loops in Protein Molecules.,Amarda Shehu,Entropy,10994300,,Jun2012,14,2,252,39.0,72325506,10.3390/e14020252,MDPI,Article,"PROTEIN structure; LIGAND binding; COMPUTATIONAL biology; PROTEIN stability; CONFORMATIONAL analysis; BIOINFORMATICS; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology; Research and development in the physical, engineering and life sciences",conformational ensemble; equilibrium fluctuations; loop modeling; native state; structural analysis of proteins; structural bioinformatics,"Unlike the secondary structure elements that connect in protein structures, loop fragments in protein chains are often highly mobile even in generally stable proteins. The structural variability of loops is often at the center of a protein's stability, folding, and even biological function. Loops are found to mediate important biological processes, such as signaling, protein-ligand binding, and protein-protein interactions. Modeling conformations of a loop under physiological conditions remains an open problem in computational biology. This article reviews computational research in loop modeling, highlighting progress and challenges. Important insight is obtained on potential directions for future research. [ABSTRACT FROM AUTHOR] Copyright of Entropy is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=72325506&site=ehost-live
566,Small molecule generation via disentangled representation learning.,Amarda Shehu,Bioinformatics,13674803,,Jun2022,38,12,3200,9.0,157413582,10.1093/bioinformatics/btac296,Oxford University Press / USA,Article,SMALL molecules; MATERIALS science; PROTEIN-protein interactions; ATOMIC interactions; DRUG discovery; CHEMICAL structure,,"Motivation Expanding our knowledge of small molecules beyond what is known in nature or designed in wet laboratories promises to significantly advance cheminformatics, drug discovery, biotechnology and material science. In silico molecular design remains challenging, primarily due to the complexity of the chemical space and the non-trivial relationship between chemical structures and biological properties. Deep generative models that learn directly from data are intriguing, but they have yet to demonstrate interpretability in the learned representation, so we can learn more about the relationship between the chemical and biological space. In this article, we advance research on disentangled representation learning for small molecule generation. We build on recent work by us and others on deep graph generative frameworks, which capture atomic interactions via a graph-based representation of a small molecule. The methodological novelty is how we leverage the concept of disentanglement in the graph variational autoencoder framework both to generate biologically relevant small molecules and to enhance model interpretability. Results Extensive qualitative and quantitative experimental evaluation in comparison with state-of-the-art models demonstrate the superiority of our disentanglement framework. We believe this work is an important step to address key challenges in small molecule generation with deep generative frameworks. Availability and implementation Training and generated data are made available at https://ieee-dataport.org/documents/dataset-disentangled-representation-learning-interpretable-molecule-generation. All code is made available at https://anonymous.4open.science/r/D-MolVAE-2799/. Supplementary information Supplementary data are available at Bioinformatics online. [ABSTRACT FROM AUTHOR] Copyright of Bioinformatics is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157413582&site=ehost-live
568,The 6th Computational Structural Bioinformatics Workshop.,Amarda Shehu,BMC Structural Biology,14726807,,2013Suppl 1,13,,1,2.0,131873982,10.1186/1472-6807-13-S1-I1,BioMed Central,Article,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131873982&site=ehost-live
569,Unsupervised multi-instance learning for protein structure determination.,Amarda Shehu,Journal of Bioinformatics & Computational Biology,02197200,,Feb2021,19,1,N.PAG,20.0,149011628,10.1142/S0219720021400023,World Scientific Publishing Company,Article,PROTEIN structure; LEARNING; PATIENT selection,decoy quality; decoy selection; multi-instance learning; Protein structure determination; unsupervised learning,"Many regions of the protein universe remain inaccessible by wet-laboratory or computational structure determination methods. A significant challenge in elucidating these dark regions in silico relates to the ability to discriminate relevant structure(s) among many structures/decoys computed for a protein of interest, a problem known as decoy selection. Clustering decoys based on geometric similarity remains popular. However, it is unclear how exactly to exploit the groups of decoys revealed via clustering to select individual structures for prediction. In this paper, we provide an intuitive formulation of the decoy selection problem as an instance of unsupervised multi-instance learning. We address the problem in three stages, first organizing given decoys of a protein molecule into bags, then identifying relevant bags, and finally drawing individual instances from these bags to offer as prediction. We propose both non-parametric and parametric algorithms for drawing individual instances. Our evaluation utilizes two datasets, one benchmark dataset of ensembles of decoys for a varied list of protein molecules, and a dataset of decoy ensembles for targets drawn from recent CASP competitions. A comparative analysis with state-of-the-art methods reveals that the proposed approach outperforms existing methods, thus warranting further investigation of multi-instance learning to advance our treatment of decoy selection. [ABSTRACT FROM AUTHOR] Copyright of Journal of Bioinformatics & Computational Biology is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149011628&site=ehost-live
570,A population-based evolutionary search approach to the multiple minima problem in de novo protein structure prediction.,Amarda Shehu,BMC Structural Biology,14726807,,2013Suppl 1,13,,1,19.0,131874027,10.1186/1472-6807-13-S1-S4,BioMed Central,Article,,,"Background: Elucidating the native structure of a protein molecule from its sequence of amino acids, a problem known as de novo structure prediction, is a long standing challenge in computational structural biology. Difficulties in silico arise due to the high dimensionality of the protein conformational space and the ruggedness of the associated energy surface. The issue of multiple minima is a particularly troublesome hallmark of energy surfaces probed with current energy functions. In contrast to the true energy surface, these surfaces are weakly-funneled and rich in comparably deep minima populated by non-native structures. For this reason, many algorithms seek to be inclusive and obtain a broad view of the low-energy regions through an ensemble of low-energy (decoy) conformations. Conformational diversity in this ensemble is key to increasing the likelihood that the native structure has been captured. Methods: We propose an evolutionary search approach to address the multiple-minima problem in decoy sampling for de novo structure prediction. Two population-based evolutionary search algorithms are presented that follow the basic approach of treating conformations as individuals in an evolving population. Coarse graining and molecular fragment replacement are used to efficiently obtain protein-like child conformations from parents. Potential energy is used both to bias parent selection and determine which subset of parents and children will be retained in the evolving population. The effect on the decoy ensemble of sampling minima directly is measured by additionally mapping a conformation to its nearest local minimum before considering it for retainment. The resulting memetic algorithm thus evolves not just a population of conformations but a population of local minima. Results and conclusions: Results show that both algorithms are effective in terms of sampling conformations in proximity of the known native structure. The additional minimization is shown to be key to enhancing sampling capability and obtaining a diverse ensemble of decoy conformations, circumventing premature convergence to sub-optimal regions in the conformational space, and approaching the native structure with proximity that is comparable to state-of-the-art decoy sampling methods. The results are shown to be robust and valid when using two representative state-of-the-art coarse-grained energy functions. [ABSTRACT FROM AUTHOR] Copyright of BMC Structural Biology is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131874027&site=ehost-live
571,A stochastic roadmap method to model protein structural transitions.,Amarda Shehu,Robotica,02635747,,Aug2016,34,8,1705,29.0,116719274,10.1017/S0263574715001058,Cambridge University Press,Article,ROBOT motion; PROTEIN structure; GENETIC mutation; DISEASES; MARKOV processes,Conformational switching; Energy landscape; First-step analysis; Human disorders; Markov state model; Path smoothing; Protein dynamics; SOD1 and Ras variants; Stochastic roadmap simulation; Structural transitions,"Evidence is emerging that the role of protein structure in disease needs to be rethought. Sequence mutations in proteins are often found to affect the rate at which a protein switches between structures. Modeling structural transitions in wildtype and variant proteins is central to understanding the molecular basis of disease. This paper investigates an efficient algorithmic realization of the stochastic roadmap simulation framework to model structural transitions in wildtype and variants of proteins implicated in human disorders. Our results indicate that the algorithm is able to extract useful information on the impact of mutations on protein structure and function. [ABSTRACT FROM PUBLISHER] Copyright of Robotica is the property of Cambridge University Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116719274&site=ehost-live
572,Attenuating dependence on structural data in computing protein energy landscapes.,Amarda Shehu,BMC Bioinformatics,14712105,,6/6/2019 Supplement 11,20,11,1,10.0,136827411,10.1186/s12859-019-2822-5,BioMed Central,Article,ATTENUATION (Physics); PROTEINS; HUMAN biology; LANDSCAPES; STOCHASTIC analysis,Protein energy landscape; Stochastic optimization; Structural dynamics,"Background: Nearly all cellular processes involve proteins structurally rearranging to accommodate molecular partners. The energy landscape underscores the inherent nature of proteins as dynamic molecules interconverting between structures with varying energies. In principle, reconstructing a protein's energy landscape holds the key to characterizing the structural dynamics and its regulation of protein function. In practice, the disparate spatio-temporal scales spanned by the slow dynamics challenge both wet and dry laboratories. However, the growing number of deposited structures for proteins central to human biology presents an opportunity to infer the relevant dynamics via exploitation of the information encoded in such structures about equilibrium dynamics. Results: Recent computational efforts using extrinsic modes of motion as variables have successfully reconstructed detailed energy landscapes of several medium-size proteins. Here we investigate the extent to which one can reconstruct the energy landscape of a protein in the absence of sufficient, wet-laboratory structural data. We do so by integrating intrinsic modes of motion extracted off a single structure in a stochastic optimization framework that supports the plug-and-play of different variable selection strategies. We demonstrate that, while knowledge of more wet-laboratory structures yields better-reconstructed landscapes, precious information can be obtained even when only one structural model is available. Conclusions: The presented work shows that it is possible to reconstruct the energy landscape of a protein with reasonable detail and accuracy even when the structural information about the protein is limited to one structure. By attenuating the dependence on structural data of methods designed to compute protein energy landscapes, the work opens up interesting venues of research on structure-based inference of dynamics. Of particular interest are directions of research that will extend such inference to proteins with no experimentally-characterized structures. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136827411&site=ehost-live
573,"Computational Structural Biology: Successes, Future Directions, and Challenges.",Amarda Shehu,Molecules,14203049,,Feb2019,24,3,637,1.0,134795891,10.3390/molecules24030637,MDPI,Article,CHROMATIN; GENOMES; PUBLIC health; BIG data; SUPRAMOLECULAR chemistry; GENETIC mutation; Health and Welfare Funds,big data; bioinformatics; biological modeling; free-energy landscape; machine intelligence; mutations,"Computational biology has made powerful advances. Among these, trends in human health have been uncovered through heterogeneous 'big data' integration, and disease-associated genes were identified and classified. Along a different front, the dynamic organization of chromatin is being elucidated to gain insight into the fundamental question of genome regulation. Powerful conformational sampling methods have also been developed to yield a detailed molecular view of cellular processes. when combining these methods with the advancements in the modeling of supramolecular assemblies, including those at the membrane, we are finally able to get a glimpse into how cells' actions are regulated. Perhaps most intriguingly, a major thrust is on to decipher the mystery of how the brain is coded. Here, we aim to provide a broad, yet concise, sketch of modern aspects of computational biology, with a special focus on computational structural biology. We attempt to forecast the areas that computational structural biology will embrace in the future and the challenges that it may face. We skirt details, highlight successes, note failures, and map directions. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=134795891&site=ehost-live
574,Computing energy landscape maps and structural excursions of proteins.,Amarda Shehu,BMC Genomics,14712164,,8/18/2016,17,,433,24.0,117594572,10.1186/s12864-016-2798-8,BioMed Central,Article,PROTEINS; BIOMOLECULES; ORGANIC compounds; BIOSYNTHESIS; PROTEOMICS; Other basic organic chemical manufacturing; All Other Basic Organic Chemical Manufacturing,Energy landscape map; Evolutionary algorithm; Low-cost paths; Mechanical work; Multi-basin energy landscape; Multi-state protein; Nearest-neighbor graph; Protein equilibrium dynamics; Sample-based representation; Structural excursion,"Background: Structural excursions of a protein at equilibrium are key to biomolecular recognition and function modulation. Protein modeling research is driven by the need to aid wet laboratories in characterizing equilibrium protein dynamics. In principle, structural excursions of a protein can be directly observed via simulation of its dynamics, but the disparate temporal scales involved in such excursions make this approach computationally impractical. On the other hand, an informative representation of the structure space available to a protein at equilibrium can be obtained efficiently via stochastic optimization, but this approach does not directly yield information on equilibrium dynamics. Methods: We present here a novel methodology that first builds a multi-dimensional map of the energy landscape that underlies the structure space of a given protein and then queries the computed map for energetically-feasible excursions between structures of interest. An evolutionary algorithm builds such maps with a practical computational budget. Graphical techniques analyze a computed multi-dimensional map and expose interesting features of an energy landscape, such as basins and barriers. A path searching algorithm then queries a nearest-neighbor graph representation of a computed map for energetically-feasible basin-to-basin excursions. Results: Evaluation is conducted on intrinsically-dynamic proteins of importance in human biology and disease. Visual statistical analysis of the maps of energy landscapes computed by the proposed methodology reveals features already captured in the wet laboratory, as well as new features indicative of interesting, unknown thermodynamically-stable and semi-stable regions of the equilibrium structure space. Comparison of maps and structural excursions computed by the proposed methodology on sequence variants of a protein sheds light on the role of equilibrium structure and dynamics in the sequence-function relationship. Conclusions: Applications show that the proposed methodology is effective at locating basins in complex energy landscapes and computing basin-basin excursions of a protein with a practical computational budget. While the actual temporal scales spanned by a structural excursion cannot be directly obtained due to the foregoing of simulation of dynamics, hypotheses can be formulated regarding the impact of sequence mutations on protein function. These hypotheses are valuable in instigating further research in wet laboratories. [ABSTRACT FROM AUTHOR] Copyright of BMC Genomics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117594572&site=ehost-live
575,Decoy selection for protein structure prediction via extreme gradient boosting and ranking.,Amarda Shehu,BMC Bioinformatics,14712105,,12/9/2020,21,1,1,21.0,147500708,10.1186/s12859-020-3523-9,BioMed Central,Article,"FORECASTING; SPACE probes; MACHINE learning; COMPUTATIONAL biology; PROTEIN structure; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences; Research and Development in Biotechnology",Basins; Decoy selection; Energy landscape; Machine learning; Purity; Ranking; XGBoost,"Background: Identifying one or more biologically-active/native decoys from millions of non-native decoys is one of the major challenges in computational structural biology. The extreme lack of balance in positive and negative samples (native and non-native decoys) in a decoy set makes the problem even more complicated. Consensus methods show varied success in handling the challenge of decoy selection despite some issues associated with clustering large decoy sets and decoy sets that do not show much structural similarity. Recent investigations into energy landscape-based decoy selection approaches show promises. However, lack of generalization over varied test cases remains a bottleneck for these methods. Results: We propose a novel decoy selection method, ML-Select, a machine learning framework that exploits the energy landscape associated with the structure space probed through a template-free decoy generation. The proposed method outperforms both clustering and energy ranking-based methods, all the while consistently offering better performance on varied test-cases. Moreover, ML-Select shows promising results even for the decoy sets consisting of mostly low-quality decoys. Conclusions: ML-Select is a useful method for decoy selection. This work suggests further research in finding more effective ways to adopt machine learning frameworks in achieving robust performance for decoy selection in template-free protein structure prediction. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147500708&site=ehost-live
576,Effective Automated Feature Construction and Selection for Classification of Biological Sequences.,Amarda Shehu,PLoS ONE,19326203,,Jul2014,9,7,1,14.0,97360147,10.1371/journal.pone.0099982,Public Library of Science,Article,"BIOINFORMATICS; NUCLEOTIDE sequence; EXONS (Genetics); GENETIC engineering; MACHINE learning; EVOLUTIONARY algorithms; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and Development in Biotechnology",Algorithms; Applied mathematics; Biochemistry; Biology and life sciences; Biophysics; Cell biology; Computational biology; Computer and information sciences; Computerized simulations; DNA; DNA amplification; DNA modification; DNA transcription; Epigenetics; Gene expression; Genetics; Mathematics; Molecular cell biology; Nucleic acids; Physical sciences; Physics; Research Article,"Background: Many open problems in bioinformatics involve elucidating underlying functional signals in biological sequences. DNA sequences, in particular, are characterized by rich architectures in which functional signals are increasingly found to combine local and distal interactions at the nucleotide level. Problems of interest include detection of regulatory regions, splice sites, exons, hypersensitive sites, and more. These problems naturally lend themselves to formulation as classification problems in machine learning. When classification is based on features extracted from the sequences under investigation, success is critically dependent on the chosen set of features. Methodology: We present an algorithmic framework (EFFECT) for automated detection of functional signals in biological sequences. We focus here on classification problems involving DNA sequences which state-of-the-art work in machine learning shows to be challenging and involve complex combinations of local and distal features. EFFECT uses a two-stage process to first construct a set of candidate sequence-based features and then select a most effective subset for the classification task at hand. Both stages make heavy use of evolutionary algorithms to efficiently guide the search towards informative features capable of discriminating between sequences that contain a particular functional signal and those that do not. Results: To demonstrate its generality, EFFECT is applied to three separate problems of importance in DNA research: the recognition of hypersensitive sites, splice sites, and ALU sites. Comparisons with state-of-the-art algorithms show that the framework is both general and powerful. In addition, a detailed analysis of the constructed features shows that they contain valuable biological information about DNA architecture, allowing biologists and other researchers to directly inspect the features and potentially use the insights obtained to assist wet-laboratory studies on retainment or modification of a specific signal. Code, documentation, and all data for the applications presented here are provided for the community at http://www.cs.gmu.edu/~ashehu/?q=OurTools. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=97360147&site=ehost-live
577,Evaluating Autoencoder-Based Featurization and Supervised Learning for Protein Decoy Selection.,Amarda Shehu,Molecules,14203049,,Mar2020,25,5,1146,1.0,142478940,10.3390/molecules25051146,MDPI,Article,"TERTIARY structure; MORPHOLOGY; PROTEIN structure; MOLECULAR structure; LIBRARY information networks; WEB servers; Data Processing, Hosting, and Related Services",autoencoder; decoy selection; featurization; protein modeling; tertiary structure,"Rapid growth in molecular structure data is renewing interest in featurizing structure. Featurizations that retain information on biological activity are particularly sought for protein molecules, where decades of research have shown that indeed structure encodes function. Research on featurization of protein structure is active, but here we assess the promise of autoencoders. Motivated by rapid progress in neural network research, we investigate and evaluate autoencoders on yielding linear and nonlinear featurizations of protein tertiary structures. An additional reason we focus on autoencoders as the engine to obtain featurizations is the versatility of their architectures and the ease with which changes to architecture yield linear versus nonlinear features. While open-source neural network libraries, such as Keras, which we employ here, greatly facilitate constructing, training, and evaluating autoencoder architectures and conducting model search, autoencoders have not yet gained popularity in the structure biology community. Here we demonstrate their utility in a practical context. Employing autoencoder-based featurizations, we address the classic problem of decoy selection in protein structure prediction. Utilizing off-the-shelf supervised learning methods, we demonstrate that the featurizations are indeed meaningful and allow detecting active tertiary structures, thus opening the way for further avenues of research. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=142478940&site=ehost-live
578,Exploring representations of protein structure for automated remote homology detection and mapping of protein structure space.,Amarda Shehu,BMC Bioinformatics,14712105,,2014,15,Suppl 8,1,14.0,131680689,10.1186/1471-2105-15-S8-S4,BioMed Central,Article,,,"Background: Due to rapid sequencing of genomes, there are now millions of deposited protein sequences with no known function. Fast sequence-based comparisons allow detecting close homologs for a protein of interest to transfer functional information from the homologs to the given protein. Sequence-based comparison cannot detect remote homologs, in which evolution has adjusted the sequence while largely preserving structure. Structure-based comparisons can detect remote homologs but most methods for doing so are too expensive to apply at a large scale over structural databases of proteins. Recently, fragment-based structural representations have been proposed that allow fast detection of remote homologs with reasonable accuracy. These representations have also been used to obtain linearly-reducible maps of protein structure space. It has been shown, as additionally supported from analysis in this paper that such maps preserve functional co-localization of the protein structure space. Methods: Inspired by a recent application of the Latent Dirichlet Allocation (LDA) model for conducting structural comparisons of proteins, we propose higher-order LDA-obtained topic-based representations of protein structures to provide an alternative route for remote homology detection and organization of the protein structure space in few dimensions. Various techniques based on natural language processing are proposed and employed to aid the analysis of topics in the protein structure domain. Results: We show that a topic-based representation is just as effective as a fragment-based one at automated detection of remote homologs and organization of protein structure space. We conduct a detailed analysis of the information content in the topic-based representation, showing that topics have semantic meaning. The fragmentbased and topic-based representations are also shown to allow prediction of superfamily membership. Conclusions: This work opens exciting venues in designing novel representations to extract information about protein structures, as well as organizing and mining protein structure space with mature text mining tools. [ABSTRACT FROM AUTHOR] Copyright of BMC Bioinformatics is the property of BioMed Central and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131680689&site=ehost-live
579,From molecular energy landscapes to equilibrium dynamics via landscape analysis and markov state models.,Amarda Shehu,Journal of Bioinformatics & Computational Biology,02197200,,Dec2019,17,6,N.PAG,21.0,141546481,10.1142/S0219720019400146,World Scientific Publishing Company,Article,MARKOV processes; NANOTECHNOLOGY; STRUCTURAL dynamics; MOLECULAR dynamics; Research and Development in Biotechnology,basins; energy landscape; Equilibrium dynamics; graph embeddings; markov state models; molecular dynamics simulation,"Molecular dynamics (MD) simulation software allows probing the equilibrium structural dynamics of a molecule of interest, revealing how a molecule navigates its structure space one structure at a time. To obtain a broader view of dynamics, typically one needs to launch many such simulations, obtaining many trajectories. A summarization of the equilibrium dynamics requires integrating the information in the various trajectories, and Markov State Models (MSM) are increasingly being used for this task. At its core, the task involves organizing the structures accessed in simulation into structural states, and then constructing a transition probability matrix revealing the transitions between states. While now considered a mature technology and widely used to summarize equilibrium dynamics, the underlying computational process in the construction of an MSM ignores energetics even though the transition of a molecule between two nearby structures in an MD trajectory is governed by the corresponding energies. In this paper, we connect theory with simulation and analysis of equilibrium dynamics. A molecule navigates the energy landscape underlying the structure space. The structural states that are identified via off-the-shelf clustering algorithms need to be connected to thermodynamically-stable and semi-stable (macro)states among which transitions can then be quantified. Leveraging recent developments in the analysis of energy landscapes that identify basins in the landscape, we evaluate the hypothesis that basins, directly tied to stable and semi-stable states, lead to better models of dynamics. Our analysis indicates that basins lead to MSMs of better quality and thus can be useful to further advance this widely-used technology for summarization of molecular equilibrium dynamics. [ABSTRACT FROM AUTHOR] Copyright of Journal of Bioinformatics & Computational Biology is the property of World Scientific Publishing Company and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=141546481&site=ehost-live
580,Generative Adversarial Learning of Protein Tertiary Structures.,Amarda Shehu,Molecules,14203049,,Mar2021,26,5,1209,1.0,149272646,10.3390/molecules26051209,MDPI,Article,,deep learning; generative adversarial learning; protein modeling; tertiary structure,"Protein molecules are inherently dynamic and modulate their interactions with different molecular partners by accessing different tertiary structures under physiological conditions. Elucidating such structures remains challenging. Current momentum in deep learning and the powerful performance of generative adversarial networks (GANs) in complex domains, such as computer vision, inspires us to investigate GANs on their ability to generate physically-realistic protein tertiary structures. The analysis presented here shows that several GAN models fail to capture complex, distal structural patterns present in protein tertiary structures. The study additionally reveals that mechanisms touted as effective in stabilizing the training of a GAN model are not all effective, and that performance based on loss alone may be orthogonal to performance based on the quality of generated datasets. A novel contribution in this study is the demonstration that Wasserstein GAN strikes a good balance and manages to capture both local and distal patterns, thus presenting a first step towards more powerful deep generative models for exploring a possibly very diverse set of structures supporting diverse activities of a protein molecule in the cell. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149272646&site=ehost-live
582,"Mapping the Conformation Space of Wildtype and Mutant H-Ras with a Memetic, Cellular, and Multiscale Evolutionary Algorithm.",Amarda Shehu,PLoS Computational Biology,1553734X,,9/1/2015,11,9,1,26.0,109205244,10.1371/journal.pcbi.1004470,Public Library of Science,Article,MOLECULAR biology; EVOLUTIONARY algorithms; PROTEIN structure; CONFORMATIONAL analysis; MEMETICS; GENETIC mutation,Research Article,"An important goal in molecular biology is to understand functional changes upon single-point mutations in proteins. Doing so through a detailed characterization of structure spaces and underlying energy landscapes is desirable but continues to challenge methods based on Molecular Dynamics. In this paper we propose a novel algorithm, SIfTER, which is based instead on stochastic optimization to circumvent the computational challenge of exploring the breadth of a protein’s structure space. SIfTER is a data-driven evolutionary algorithm, leveraging experimentally-available structures of wildtype and variant sequences of a protein to define a reduced search space from where to efficiently draw samples corresponding to novel structures not directly observed in the wet laboratory. The main advantage of SIfTER is its ability to rapidly generate conformational ensembles, thus allowing mapping and juxtaposing landscapes of variant sequences and relating observed differences to functional changes. We apply SIfTER to variant sequences of the H-Ras catalytic domain, due to the prominent role of the Ras protein in signaling pathways that control cell proliferation, its well-studied conformational switching, and abundance of documented mutations in several human tumors. Many Ras mutations are oncogenic, but detailed energy landscapes have not been reported until now. Analysis of SIfTER-computed energy landscapes for the wildtype and two oncogenic variants, G12V and Q61L, suggests that these mutations cause constitutive activation through two different mechanisms. G12V directly affects binding specificity while leaving the energy landscape largely unchanged, whereas Q61L has pronounced, starker effects on the landscape. An implementation of SIfTER is made available at . We believe SIfTER is useful to the community to answer the question of how sequence mutations affect the function of a protein, when there is an abundance of experimental structures that can be exploited to reconstruct an energy landscape that would be computationally impractical to do via Molecular Dynamics. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109205244&site=ehost-live
583,Menthol Binding and Inhibition of α7-Nicotinic Acetylcholine Receptors.,Amarda Shehu,PLoS ONE,19326203,,Jul2013,8,7,1,12.0,89626840,10.1371/journal.pone.0067674,Public Library of Science,Article,MENTHOL; NICOTINIC acetylcholine receptors; CIGARETTES; BIOCHEMISTRY; XENOPUS laevis; NEUROSCIENCES; BRAIN imaging; ANIMAL models in research; Tobacco and Tobacco Product Merchant Wholesalers; All other miscellaneous store retailers (except beer and wine-making supplies stores); Cigarette and tobacco product merchant wholesalers; Tobacco Manufacturing; Tobacco Stores; Tobacco product manufacturing,Animal models; Biochemistry; Biology; Calcium imaging; Chemistry; Drugs and devices; Medicine; Methanol; Model organisms; Neuroimaging; Neuropharmacology; Neuroscience; Nicotinic acetylcholine receptors; Organic chemistry; Organic compounds; Proteins; Research Article; Terpenes; Xenopus laevis,"Menthol is a common compound in pharmaceutical and commercial products and a popular additive to cigarettes. The molecular targets of menthol remain poorly defined. In this study we show an effect of menthol on the α7 subunit of the nicotinic acetylcholine (nACh) receptor function. Using a two-electrode voltage-clamp technique, menthol was found to reversibly inhibit α7-nACh receptors heterologously expressed in Xenopus oocytes. Inhibition by menthol was not dependent on the membrane potential and did not involve endogenous Ca2+-dependent Cl− channels, since menthol inhibition remained unchanged by intracellular injection of the Ca2+ chelator BAPTA and perfusion with Ca2+-free bathing solution containing Ba2+. Furthermore, increasing ACh concentrations did not reverse menthol inhibition and the specific binding of [125I] α-bungarotoxin was not attenuated by menthol. Studies of α7- nACh receptors endogenously expressed in neural cells demonstrate that menthol attenuates α7 mediated Ca2+ transients in the cell body and neurite. In conclusion, our results suggest that menthol inhibits α7-nACh receptors in a noncompetitive manner. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=89626840&site=ehost-live
584,Principles and Overview of Sampling Methods for Modeling Macromolecular Structure and Dynamics.,Amarda Shehu,PLoS Computational Biology,1553734X,,4/28/2016,12,4,1,70.0,114917157,10.1371/journal.pcbi.1004619,Public Library of Science,Article,MACROMOLECULAR dynamics; COMPUTATIONAL mechanics; NUCLEIC acid isolation methods; FUNCTIONAL genomics; X-ray crystallography; NUCLEAR magnetic resonance,Algorithms; Applied mathematics; Biochemical simulations; Biochemistry; Biology and life sciences; Biophysical simulations; Biophysics; Chemistry; Computational biology; Free energy; Macromolecular structure analysis; Macromolecules; Mathematics; Molecular biology; Physical sciences; Physics; Polymer chemistry; Protein structure; Protein structure prediction; Proteins; Research and analysis methods; Review; Simulation and modeling; Thermodynamics,"Investigation of macromolecular structure and dynamics is fundamental to understanding how macromolecules carry out their functions in the cell. Significant advances have been made toward this end in silico, with a growing number of computational methods proposed yearly to study and simulate various aspects of macromolecular structure and dynamics. This review aims to provide an overview of recent advances, focusing primarily on methods proposed for exploring the structure space of macromolecules in isolation and in assemblies for the purpose of characterizing equilibrium structure and dynamics. In addition to surveying recent applications that showcase current capabilities of computational methods, this review highlights state-of-the-art algorithmic techniques proposed to overcome challenges posed in silico by the disparate spatial and time scales accessed by dynamic macromolecules. This review is not meant to be exhaustive, as such an endeavor is impossible, but rather aims to balance breadth and depth of strategies for modeling macromolecular structure and dynamics for a broad audience of novices and experts. [ABSTRACT FROM AUTHOR] Copyright of PLoS Computational Biology is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=114917157&site=ehost-live
585,Restriction versus guidance in protein structure prediction.,Amarda Shehu,Proceedings of the National Academy of Sciences of the United States of America,00278424,,9/8/2009,106,36,15302,6.0,44288999,10.1073/pnas.0907002106,National Academy of Sciences,Article,CONFORMATIONAL analysis; PROTEIN conformation; HAMILTONIAN systems; MOLECULE-molecule collisions; MOLECULAR dynamics; FORCE & energy,annealing; associative memory Hamiltonian; fragment assembly; molecular dynamics; protein folding,"Conformational restriction by fragment assembly and guidance in molecular dynamics are alternate conformational search strategies in protein structure prediction. We examine both approaches using a version of the associative memory Hamiltonian that incorporates the influence of water-mediated interactions (AMW). For short proteins (<70 residues), fragment assembly, while searching a restricted space, compares well to molecular dynamics and is often sufficient to fold such proteins to near-native conformations (4A) via simulated annealing. Longer proteins encounter kinetic sampling limitations in fragment assembly not seen in molecular dynamics which generally samples more native-like conformations. We also present a fragment enriched version of the standard AMW energy function, AMW-FME, which incorporates the local sequence alignment derived fragment libraries from fragment assembly directly into the energy function. This energy function, in which fragment information acts as a guide not a restriction, is found by molecular dynamics to improve on both previous approaches. [ABSTRACT FROM AUTHOR] Copyright of Proceedings of the National Academy of Sciences of the United States of America is the property of National Academy of Sciences and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=44288999&site=ehost-live
586,Sample-Based Models of Protein Energy Landscapes and Slow Structural Rearrangements.,Amarda Shehu,Journal of Computational Biology,10665277,,Jan2018,25,1,33,18.0,127185401,10.1089/cmb.2017.0158,"Mary Ann Liebert, Inc.",Article,PROTEIN structure; STRUCTURAL dynamics; PROTEIN models; SIMULATION methods & models; ALGORITHMS,energy landscape; protein modeling; sample-based model; sampling capability; structural rearrangements,"Proteins often undergo slow structural rearrangements that involve several angstroms and surpass the nanosecond timescale. These spatiotemporal scales challenge physics-based simulations and open the way to sample-based models of structural dynamics. This article improves an understanding of current capabilities and limitations of sample-based models of dynamics. Borrowing from widely used concepts in evolutionary computation, this article introduces two conflicting aspects of sampling capability and quantifies them via statistical (and graphical) analysis tools. This allows not only conducting a principled comparison of different sample-based algorithms but also understanding which algorithmic ingredients to use as knobs via which to control sampling and, in turn, the accuracy and detail of modeled structural rearrangements. We demonstrate the latter by proposing two powerful variants of a recently published sample-based algorithm. We believe that this work will advance the adoption of sample-based models as reliable tools for modeling slow protein structural rearrangements. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational Biology is the property of Mary Ann Liebert, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127185401&site=ehost-live
587,The 7th Computational Structural Bioinformatics Workshop.,Amarda Shehu,Journal of Computational Biology,10665277,,Sep2015,22,9,785,2.0,109251701,10.1089/cmb.2015.28999.jh,"Mary Ann Liebert, Inc.",Article,MOLECULAR docking; CONFORMATIONAL analysis; PROTEINS,,"An introduction is presented in which the editor discusses various reports within the issue on topics including evaluation of decoy databases to improve its quality, exploring conformational space of proteins by introducing crystallographic structures and, an algorithm for protein docking.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109251701&site=ehost-live
588,Are nicotinic acetylcholine receptors coupled to G proteins?,Amarda Shehu,BioEssays,02659247,,Dec2013,35,12,1025,10.0,92005592,10.1002/bies.201300082,"John Wiley & Sons, Inc.",Article,NICOTINIC acetylcholine receptors; G proteins; PROTEOMICS; ION channels; CONSERVED sequences (Genetics); CELLULAR signal transduction,acetylcholine; G protein coupling; intracellular loop; ligand‐gated ion channel; ligand-gated ion channel; loop modeling; protein interaction; signal transduction,"It was, until recently, accepted that the two classes of acetylcholine (ACh) receptors are distinct in an important sense: muscarinic ACh receptors signal via heterotrimeric GTP binding proteins (G proteins), whereas nicotinic ACh receptors (nAChRs) open to allow flux of Na+, Ca2+, and K+ ions into the cell after activation. Here we present evidence of direct coupling between G proteins and nAChRs in neurons. Based on proteomic, biophysical, and functional evidence, we hypothesize that binding to G proteins modulates the activity and signaling of nAChRs in cells. It is important to note that while this hypothesis is new for the nAChR, it is consistent with known interactions between G proteins and structurally related ligand-gated ion channels. Therefore, it underscores an evolutionarily conserved metabotropic mechanism of G protein signaling via nAChR channels. Also watch the Video Abstract. [ABSTRACT FROM AUTHOR] Copyright of BioEssays is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=92005592&site=ehost-live
589,Foreword on special issue on robotics methods for structural and dynamic modeling of molecular systems.,Amarda Shehu,Robotica,02635747,,Aug2016,34,8,1677,2.0,116719278,10.1017/S0263574716000370,Cambridge University Press,Article,ROBOTICS; MOLECULAR biology; ROBOT motion; EVOLUTIONARY computation; CONSTRAINT programming; DATA mining,,"Molecular biological systems can be seen as extremely complex mobile systems. The development of methods for modeling the structure and the motion of such systems is essential to better understand their physiochemical properties and biological functions. In recent years, many computer scientists in robotics and artificial intelligence have made significant contributions to modeling biological systems. Research expertise in planning, search, learning, evolutionary computation, constraint programming, and data mining is being used to make great progress on molecular motion, structure prediction, and design. [ABSTRACT FROM PUBLISHER] Copyright of Robotica is the property of Cambridge University Press and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=116719278&site=ehost-live
590,Graph-Based Community Detection for Decoy Selection in Template-Free Protein Structure Prediction.,Amarda Shehu,Molecules,14203049,,Mar2019,24,5,854,1.0,135380903,10.3390/molecules24050854,MDPI,Article,PROTEIN structure; TERTIARY structure; PROTEIN analysis; AMINO acid sequence; PROTEIN folding,community detection; decoy selection; nearest-neighbor graph; protein structure space; template-free protein structure prediction,"Significant efforts in wet and dry laboratories are devoted to resolving molecular structures. In particular, computational methods can now compute thousands of tertiary structures that populate the structure space of a protein molecule of interest. These advances are now allowing us to turn our attention to analysis methodologies that are able to organize the computed structures in order to highlight functionally relevant structural states. In this paper, we propose a methodology that leverages community detection methods, designed originally to detect communities in social networks, to organize computationally probed protein structure spaces. We report a principled comparison of such methods along several metrics on proteins of diverse folds and lengths. We present a rigorous evaluation in the context of decoy selection in template-free protein structure prediction. The results make the case that network-based community detection methods warrant further investigation to advance analysis of protein structure spaces for automated selection of functionally relevant structures. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=135380903&site=ehost-live
592,Modeling the Tertiary Structure of the Rift Valley Fever Virus L Protein.,Amarda Shehu,Molecules,14203049,,May2019,24,9,1768,1.0,136449552,10.3390/molecules24091768,MDPI,Article,RIFT Valley fever; GENE expression; MOLECULAR dynamics; AMINO acids; GENOMES,computational structure determination; multidomain protein; Rift Valley fever virus; tertiary structure,"A tertiary structure governs, to a great extent, the biological activity of a protein in the living cell and is consequently a central focus of numerous studies aiming to shed light on cellular processes central to human health. Here, we aim to elucidate the structure of the Rift Valley fever virus (RVFV) L protein using a combination of in silico techniques. Due to its large size and multiple domains, elucidation of the tertiary structure of the L protein has so far challenged both dry and wet laboratories. In this work, we leverage complementary perspectives and tools from the computational-molecular-biology and bioinformatics domains for constructing, refining, and evaluating several atomistic structural models of the L protein that are physically realistic. All computed models have very flexible termini of about 200 amino acids each, and a high proportion of helical regions. Properties such as potential energy, radius of gyration, hydrodynamics radius, flexibility coefficient, and solvent-accessible surface are reported. Structural characterization of the L protein enables our laboratories to better understand viral replication and transcription via further studies of L protein-mediated protein–protein interactions. While results presented a focus on the RVFV L protein, the following workflow is a more general modeling protocol for discovering the tertiary structure of multidomain proteins consisting of thousands of amino acids. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136449552&site=ehost-live
593,Unsupervised and Supervised Learning over the Energy Landscape for Protein Decoy Selection.,Amarda Shehu,Biomolecules (2218-273X),2218273X,,Oct2019,9,10,607,1.0,139415532,10.3390/biom9100607,MDPI,Article,"STRUCTURAL dynamics; MOLECULAR dynamics; PROTEIN structure; COMPUTATIONAL biology; MOLECULAR structure; LANDSCAPE assessment; SYSTEMS biology; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Research and development in the physical, engineering and life sciences; Research and Development in Biotechnology",basin; decoy selection; energy landscape; machine learning; model quality assessment; purity,"The energy landscape that organizes microstates of a molecular system and governs the underlying molecular dynamics exposes the relationship between molecular form/structure, changes to form, and biological activity or function in the cell. However, several challenges stand in the way of leveraging energy landscapes for relating structure and structural dynamics to function. Energy landscapes are high-dimensional, multi-modal, and often overly-rugged. Deep wells or basins in them do not always correspond to stable structural states but are instead the result of inherent inaccuracies in semi-empirical molecular energy functions. Due to these challenges, energetics is typically ignored in computational approaches addressing long-standing central questions in computational biology, such as protein decoy selection. In the latter, the goal is to determine over a possibly large number of computationally-generated three-dimensional structures of a protein those structures that are biologically-active/native. In recent work, we have recast our attention on the protein energy landscape and its role in helping us to advance decoy selection. Here, we summarize some of our successes so far in this direction via unsupervised learning. More importantly, we further advance the argument that the energy landscape holds valuable information to aid and advance the state of protein decoy selection via novel machine learning methodologies that leverage supervised learning. Our focus in this article is on decoy selection for the purpose of a rigorous, quantitative evaluation of how leveraging protein energy landscapes advances an important problem in protein modeling. However, the ideas and concepts presented here are generally useful to make discoveries in studies aiming to relate molecular structure and structural dynamics to function. [ABSTRACT FROM AUTHOR] Copyright of Biomolecules (2218-273X) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=139415532&site=ehost-live
594,Analysis of performance and equity in ground delay programs,Lance Sherry,Transportation Research: Part C,0968090X,,Dec2010,18,6,910,11.0,53303750,10.1016/j.trc.2010.03.009,Elsevier B.V.,Article,FLIGHT delays & cancellations (Airlines); AIRPORTS; RATIONING; CONJOINT analysis; AIR travelers; STAKEHOLDERS; AIRPLANE fuel consumption; AIRLINE industry; Scheduled air transportation; Scheduled Passenger Air Transportation; Other Airport Operations,Equity; Fuel burn; Ground delay program; Passenger delay; Performance; Utility,"Abstract: The discrepancy between the projected demand for arrival slots at an airport and the projected available arrival slots on a given day is resolved by the Ground Delay Program (GDP). The current GDP rationing rule, Ration-by-Schedule, allocates the available arrival slots at the affected airport by scheduled arrival time of the flights with some adjustments to balance the equity between airlines. This rule does not take into account passenger flow and fuel flow performance in the rationing assignment tradeoff. This paper examines the trade-off between passenger delays and excess surface fuel burn as well as airline equity and passenger equity in GDP slot allocation using different rationing rules. A GDP Rationing Rule Simulator (GDP-RRS) is developed to calculate performance and equity metrics for all stakeholders using six alternate rules. The results show that there is a trade-off between GDP performance and GDP equity. Ration-by-Passengers (a rule which maximizes the passenger throughput) decreased total passenger delay by 22% and decreased total excess fuel burn by 57% with no change in total flight delay compared to the traditional Ration-by-Schedule. However, when the airline and passenger equity are primary concerns, the Ration-by-Schedule is preferred. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53303750&site=ehost-live
595,Automation for task analysis of next generation air traffic management systems,Lance Sherry,Transportation Research: Part C,0968090X,,Dec2010,18,6,921,9.0,53303751,10.1016/j.trc.2010.03.006,Elsevier B.V.,Article,AIR traffic control; AUTOMATION; TASK analysis; ERROR rates; JOB analysis; USER-centered system design; HUMAN-computer interaction; INDUSTRIAL engineering; Air Traffic Control,Human–computer interaction; Probability of failure-to-complete a task; Task analysis; Trials-to-mastery; Usability analysis,"Abstract: The increasing span of control of Air Traffic Control enterprise automation (e.g. Flight Schedule Monitor, Departure Flow Management), along with lean-processes and pay-for-performance business models, has placed increased emphasis on operator training time and error rates. There are two traditional approaches to the design of human–computer interaction (HCI) to minimize training time and reduce error rates: (1) experimental user testing provides the most accurate assessment of training time and error rates, but occurs too late in the development cycle and is cost prohibitive, (2) manual review methods (e.g. cognitive walkthrough) can be used earlier in the development cycle, but suffer from poor accuracy and poor inter-rater reliability. Recent development of “affordable” human performance models provide the basis for the automation of task analysis and HCI design to obtain low cost, accurate, estimates of training time and error rates early in the development cycle. This paper describes a usability/HCI analysis tool that this intended for use by design engineers in the course of their software engineering duties. The tool computes estimates of trials-to-mastery (i.e. time to competence for training) and the probability of failure-to-complete for each task. The HCI required to complete a task on the automation under development is entered into the web-based tool via a form. Assessments of the salience of visual cues to prompt operator actions for the proposed design are used to compute training time and error rates. The web-based tool enables designers in multiple locations to review and contribute to the design. An example analysis is provided along with a discussion of the limitations of the tool and directions for future research. [ABSTRACT FROM AUTHOR] Copyright of Transportation Research: Part C is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53303751&site=ehost-live
596,Improving the accuracy of airport emissions inventories using disparate datasets.,Lance Sherry,IIE Transactions,0740817X,,Jun2015,47,6,577,9.0,101893084,10.1080/0740817X.2014.938845,Taylor & Francis Ltd,Article,AIRPORTS; DATA analysis; ENVIRONMENTAL regulations; AIR quality; CARBON oxides; Other Airport Operations; Administration of Air and Water Resource and Solid Waste Management Programs,aircraft performance models; Airport emissions inventories; radar surveillance track data; takeoff thrust setting,"Environmental regulations require airports to report air quality emissions inventories (i.e., tons emitted) for aircraft emissions such as carbon oxides (COx) and nitrogen oxides (NOx). Traditional methods for emission inventory calculations yield over-estimated inventories due an assumption of the use of maximum takeoff thrust settings for all departures. To reduce costs, airlines use “reduced” thrust settings (such as derated or flex temperature thrust settings) that can be up to 25% lower than the maximum takeoff thrust setting. Thrust data for each flight operation are not readily available to those responsible for the emission inventory. This article describes an approach to estimate the actual takeoff thrust for each flight operation using algorithms that combine radar surveillance track data, weather data, and standardized aircraft performance models. A case study for flights from Chicago's O’Hare airport exhibited an average takeoff thrust of 86% of maximum takeoff thrust (within 4% of the average for actual takeoff thrust settings). The implications and limitations of this method are discussed. [ABSTRACT FROM PUBLISHER] Copyright of IIE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101893084&site=ehost-live
597,Methodology and Case Study for Validation of Aircraft-Induced Clouds from Hyperspectral Imagery.,Lance Sherry,Atmosphere,20734433,,Aug2022,13,8,N.PAG,11.0,158731404,10.3390/atmos13081257,MDPI,Article,WEATHER; JET engines; REMOTE-sensing images; MATCHED filters; TERRESTRIAL radiation; WEST Virginia,aviation; climate change; contrails; in-scene spectra; remote sensing,"Aircraft-Induced Clouds (AICs), colloquially called contrails, form from the emission of soot from jet engines during cruise flight in favorable atmospheric conditions. AICs absorb, scatter, and reflect shortwave and longwave radiation. This radiative transfer has a cooling effect during the day; however, the night experiences an overwhelming warming effect, which leads to an overall warming effect on Earth, contributing to anthropogenically propelled climate change. Reducing AICs significantly mitigates aviation's contribution to climate change by reducing the disruption in Earth's radiation budget. Researchers have proposed AIC Abatement Programs (AAPs) to increase cruise flight levels without additional fuel burn. In order to effectively implement AAPs, it is crucial to be able to accurately identify AICs from publicly available aerial and satellite imagery. This study aims at the identification of AICs from hyperspectral imagery to help the effective implementation of an AAP and to mitigate climate change. This paper describes a method for the hyperspectral analysis of aerial images in order to accurately identify AICs through a case study based in West Virginia. The results show that both the Adaptive Coherence Estimator and the Matched Filter algorithms based on unique in-scene spectra were successful in the isolation of the AICs from other cloud types and the background. It is found that AICs can be identified with 84% confidence in this case study. The method, a case study, and future works are provided. [ABSTRACT FROM AUTHOR] Copyright of Atmosphere is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158731404&site=ehost-live
598,Methodology for collision risk assessment of an airspace flow corridor concept.,John Shortle,Reliability Engineering & System Safety,09518320,,Oct2015,142,,444,12.0,108552086,10.1016/j.ress.2015.05.015,Elsevier B.V.,Article,TRAFFIC accidents; RISK assessment; PARAMETER estimation; UNITED States. Federal Aviation Administration; INTERNATIONAL Civil Aviation Organization; Regulation and Administration of Transportation Programs,ADS-B Automatic Dependent Surveillance Broadcast; AFM Autonomous Flight Management; Air traffic management; Autonomous Flight Management; CD&R conflict detection and resolution; Dynamic event tree; FAA Federal Aviation Administration; Fault tree; Flow corridor concept; ICAO international civil aviation organization; LOL loss of locatability; Monte Carlo simulation; NextGen Next Generation Air Transportation System; NMAC Near Mid-air Collision; RNP required navigation performance; SICDR Strategic Intent-based CD&R Function; TCAS Traffic Collision Avoidance System; TICDR tactical intent-based CD&R function; TIS-B Traffic Information Service Broadcast; TSCDR tactical state-based CD&R function,"This paper presents a methodology to estimate the collision risk associated with a future air-transportation concept called the flow corridor . This concept is designed to reduce congestion and increase throughput in en-route airspace by creating dedicated flight corridors across the continent. The methodology is a hybrid collision-risk methodology combining Monte Carlo simulation and dynamic event trees. Monte Carlo simulation is used to model the movement of aircraft within the corridor and to identify potential trajectories that might lead to a collision. Dynamic event trees are used to evaluate the effectiveness of subsequent safety layers that protect against collisions. The overall risk assessment captures the unique characteristics of the flow corridor concept, including self-separation within the corridor, lane change maneuvers, speed adjustments, and the automated separation assurance system. A tradeoff between safety and throughput is conducted, and a sensitivity analysis identifies the most critical parameters in the model. [ABSTRACT FROM AUTHOR] Copyright of Reliability Engineering & System Safety is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108552086&site=ehost-live
598,Methodology for collision risk assessment of an airspace flow corridor concept.,Lance Sherry,Reliability Engineering & System Safety,09518320,,Oct2015,142,,444,12.0,108552086,10.1016/j.ress.2015.05.015,Elsevier B.V.,Article,TRAFFIC accidents; RISK assessment; PARAMETER estimation; UNITED States. Federal Aviation Administration; INTERNATIONAL Civil Aviation Organization; Regulation and Administration of Transportation Programs,ADS-B Automatic Dependent Surveillance Broadcast; AFM Autonomous Flight Management; Air traffic management; Autonomous Flight Management; CD&R conflict detection and resolution; Dynamic event tree; FAA Federal Aviation Administration; Fault tree; Flow corridor concept; ICAO international civil aviation organization; LOL loss of locatability; Monte Carlo simulation; NextGen Next Generation Air Transportation System; NMAC Near Mid-air Collision; RNP required navigation performance; SICDR Strategic Intent-based CD&R Function; TCAS Traffic Collision Avoidance System; TICDR tactical intent-based CD&R function; TIS-B Traffic Information Service Broadcast; TSCDR tactical state-based CD&R function,"This paper presents a methodology to estimate the collision risk associated with a future air-transportation concept called the flow corridor . This concept is designed to reduce congestion and increase throughput in en-route airspace by creating dedicated flight corridors across the continent. The methodology is a hybrid collision-risk methodology combining Monte Carlo simulation and dynamic event trees. Monte Carlo simulation is used to model the movement of aircraft within the corridor and to identify potential trajectories that might lead to a collision. Dynamic event trees are used to evaluate the effectiveness of subsequent safety layers that protect against collisions. The overall risk assessment captures the unique characteristics of the flow corridor concept, including self-separation within the corridor, lane change maneuvers, speed adjustments, and the automated separation assurance system. A tradeoff between safety and throughput is conducted, and a sensitivity analysis identifies the most critical parameters in the model. [ABSTRACT FROM AUTHOR] Copyright of Reliability Engineering & System Safety is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=108552086&site=ehost-live
599,Troublesome trends in U. S. air transportation.,Lance Sherry,Aerospace America,0740722X,,Nov2012,50,10,20,5.0,83301866,,American Institute of Aeronautics & Astronautics,Article,COMMERCIAL aeronautics; TRAVELERS; AIRLINE rates; TECHNOLOGICAL innovations; UNITED States; Nonscheduled Chartered Passenger Air Transportation; Non-scheduled specialty flying services; Scheduled Freight Air Transportation; Scheduled Passenger Air Transportation; Scheduled air transportation; All Other Traveler Accommodation; UNITED States economy,,"The article reports that trends in the air transportation industry are changing its role as a prime mover of the U.S. economy. Studies showed that since the 1990s, U.S. domestic airlines are serving fewer, higher paying passengers. Increased airfare and reduced accessibility lead to higher transportation costs for businesses, affecting expansion and productivity. To maintain affordable levels, the authors recommend a two pronged approach, namely regulatory incentives and technical innovation.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=83301866&site=ehost-live
600,Resilient Consensus in Robot Swarms With Periodic Motion and Intermittent Communication.,Daigo Shishika,IEEE Transactions on Robotics,15523098,,Feb2022,38,1,110,16.0,155186448,10.1109/TRO.2021.3088765,IEEE,Article,PERIODIC motion; ROBOTS; CONSENSUS (Social sciences); MULTIAGENT systems; TASK analysis,Communication networks; Consensus; multiagent systems; Network topology; resilience systems; Robots; Robustness; Shape; Task analysis; Topology,"In this article, we propose an approach to construct a time-varying communication topology with a resilient consensus performance for robot swarms with limited communication ranges. The robots are deployed to explore a large task space and achieve consensus despite the existence of a finite number of noncooperative members in the team. Existing methods encouraged robots to stay close to each other to achieve certain robustness requirements on the connectivity of the communication topology. We leverage on the robots’ mobility to design a time-varying integrated topology composed of several subgroups of robots deployed on nonoverlapping closed-loop paths. Robots are spread out and move along the paths, forming periodic communication links within or across groups. We analyze the time-varying topology synthesized and provide sufficient conditions for individual subgroups and the interconnection between them. We show designs satisfying the conditions with simulated examples in a lattice space, as well as in a task space with predefined paths. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Robotics is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155186448&site=ehost-live
605,Safety Comparison of Centralized and Distributed Aircraft Separation Assurance Concepts.,John Shortle,IEEE Transactions on Reliability,00189529,,Mar2014,63,1,259,11.0,94763939,10.1109/TR.2014.2299193,IEEE,Article,AIRCRAFT separation; TRAFFIC safety; PROPULSION systems; AERONAUTICAL safety measures; DECENTRALIZATION in management; PROBABILITY theory,Aircraft; Aircraft propulsion; Atmospheric modeling; Aviation safety; case study; centralized versus distributed; common cause failure; near mid-air collisions; Numerical models; Probability; Safety; separation assurance; Trajectory,"This paper presents several models to compare centralized and distributed automated separation assurance concepts in aviation. In a centralized system, safety-related functions are implemented by common equipment on the ground. In a distributed system, safety-related functions are implemented by equipment on each aircraft. Failures of the safety-related functions can increase the risk of near mid-air collisions. Intuitively, failures on the ground are worse than failures in the air because the ground failures simultaneously affect multiple aircraft. This paper evaluates the degree to which this belief is true. Using region-wide models to account for dependencies between aircraft pairs, we derive the region-wide expectation and variance of the number of separation losses for both centralized and distributed concepts. This derivation is done first for a basic scenario involving a single component and function. We show that the variance of the number of separation losses is always higher for the centralized system, holding the expectations equal. However, numerical examples show that the difference is negligible when the events of interest are rare. Results are extended to a hybrid centralized-distributed scenario involving multiple components and functions on the ground and in the air. In this case, the variance of the centralized system may actually be less than that of the distributed system. The overall implication is that the common-cause failure of the ground function does not seriously weaken the overall case for using a centralized concept versus a distributed concept. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Reliability is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94763939&site=ehost-live
607,Transmission-Capacity Expansion for Minimizing Blackout Probabilities.,John Shortle,IEEE Transactions on Power Systems,08858950,,Jan2014,29,1,43,10.0,93281036,10.1109/TPWRS.2013.2279508,IEEE,Article,ELECTRIC power distribution grids; ELECTRIC power failures; MATHEMATICAL optimization; SIMULATION methods & models; MONTE Carlo method,Cascading blackouts; Computational modeling; Linear programming; Load modeling; Mathematical model; Optimization; Power system faults; Power system protection; rare-event simulation; simulation optimization; splitting; transmission expansion,"The objective of this paper is to determine an optimal plan for expanding the capacity of a power grid in order to minimize the likelihood of a large cascading blackout. Capacity-expansion decisions considered in this paper include the addition of new transmission lines and the addition of capacity to existing lines. We embody these interacting considerations in a simulation optimization model, where the objective is to minimize the probability of a large blackout subject to a budget constraint. The probability of a large-scale blackout is estimated via Monte Carlo simulation of a probabilistic cascading blackout model. Because the events of interest are rare, standard simulation is often intractable from a computational perspective. We apply a variance-reduction technique within the simulation to provide results in a reasonable time frame. Numerical results are given for some small test networks including an IEEE 14-bus test network. A key conclusion is that the different expansion strategies lead to different shapes of the tails of the blackout distributions. In other words, there is a tradeoff between reducing the frequency of small-scale blackouts versus reducing the frequency of large-scale blackouts. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Power Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=93281036&site=ehost-live
609,Biomechanics of Walking in Healthy Adults at Different Gait Speeds: 121 Board #2 May 30 9:30 AM - 11:30 AM.,Siddhartha Sikdar,Medicine & Science in Sports & Exercise,01959131,,2018 Supplement 1,50,,10,1.0,133521716,10.1249/01.mss.0000535118.61829.e2,Lippincott Williams & Wilkins,Abstract,CONFERENCES & conventions; BIOMECHANICS; WALKING; WALKING speed; ADULTS; MINNESOTA; AMERICAN College of Sports Medicine; Convention and Trade Show Organizers,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133521716&site=ehost-live
610,Knee joint angular velocities and accelerations during the patellar tendon jerk,Siddhartha Sikdar,Journal of Neuroscience Methods,01650270,,Jun2011,198,2,255,5.0,61172286,10.1016/j.jneumeth.2011.04.018,Elsevier B.V.,Article,KNEE; TENDON reflex; STRETCH reflex; ACCELERATION (Mechanics); MAGNETIC resonance imaging; ACTIVITIES of daily living; Diagnostic Imaging Centers,"activities of daily living ( ADL ); coefficients of linear function y(x) with equation y =Ax +B ( A, B ); estimated angular knee extension acceleration due to tendon tap (rads−2) ( α ); estimated angular knee extension velocity due to tendon tap (rads−1) ( ω ); first component of stretch reflex ( M1 ); Human; Imaging; magnetic resonance imaging ( MRI ); maximum knee angular velocity during passive knee flexion, following reflexive knee extension caused by TJ (rads−1) ( ω p ); maximum longitudinal elongation acceleration during tendon tap (cms−2) ( a ); maximum longitudinal elongation velocity of the muscle after TJ (cms−1) ( v el ); maximum longitudinal elongation velocity of the muscle during passive knee flexion following reflexive knee extension caused by TJ (cms−1) ( v p ); Monosynaptic reflex; number of subjects in the study ( N ); Rectus femoris; Stretch reflex; stretch reflex ( SR ); tendon jerk, patellar tendon tap ( TJ ); Ultrasound; vector tissue Doppler imaging ( TDI )","Abstract: Tendon jerk (TJ) is one of the most commonly used clinical tests in differential diagnosis of human motor disorders. There remains some ambiguity in the physiological interpretation of the test, especially with respect to its association to the functional status of patients. The TJ test inputs a non-physiological stimuli, but it is unclear to what degree the kinematics generated during the TJ test exceed the ranges that muscles encounter in activities of daily living (ADLs). The aim of our pilot study was to determine the range of angular knee kinematics (angular velocities and accelerations) corresponding to the muscle stretch elicited by TJ. We measured the longitudinal kinematics (velocities and accelerations) of the rectus femoris muscle in vivo using vector tissue Doppler imaging, an ultrasound-based method, and measured the angular kinematics of the knee in response to tendon taps with an electrogoniometer. We concluded that muscle longitudinal elongation accelerations elicited during the standard TJ test exceed angular accelerations (104.40–4534.20rads−2) encountered in typical ADLs, but the velocities (0.82–6.21rads−1) elicited do not exceed those elicited by ADLs. [Copyright &y& Elsevier] Copyright of Journal of Neuroscience Methods is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=61172286&site=ehost-live
611,Proprioceptive Sonomyographic Control: A novel method for intuitive and proportional control of multiple degrees-of-freedom for individuals with upper extremity limb loss.,Siddhartha Sikdar,Scientific Reports,20452322,,7/1/2019,9,1,N.PAG,1.0,137275945,10.1038/s41598-019-45459-7,Springer Nature,Article,,,"Technological advances in multi-articulated prosthetic hands have outpaced the development of methods to intuitively control these devices. In fact, prosthetic users often cite ""difficulty of use"" as a key contributing factor for abandoning their prostheses. To overcome the limitations of the currently pervasive myoelectric control strategies, namely unintuitive proportional control of multiple degrees-of-freedom, we propose a novel approach: proprioceptive sonomyographiccontrol. Unlike myoelectric control strategies which measure electrical activation of muscles and use the extracted signals to determine the velocity of an end-effector; our sonomyography-based strategy measures mechanical muscle deformation directly with ultrasound and uses the extracted signals to proportionally control the position of an end-effector. Therefore, our sonomyography-based control is congruent with a prosthetic user's innate proprioception of muscle deformation in the residual limb. In this work, we evaluated proprioceptive sonomyographic control with 5 prosthetic users and 5 able-bodied participants in a virtual target achievement and holding task for 5 different hand motions. We observed that with limited training, the performance of prosthetic users was comparable to that of able-bodied participants and thus conclude that proprioceptive sonomyographic control is a robust and intuitive prosthetic control strategy. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=137275945&site=ehost-live
612,Semiautomatic segmentation of atherosclerotic carotid artery wall volume using 3D ultrasound imaging.,Siddhartha Sikdar,Medical Physics,00942405,,Apr2015,42,4,2029,15.0,101903559,10.1118/1.4915925,"John Wiley & Sons, Inc.",Article,ATHEROSCLEROTIC plaque; ULTRASONIC imaging; THREE-dimensional imaging; STROKE; CAROTID artery abnormalities,3D US imaging; adventitial wall boundary; Bifurcations; Biological material; Biomedical instrumentation and transducers; biomedical transducers; biomedical ultrasonics; blood vessels; carotid segmentation; Diagnosis using ultrasonic; Digital computing or data processing equipment or methods; e.g. blood; Image data processing or generation; image reconstruction; image segmentation; Image sensors; in general; including micro‐electro‐mechanical systems (MEMS); level set method; Logic and set theory; lumen intima boundary; Magnetohydrodynamics; media adventitia boundary; medical image processing; Medical image segmentation; Processes or apparatus for generating mechanical vibrations of infrasonic; Reconstruction; Segmentation; set theory; sonic or infrasonic waves; sonic or ultrasonic frequency; specially adapted for specific applications; Speckle; stopping criteria; stroke; Three dimensional image processing; Transducers; ultrasonic transducers; Ultrasonographic imaging; Ultrasonography; urine; Haemocytometers; Vascular system; vessel wall volume,"Purpose: Rupture of atherosclerotic plaques in the carotid artery has been implicated in 20% of strokes. 3D ultrasound (US) imaging is emerging as an attractive method to quantify plaque burden and track changes in plaque longitudinally over time. However, plaque segmentation from US images is challenging because of poor boundary contrast and shadowing. The objective of this study is to develop and evaluate a semiautomatic segmentation algorithm with a novel stopping criterion for segmenting outer wall boundary (OWB) and lumen intima boundary (LIB) of common, internal, and external carotid artery from 3D US images for quantifying the vessel wall volume (VWV). Methods: 3D US image volumes were acquired from ten subjects with asymptomatic carotid stenoses. Volumes were acquired using a mechanically scanned linear probe, and the reconstructed volume consisted of 21 slices acquired at an interslice distance of 1 mm. The authors used distance regularized level set method with edge-based energy, region-based energy, smoothness energy, and a novel stopping criterion to segment the LIB and OWB of carotid artery. The algorithm was initialized by six user-selected points on the LIB and OWB in seven 2D cross-sectional slices in each volume. An ellipse fitting and a stopping boundary-based energy is proposed to smooth the OWB contour and to stop leaking of the evolving contour, respectively. The algorithm was compared against ground truth boundaries generated from manual segmentations. The dice similarity coefficient (DSC), Hausdorff distance (HD), and modified HD (MHD) were used as error metrics. Results: The authors' proposed stopping boundary energy-based stopping criterion was compared with percentage change of area and change of the MHD between evolving contours at successive iterations stopping criteria. The performance of the proposed algorithm was better than other two stopping criteria and yielded mean of LIBDSC = 88.78%, OWBDSC = 94.81%, LIBMHD = 0.26 mm, OWBMHD = 0.25 mm, LIBHD = 0.74 mm, and OWBHD = 0.80 mm. The Bland-Altman plot and correlation coefficient (r = 0.99) indicated a high agreement between ground truth and algorithm-generated boundaries. The coefficient of variation (COV) and minimum detectable change of the VWV are 5.2% and 57.2 mm³ (5.18% of mean VWV), calculated from repeated measurements of the VWV by algorithm. The mean absolute distance between corresponding points of the algorithm-generated and the ground truth boundaries was 0.25 mm. Conclusions: The authors have developed a semiautomatic segmentation algorithm for measuring the VWV of the carotid artery using 3D US images with reduced operator interaction and computational time and higher reproducibility using a commercially available 3D US transducer. Their method is a step forward toward routine longitudinal monitoring of 3D plaque progression. [ABSTRACT FROM AUTHOR] Copyright of Medical Physics is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=101903559&site=ehost-live
613,A Pseudo-Likelihood Approach to Linear Regression With Partially Shuffled Data.,Martin Slawski,Journal of Computational & Graphical Statistics,10618600,,Oct-Dec 2021,30,4,991,13.0,154362940,10.1080/10618600.2020.1870482,Taylor & Francis Ltd,Article,ACQUISITION of data; CONFIDENCE intervals; PARAMETER estimation; EXPECTATION-maximization algorithms; DATA integration,Broken sample problem; Expectation-maximization algorithm; Mixture models; Pseudo-likelihood; Record linkage,"Recently, there has been significant interest in linear regression in the situation where predictors and responses are not observed in matching pairs corresponding to the same statistical unit as a consequence of separate data collection and uncertainty in data integration. Mismatched pairs can considerably impact the model fit and disrupt the estimation of regression parameters. In this article, we present a method to adjust for such mismatches under ""partial shuffling"" in which a sufficiently large fraction of (predictors, response)-pairs are observed in their correct correspondence. The proposed approach is based on a pseudo-likelihood in which each term takes the form of a two-component mixture density. expectation-maximization schemes are proposed for optimization, which (i) scale favorably in the number of samples, and (ii) achieve excellent statistical performance relative to an oracle that has access to the correct pairings as certified by simulations and case studies. In particular, the proposed approach can tolerate considerably larger fraction of mismatches than existing approaches, and enables estimation of the noise level as well as the fraction of mismatches. Inference for the resulting estimator (standard errors, confidence intervals) can be based on established theory for composite likelihood estimation. Along the way, we also propose a statistical test for the presence of mismatches and establish its consistency under suitable conditions. Supplemental files for this article are available online. [ABSTRACT FROM AUTHOR] Copyright of Journal of Computational & Graphical Statistics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154362940&site=ehost-live
614,Large-scale Cost-Aware Classification Using Feature Computational Dependency Graph.,Martin Slawski,IEEE Transactions on Knowledge & Data Engineering,10414347,,May2021,33,5,2029,16.0,149773599,10.1109/TKDE.2019.2948607,IEEE,Article,FEATURE selection; DATA mining; MACHINE learning; LOSSLESS data compression; MATHEMATICAL optimization; CLASSIFICATION,Computational modeling; cost-sensitive learning; Data models; Feature computational dependency; Feature extraction; Machine learning; Optimization; Runtime; Standards,"With the rapid growth of real-time machine learning applications, the process of feature selection and model optimization requires to integrate with the constraints on computational budgets. A specific computational resource in this regard is the time needed for evaluating predictions on test instances. The joint optimization problem of prediction accuracy and prediction-time efficiency draws more and more attention in the data mining and machine learning communities. The runtime cost is dominated by the feature generation process that contains significantly redundant computations across different features that sharing the same computational component in practice. Eliminating such redundancies would obviously reduce the time costs in the feature generation process. Our previous Cost-aware classification using Feature computational dependencies heterogeneous Hypergraph (CAFH) model has achieved excellent performance on the effectiveness. In the big data era, the high dimensionality caused by the heterogeneous data sources leads to the difficulty in fitting the entire hypergraph into the main memory and the high computational cost during the optimization process. Simply partitioning the features into batches cannot give the optimal solution since it will lose some feature dependencies across the batches. To improve the high memory and computational costs in the CAFH model, we propose an equivalent Accelerated CAFH (ACAFH) model based on the lossless heterogeneous hypergraph decomposition. An efficient and effective nonconvex optimization algorithm based on the alternating direction method of multipliers (ADMM) is developed to optimize the ACAFH model. The time and space complexities of the optimization algorithm for the ACAFH model are three and one polynomial degrees less than our previous algorithm for the CAFH model, respectively. Extensive experiments demonstrate the proposed ACAFH model achieves competitive performance on the effectiveness and much better performance on the efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149773599&site=ehost-live
614,Large-scale Cost-Aware Classification Using Feature Computational Dependency Graph.,Kai Zeng,IEEE Transactions on Knowledge & Data Engineering,10414347,,May2021,33,5,2029,16.0,149773599,10.1109/TKDE.2019.2948607,IEEE,Article,FEATURE selection; DATA mining; MACHINE learning; LOSSLESS data compression; MATHEMATICAL optimization; CLASSIFICATION,Computational modeling; cost-sensitive learning; Data models; Feature computational dependency; Feature extraction; Machine learning; Optimization; Runtime; Standards,"With the rapid growth of real-time machine learning applications, the process of feature selection and model optimization requires to integrate with the constraints on computational budgets. A specific computational resource in this regard is the time needed for evaluating predictions on test instances. The joint optimization problem of prediction accuracy and prediction-time efficiency draws more and more attention in the data mining and machine learning communities. The runtime cost is dominated by the feature generation process that contains significantly redundant computations across different features that sharing the same computational component in practice. Eliminating such redundancies would obviously reduce the time costs in the feature generation process. Our previous Cost-aware classification using Feature computational dependencies heterogeneous Hypergraph (CAFH) model has achieved excellent performance on the effectiveness. In the big data era, the high dimensionality caused by the heterogeneous data sources leads to the difficulty in fitting the entire hypergraph into the main memory and the high computational cost during the optimization process. Simply partitioning the features into batches cannot give the optimal solution since it will lose some feature dependencies across the batches. To improve the high memory and computational costs in the CAFH model, we propose an equivalent Accelerated CAFH (ACAFH) model based on the lossless heterogeneous hypergraph decomposition. An efficient and effective nonconvex optimization algorithm based on the alternating direction method of multipliers (ADMM) is developed to optimize the ACAFH model. The time and space complexities of the optimization algorithm for the ACAFH model are three and one polynomial degrees less than our previous algorithm for the CAFH model, respectively. Extensive experiments demonstrate the proposed ACAFH model achieves competitive performance on the effectiveness and much better performance on the efficiency. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Knowledge & Data Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149773599&site=ehost-live
615,On the Trade-Off Between Bit Depth and Number of Samples for a Basic Approach to Structured Signal Recovery From $b$ -Bit Quantized Linear Measurements.,Martin Slawski,IEEE Transactions on Information Theory,00189448,,Jun2018,64,6,4159,20.0,129840965,10.1109/TIT.2018.2826459,IEEE,Article,QUANTUM groups; LINEAR statistical models; GAUSSIAN measures; QUANTIZATION (Physics); COMPRESSED sensing,Additives; Compressed sensing; Gaussian noise; Gaussian width; low-complexity signals; Noise level; Noise measurement; quantization; Quantization (signal); Signal to noise ratio,"We consider the problem of recovering a high-dimensional structured signal from independent Gaussian linear measurements each of which is quantized to $b$ bits. The focus is on a specific method of signal recovery that extends a procedure originally proposed by Plan and Vershynin for one-bit quantization to a multi-bit setting. At the heart of this paper is a characterization of the optimal trade-off between the number of measurements m$ and the bit depth per measurement b$ given a total budget of B = m \cdot b -error in estimating the signal. It turns out that the choice b = 1$ is optimal for estimating the unit vector (direction) corresponding to the signal for any level of additive Gaussian noise before quantization as well as for a specific model of adversarial noise, whereas in a noiseless setting the choice b = 2$ is optimal for estimating the direction and the norm (scale) of the signal. Moreover, Lloyd–Max quantization is shown to be an optimal quantization scheme with respect to -estimation error. Our analysis is corroborated by the numerical experiments showing nearly perfect agreement with our theoretical predictions. This paper is complemented by an empirical comparison to alternative methods of signal recovery. The results of that comparison point to a regime change depending on the noise level: in a low-noise setting, the approach under study falls short of more sophisticated competitors while being competitive in moderate- and high-noise settings. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Information Theory is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129840965&site=ehost-live
616,Order-Constrained ROC Regression With Application to Facial Recognition.,Martin Slawski,Technometrics,00401706,,Aug2021,63,3,343,11.0,151763386,10.1080/00401706.2020.1785549,Taylor & Francis Ltd,Article,,Area under the ROC curve; Biometric matching; Facial recognition; Order constraint; ROC regression; Stochastic ordering,"The receiver operating characteristic (ROC) curve is widely used to assess discriminative accuracy of two groups based on a continuous score. In a variety of applications, the distributions of such scores across the two groups exhibit a stochastic ordering. Specific examples include calibrated biomarkers in medical diagnostics or the output of matching algorithms in biometric recognition. Incorporating stochastic ordering as an additional constraint into estimation can improve statistical efficiency. In this article, we consider modeling of ROC curves using both the order constraint and covariates associated with each score given that the latter (e.g., demographic characteristics of the underlying subjects) often have a substantial impact on discriminative accuracy. The proposed method is based on the indirect ROC regression approach using a location-scale model, and quadratic optimization is used to implement the order constraint. The statistical properties of the proposed order-constrained least squares estimator are studied. Based on the theoretical results developed herein, we deduce that the proposed estimator can achieve substantial reductions in mean squared error relative to its unconstrained counterpart. Simulation studies corroborate the superior performance of the proposed approach. Its practical usefulness is demonstrated in an application to face recognition data from the ""Good, Bad, and Ugly"" face challenge, a domain in which accounting for covariates has hardly been studied. [ABSTRACT FROM AUTHOR] Copyright of Technometrics is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151763386&site=ehost-live
617,Regression with linked datasets subject to linkage error.,Martin Slawski,WIREs: Computational Statistics,19395108,,Jul2022,14,4,1,19.0,157874736,10.1002/wics.1570,Wiley-Blackwell,Article,REGRESSION analysis; DATA analysis; BAYESIAN analysis; DATA integration; EMPIRICAL research,Bayesian analysis; data integration; linkage error; mixture models; record linkage; regression,"Data are often collected from multiple heterogeneous sources and are combined subsequently. In combing data, record linkage is an essential task for linking records in datasets that refer to the same entity. Record linkage is generally not error‐free; there is a possibility that records belonging to different entities are linked or that records belonging to the same entity are missed. It is not advisable to simply ignore such errors because they can lead to data contamination and introduce bias in sample selection or estimation, which, in return, can lead to misleading statistical results and conclusions. For a long while, this problem was not properly recognized, but in recent years a growing number of researchers have developed methodology for dealing with linkage errors in regression analysis with linked datasets. The main goal of this overview is to give an account of those developments, with an emphasis on recent approaches and their connection to the so‐called ""Broken Sample"" problem. We also provide a short empirical study that illustrates the efficacy of corrective methods in different scenarios. This article is categorized under:Statistical Models > Model SelectionStatistical and Graphical Methods of Data Analysis > Robust MethodsStatistical and Graphical Methods of Data Analysis > Multivariate Analysis [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157874736&site=ehost-live
618,Two-Stage Approach to Multivariate Linear Regression with Sparsely Mismatched Data.,Martin Slawski,Journal of Machine Learning Research,15324435,,2020,21,189-216,1,42.0,146744451,,Microtome Publishing,Article,SIGNAL-to-noise ratio; PERMUTATIONS; DATA logging,,"A tacit assumption in linear regression is that (response, predictor)-pairs correspond to identical observational units. A series of recent works have studied scenarios in which this assumption is violated under terms such as \Unlabeled Sensing and \Regression with Unknown Permutation"". In this paper, we study the setup of multiple response variables and a notion of mismatches that generalizes permutations in order to allow for missing matches as well as for one-to-many matches. A two-stage method is proposed under the assumption that most pairs are correctly matched. In the first stage, the regression parameter is estimated by handling mismatches as contaminations, and subsequently the generalized permutation is estimated by a basic variant of matching. The approach is both computationally convenient and equipped with favorable statistical guarantees. Specifically, it is shown that the conditions for permutation recovery become considerably less stringent as the number of responses m per observation increase. Particularly, for m = (log n), the required signal-to-noise ratio no longer depends on the sample size n. Numerical results on synthetic and real data are presented to support the main findings of our analysis. [ABSTRACT FROM AUTHOR] Copyright of Journal of Machine Learning Research is the property of Microtome Publishing and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146744451&site=ehost-live
619,Development and evaluation of optimal prostate biopsy protocols for individual groups of patients,Ariela Sofer,International Congress Series,05315131,,Jul2004,1268,,803,6.0,13327498,10.1016/j.ics.2004.03.347,Elsevier B.V.,Article,PROSTATE cancer; BIOPSY; MALE reproductive organs; PATIENTS,Age-specific; Biopsy simulation; Needle biopsy protocols; Prostate cancer; Race-specific; Volume-specific; Optimization,"We have developed a number of optimal biopsy protocols for the detection of prostate cancer for both general and specific groups of patients. Age, race and prostate volume were considered in the development and optimal protocols were developed for individual patients based on these factors. Experimental results showed that optimal 6-needle protocols outperformed traditional sextant protocol for all age, race and prostate volume groups of patients, and the difference is statistically significant. We did not find significant improvements for developing protocols for age-specific, race-specific or prostate volume-specific patients in this preliminary study. Our results also indicated that more biopsy cores should be used in older men to achieve a targeted detection rate. [Copyright &y& Elsevier] Copyright of International Congress Series is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=13327498&site=ehost-live
620,Preface.,Ariela Sofer,Annals of Operations Research,02545330,,Nov2006,148,1,1,3.0,23302529,10.1007/s10479-006-0088-6,Springer Nature,Article,PREFACES & forewords; SMALLPOX,,The article discusses various reports within the issue including a dynamic epidemic-intervention model that captures key features of mass vaccination for smallpox by Kress and a real-time decision support system that incorporates efficient optimization technology seamlessly with a simulation module by Lee et al.,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=23302529&site=ehost-live
621,Bayesian regularization: From Tikhonov to horseshoe.,Vadim Sokolov,WIREs: Computational Statistics,19395108,,Jul2019,11,4,N.PAG,1.0,136997958,10.1002/wics.1463,Wiley-Blackwell,Article,TIKHONOV regularization; STATISTICAL learning; MATHEMATICAL regularization; MACHINE learning; DATA analysis; LITERATURE reviews,Bayesian regression; horseshoe; lasso; regularization,"Bayesian regularization is a central tool in modern‐day statistical and machine learning methods. Many applications involve high‐dimensional sparse signal recovery problems. The goal of our paper is to provide a review of the literature on penalty‐based regularization approaches, from Tikhonov (Ridge, Lasso) to horseshoe regularization. This article is categorized under:Statistical and Graphical Methods of Data Analysis > Robust MethodsStatistical Models > Linear ModelsStatistical Models > Bayesian Models [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136997958&site=ehost-live
623,Deep learning: Computational aspects.,Vadim Sokolov,WIREs: Computational Statistics,19395108,,Sep/Oct2020,12,5,1,17.0,145036839,10.1002/wics.1500,Wiley-Blackwell,Article,STATISTICAL learning; DEEP learning; LINEAR algebra; LATENT variables; DATA science,deep learning; linear algebra; stochastic gradient descent,"In this article, we review computational aspects of deep learning (DL). DL uses network architectures consisting of hierarchical layers of latent variables to construct predictors for high‐dimensional input–output models. Training a DL architecture is computationally intensive, and efficient linear algebra library is the key for training and inference. Stochastic gradient descent (SGD) optimization and batch sampling are used to learn from massive datasets. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences > Deep Learning Statistical Learning and Exploratory Methods of the Data Sciences > Modeling Methods Statistical Learning and Exploratory Methods of the Data Sciences > Neural Networks [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=145036839&site=ehost-live
624,Building Security into Off-the-Shelf Smartphones.,Angelos Stavrou,Computer (00189162),00189162,,Feb2012,45,2,82,0.0,73617961,10.1109/MC.2012.44,IEEE,Article,"MOBILE communication systems; MALWARE; COMPUTER software; APPLICATION software; COMPUTER security; Custom Computer Programming Services; Software publishers (except video game publishers); Software Publishers; Computer and software stores; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer, computer peripheral and pre-packaged software merchant wholesalers; Wireless Telecommunications Carriers (except Satellite)",Androids; application testing; Computer security; Malware; Mobile communication; mobile computing; NIST; security; Smart phones,"Quantifying mobile application functionality and enforcing a finer-grained permission model would identify and thus thwart a wide range of malware. [ABSTRACT FROM PUBLISHER] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=73617961&site=ehost-live
625,DDoS in the IoT: Mirai and Other Botnets.,Angelos Stavrou,Computer (00189162),00189162,,Jul2017,50,7,80,5.0,124027610,10.1109/MC.2017.201,IEEE,Article,"INTERNET of things; INTERNET security; BOTNETS; DENIAL of service attacks; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Internet Publishing and Broadcasting and Web Search Portals; Wired Telecommunications Carriers",botnet; BrickerBot; Computer crime; Computer security; cybersecurity; Cybertrust; DDoS; distributed denial of service; Distributed processing; Hajime; Internet; Internet of Things; IoT; IP networks; malware; Mirai; Persirai; Ports (Computers); Risk management; security; Servers,"The Mirai botnet and its variants and imitators are a wake-up call to the industry to better secure Internet of Things devices or risk exposing the Internet infrastructure to increasingly disruptive distributed denial-of-service attacks. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=124027610&site=ehost-live
626,On the Move: Evading Distributed Denial-of-Service Attacks.,Angelos Stavrou,Computer (00189162),00189162,,Mar2016,49,3,104,4.0,113872710,10.1109/MC.2016.85,IEEE,Article,"DENIAL of service attacks; CLIENT/SERVER computing; COMPUTERS; COMPUTER networks; CYBERTERRORISM; Computer Systems Design Services; Computer, computer peripheral and pre-packaged software merchant wholesalers; Computer and Computer Peripheral Equipment and Software Merchant Wholesalers; Computer and software stores; Electronic Computer Manufacturing; Computer and peripheral equipment manufacturing; Electronics Stores",Computer crime; Computer security; Computers; Cybertrust; DDoS; distributed denial-of-service; Internet; IP networks; MOTAG; moving-target defense; Productivity; security; Servers; Service agreements,"A proposed moving-target defense against DDoS attacks repeatedly shuffles client-to-server assignments to identify and eventually quarantine malicious clients. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=113872710&site=ehost-live
627,"Securely Making ""Things"" Right.",Angelos Stavrou,Computer (00189162),00189162,,Sep2015,48,9,84,5.0,109993862,10.1109/MC.2015.258,IEEE,Article,INTERNET of things; INFORMATION technology security; COMPUTER security; CYBERINFRASTRUCTURE; INTERNET; COMPUTER worms; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals,Communication system security; Computer security; Cryptography; cybersecurity; Internet of things; IoT; IoT security; Privacy; Security; Wireless communication,"The Internet of Things (IoT) promises to seamlessly bind the physical world to cyberinfrastructure, but the Internet's insecure design principles could lead to life-threatening consequences. It's time to make security an integral IoT design tenet. [ABSTRACT FROM PUBLISHER] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=109993862&site=ehost-live
628,Verified Time.,Angelos Stavrou,Computer (00189162),00189162,,Mar2017,50,3,78,5.0,122302090,10.1109/MC.2017.63,IEEE,Article,"TIMESTAMPS; TIME measurements; DATA loggers; DIGITAL media; BLOCKCHAINS; DISTRIBUTED computing; Internet Publishing and Broadcasting and Web Search Portals; Instruments and Related Products Manufacturing for Measuring, Displaying, and Controlling Industrial Process Variables",blockchains; Internet of Things; IoT; security; timestamping; timestamping authority; TSA,"A provably secure timestamping system could be achieved by combining the trustworthiness and accuracy that comes from having a set of trusted centralized timestamping authorities with the open, decentralized nature of blockchain protocols. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=122302090&site=ehost-live
629,Virtual Extension The Ephemeral Legion: Producing an Expert Cyber-Security Work Force from Thin Air.,Angelos Stavrou,Communications of the ACM,00010782,,Jan2011,54,1,129,3.0,56676261,10.1145/1866739.1866764,Association for Computing Machinery,Opinion,INTERNET security; EMPLOYEE training; EMPLOYEE education; CERTIFICATION; UNITED States; Professional and Management Development Training; Internet Publishing and Broadcasting and Web Search Portals; Wired Telecommunications Carriers,,"The article presents the authors' views on developing a cyber-security work force in the United States. The strategy of training new information security workers through education rather than mass certification of the existing cyber-security work force is discussed. Government policy recommendations include funding cyber-security education, developing high-school pilot programs, providing guidance to ethical hackers, and expanding security expertise at universities.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=56676261&site=ehost-live
630,Development of Predictive Informatics Tool Using Electronic Health Records to Inform Personalized Evidence-Based Pressure Injury Management for Veterans with Spinal Cord Injury.,Jiayang Sun,Military Medicine,00264075,,2021 Supplement,186,,651,8.0,148660258,10.1093/milmed/usaa469,Oxford University Press / USA,journal article,,,"<bold>Background: </bold>Pressure injuries (PrI) are serious complications for many with spinal cord injury (SCI), significantly burdening health care systems, in particular the Veterans Health Administration. Clinical practice guidelines (CPG) provide recommendations. However, many risk factors span multiple domains. Effective prioritization of CPG recommendations has been identified as a need. Bioinformatics facilitates clinical decision support for complex challenges. The Veteran's Administration Informatics and Computing Infrastructure provides access to electronic health record (EHR) data for all Veterans Health Administration health care encounters. The overall study objective was to expand our prototype structural model of environmental, social, and clinical factors and develop the foundation for resource which will provide weighted systemic insight into PrI risk in veterans with SCI.<bold>Methods: </bold>The SCI PrI Resource (SCI-PIR) includes three integrated modules: (1) the SCIPUDSphere multidomain database of veterans' EHR data extracted from October 2010 to September 2015 for ICD-9-CM coding consistency together with tissue health profiles, (2) the Spinal Cord Injury Pressure Ulcer and Deep Tissue Injury Ontology (SCIPUDO) developed from the cohort's free text clinical note (Text Integration Utility) notes, and (3) the clinical user interface for direct SCI-PIR query.<bold>Results: </bold>The SCI-PIR contains relevant EHR data for a study cohort of 36,626 veterans with SCI, representing 10% to 14% of the U.S. population with SCI. Extracted datasets include SCI diagnostics, demographics, comorbidities, rurality, medications, and laboratory tests. Many terminology variations for non-coded input data were found. SCIPUDO facilitates robust information extraction from over six million Text Integration Utility notes annually for the study cohort. Visual widgets in the clinical user interface can be directly populated with SCIPUDO terms, allowing patient-specific query construction.<bold>Conclusion: </bold>The SCI-PIR contains valuable clinical data based on CPG-identified risk factors, providing a basis for personalized PrI risk management following SCI. Understanding the relative impact of risk factors supports PrI management for veterans with SCI. Personalized interactive programs can enhance best practices by decreasing both initial PrI formation and readmission rates due to PrI recurrence for veterans with SCI. [ABSTRACT FROM AUTHOR] Copyright of Military Medicine is the property of Oxford University Press / USA and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148660258&site=ehost-live
631,"Subsampling from features in large regression to find ""winning features"".",Jiayang Sun,Statistical Analysis & Data Mining,19321864,,Apr2021,14,2,168,17.0,149246460,10.1002/sam.11499,Wiley-Blackwell,Article,FEATURE selection; RANDOM forest algorithms; OVARIAN cancer; GENES; DATA science,feature selection; high dimensions; subsampling winner algorithm (SWA); variable selection,"Feature (or variable) selection from a large number of p features continuously challenges data science, especially for ever‐enlarging data and in discovering scientifically important features in a regression setting. For example, to develop valid drug targets for ovarian cancer, we must control the false‐discovery rate (FDR) of a selection procedure. The popular approach to feature selection in large‐p regression uses a penalized likelihood or a shrinkage estimation, such as a LASSO, SCAD, Elastic Net, or MCP procedure. We present a different approach called the Subsampling Winner algorithm (SWA), which subsamples from p features. The idea of SWA is analogous to selecting US national merit scholars' that selects semifinalists based on student's performance in tests done at local schools (a.k.a. subsample analyses), and then determine the finalists (a.k.a. winning features) from the semifinalists. Due to its subsampling nature, SWA can scale to data of any dimension. SWA also has the best‐controlled FDR compared to the penalized and Random Forest procedures while having a competitive true‐feature discovery rate. Our application of SWA to an ovarian cancer data revealed functionally important genes and pathways. [ABSTRACT FROM AUTHOR] Copyright of Statistical Analysis & Data Mining is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149246460&site=ehost-live
633,Nearest-neighbor methods.,Clifton Sutton,WIREs: Computational Statistics,19395108,,May/Jun2012,4,3,307,0.0,74303888,10.1002/wics.1195,Wiley-Blackwell,Article,MATHEMATICAL statistics; STATISTICS; NEAREST neighbor analysis (Statistics); SPATIAL analysis (Statistics); PROBABILITY theory,classification; distance; local; majority,"Nearest-neighbor classification is a simple and popular method for supervised classification. The basic method is to classify a query point as being of a certain class if of the k-nearest neighbors of the query point, more of them belong to this class than to any other class. Variations of the basic method include weighted nearest-neighbor methods and adaptive nearest-neighbor methods (which allow some variables to have greater influence than other variables). WIREs Comput Stat 2012, 4:307-309. doi: 10.1002/wics.1195 For further resources related to this article, please visit the [ABSTRACT FROM AUTHOR] Copyright of WIREs: Computational Statistics is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=74303888&site=ehost-live
637,"Landslide susceptibility analyses using Random Forest, C4.5, and C5.0 with balanced and unbalanced datasets.",Gheorghe Tecuci,CATENA,03418162,,Aug2021,203,,N.PAG,1.0,150444585,10.1016/j.catena.2021.105355,Elsevier B.V.,Article,RANDOM forest algorithms; LANDSLIDE hazard analysis; VECTOR data; FEATURE selection; DECISION trees; CLIMATE change,Decision tree learning; Geotechnical properties; Landslide; Natural hazard; Susceptibility analysis,"• C4.5 and C5.0 outperform Random Forest in landslide susceptibility analyses. • Use of balance vector data improves the performance of Random Forest. • Including all conditioning factors improve landslide susceptibility results. • Difference in C4.5 and C5.0 results for raster and vector data is marginal. The effects of landslides have been exponentially increasing due to the rapid growth of urbanization and global climate change. The information gained from predictive models and landslide susceptibility analyses can be used to develop warning systems and mitigation measures. A comparative study was conducted to evaluate the effectiveness of landslide susceptibility analyses in a given area using three decision tree algorithms including Random Forest (RF), C4.5, and C5.0. Two sets of imagery datasets (raster and vector) were used and three combinations of 13 conditioning factors (including seven geotechnical properties of the soil) were determined by Information Gain, Gain Ratio, Chi-Squared Test, and Random Forest Importance. Datasets for the landslide conditioning factors were created based on the outcomes from the feature selection methods, in three different scenarios. In Scenario 1 the least important factors/features (as identified by information gain, chi-square, and gain ratio measures) were eliminated. In Scenario 2 only the most important factors (as identified by RF feature selection method evaluation) were kept. In Scenario 3, no factor was eliminated, using the data directly obtained from the sources without applying any feature selection method. The performances of the models were evaluated using statistical verification scores. C4.5 was found to have the highest performance when all 13 conditioning parameters (Scenario 3) were used for both the raster and vector data set. The RF model was the least effective in predicting the landslides in all three scenarios. However, the use of the balance vector dataset significantly increased the performance of the RF model. C4.5 and C5.0 had significantly better performance in handling extremely unbalance data in comparison to RF. Density, silt and clay content, and Atterberg's limits (LL and PI) were the most important geotechnical conditioning factors in the performed landslide susceptibility analyses. [ABSTRACT FROM AUTHOR] Copyright of CATENA is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=150444585&site=ehost-live
644,Effect of ammonium on the hydraulic conductivity of geosynthetic clay liners.,Kuo Tian,Geotextiles & Geomembranes,02661144,,Dec2017,45,6,665,9.0,125419449,10.1016/j.geotexmem.2017.08.008,Elsevier B.V.,Article,"HYDRAULIC conductivity; GEOSYNTHETIC clay liners; SODIUM; BENTONITE; BIOREACTORS; Clay and Ceramic and Refractory Minerals Mining; Shale, clay and refractory mineral mining and quarrying",Ammonium solution; Bioreactor leachate; Exchange complex; Geosynthetic clay liner; Geosynthetics; Hydraulic conductivity; Recirculation landfill leachate; Swelling,"Hydraulic conductivity and swell index tests were conducted on a conventional geosynthetic clay liner (GCL) containing sodium-bentonite (Na-B) using 5, 50, 100, 500, and 1000 mM ammonium acetate (NH 4 OAc) solutions to investigate how NH 4 + accumulation in leachates in bioreactor and recirculation landfills may affect GCLs. Control tests were conducted with deionized (DI) water. Swell index of the Na-B was 27.7 mL/2 g in 5 mM NH 4 + solution and decreased to 5.0 mL/2 g in 1000 mM NH 4 + solution, whereas the swell index of Na-B in DI water was 28.0 mL/2 g. Hydraulic conductivity of the Na-B GCL to 5, 50, and 100 mM NH 4 + was low, ranging from 1.6–5.9 × 10 −11 m/s, which is comparable to the hydraulic conductivity to DI water (2.1 × 10 −11 m/s). Hydraulic conductivities of the Na-B GCL permeated with 500 and 1000 mM NH 4 + solutions were much higher (e.g., 1.6–5.2 × 10 −6 m/s) due to suppression of osmotic swelling. NH 4 + replaced native Na + , K + , Ca 2+ , and Mg 2+ in the exchange complex of the Na-B during permeation with all NH 4 + solutions, with the NH 4 + fraction in the exchange complex increasing from 0.24 to 0.83 as the NH 4 + concentration increased from 5 to 1000 mM. A Na-B GCL specimen permeated with 1000 mM NH 4 + solution to chemical equilibrium was subsequently permeated with DI water. Permeation with the NH 4 + converted the Na-B to “NH 4 -bentonite” with more than 80% of the exchange complex occupied by NH 4 + . Hydraulic conductivity of this GCL specimen decreased from 5.9 × 10 −6 m/s to 2.9 × 10 −11 m/s during permeation with DI water, indicating that “NH 4 -bentonite” can swell and have low hydraulic conductivity, and that the impact of more concentrated NH 4 + solutions on swelling and hydraulic conductivity is reversible. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=125419449&site=ehost-live
645,Effect of incineration ash leachates on the hydraulic conductivity of bentonite-polymer composite geosynthetic clay liners.,Kuo Tian,Waste Management,0956053X,,Feb2022,139,,25,14.0,154857550,10.1016/j.wasman.2021.12.011,Elsevier B.V.,Article,GEOSYNTHETIC clay liners; HYDRAULIC conductivity; LINEAR polymers; INCINERATION; CROSSLINKED polymers; LEACHATE; POLYMER blends; Waste treatment and disposal; Solid Waste Combustors and Incinerators,"Crosslinked polymer; Incinerator ash; Landfill liner; Linear polymer; Municipal solid waste incineration; Polymer elution; Waste-to-energy, Polymer modified bentonite","• Bentonite-polymer composite geosynthetic clay liners (BPC GCLs) may contain water-soluble (linear) or water-insoluble (crosslinked) polymers. • Generally, BPC GCLs with high initial polymer loading had relatively lower hydraulic conductivity than those with low initial polymer loading. • For BPC GCLs containing linear polymer, polymer elution occurred when permeated with water or leachate. • There was no correlation between the percentage of polymer retained and final hydraulic conductivity of the BPC GCLs containing linear polymer. A study was conducted to evaluate the hydraulic conductivity (k) of six bentonite-polymer composite (BPC) geosynthetic clay liners (GCLs) using five synthetic municipal solid waste incineration ash (IA) leachates with ionic strength (I) ranging from 174 to1978 mM. The BPC GCLs contained a dry blend of bentonite and proprietary polymers and had polymer loading ranging from 0.5 to 5.5%. The polymers used in the BPC GCLs were classified as linear polymer (LP) or crosslinked polymer (CP) based on the swelling characteristics of specimens extracted from the GCLs. Comparable hydraulic conductivity tests were also performed on two conventional bentonite (CB) GCLs as controls. The BPC GCLs had k of 2.6 – 6.7 × 10-11 m/s when permeated with IA leachate with I = 174 mM, whereas the CB GCLs had k > 5.0 × 10-8 m/s when permeated with the same leachate. However, k of the BPC GCLs ranged from the order of 10-10 to 10-7 m/s when permeated with IA leachates with I > 600 mM. BPC GCLs with high polymer loading generally had lower k compared to those with lower polymer loading when permeated with the same IA leachate, regardless of the polymer type. Polymer eluted from the BPC GCLs containing LP during permeation with DI water or IA leachate. Unlike CPs, LPs are water-soluble, therefore, they seem to easily migrate during permeation. There was no correlation between the percentage of polymer retained and the final hydraulic conductivity of the LPB GCLs used in this study. [ABSTRACT FROM AUTHOR] Copyright of Waste Management is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=154857550&site=ehost-live
646,Effect of specimen preparation on the swell index of bentonite-polymer GCLs.,Kuo Tian,Geotextiles & Geomembranes,02661144,,Dec2020,48,6,875,11.0,147117108,10.1016/j.geotexmem.2020.06.006,Elsevier B.V.,Article,INCINERATION; HYDRAULIC conductivity; GEOSYNTHETIC clay liners; SOLID waste; MUNICIPAL water supply; Waste treatment and disposal; Solid Waste Combustors and Incinerators,Crosslinked polymer; Geosynthetic clay liners; Hydraulic conductivity; Linear polymer; Loss on ignition; Swell index,"Experiments were conducted to investigate how specimen preparation (crushing and sieving) affects the swell index (SI) of bentonite-polymer (B–P) composites and the relationship between SI and hydraulic conductivity of B–P GCLs. Seven B–P and one Na–B GCLs were used in this study. Tests were conducted using DI water and synthetic municipal solid waste incineration ash leachates. Specimens were prepared using the ASTM D5890 and two alternative methods prior to SI testing. For both Na–B and B–P composites, <100% of the specimen passed through the #100 sieve regardless of the amount of crushing performed using a mortar and pestle. SIs and loss on ignitions (LOI) of the portion of the B–P composites passing #100 sieve were comparable to the Na–B, whereas the B–P specimen retained on #100 sieve had very high SIs and LOIs. These observations indicate that crushing and sieving of the B–P composites lead to segregation of polymer. A stronger correlation (R2 = 0.90) was observed between SI and hydraulic conductivity, only when SI tests were conducted with B–P without any crushing and sieving, suggesting that SI tests should conduct with B–P composites retrieved from the GCLs without sieving to provide a better prediction of hydraulic compatibility. • There are limitations in the ASTM D5890 specimen preparation procedure. • < 100% of the Na–B and B–P specimens passed through the #100 sieve after crushing. • The portion of the B–P specimens retained on the #100 sieve mainly comprised polymer. • SIs of the uncrushed B–P specimens were higher than that of the ASTM D5890 specimens. • SI of uncrushed B–P specimens correlates well with hydraulic conductivity of B–P GCLs. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147117108&site=ehost-live
647,Hydraulic conductivity of bentonite-polymer geosynthetic clay liners to coal combustion product leachates.,Kuo Tian,Geotextiles & Geomembranes,02661144,,Oct2021,49,5,1129,10.0,151632293,10.1016/j.geotexmem.2021.03.007,Elsevier B.V.,Article,HYDRAULIC conductivity; GEOSYNTHETIC clay liners; COMBUSTION products; LEACHATE; COAL products; COAL combustion; STRAINS & stresses (Mechanics); IONIC strength; Other petroleum and coal product manufacturing; All Other Petroleum and Coal Products Manufacturing,Bentonite-polymer (B–P); CCP leachates; Geosynthetic clay liners (GCLs),"Hydraulic conductivity of seven geosynthetic clay liners (GCLs) to synthetic coal combustion product (CCP) leachates were evaluated in this study. The leachates are chemically representative of typical and worst scenarios observed in CCP landfills. The ionic strength (I) of the synthetic CCP leachates ranged from 50 mM to 4676 mM (TCCP-50, LRMD-96, TFGDS-473, LR-2577, HI-3179 and HR-4676). One of the GCLs contained conventional sodium bentonite (Na–B) and the other six contained bentonite-polymer (B–P) mixture with polymer loadings ranging from 0.5% to 12.7%. Hydraulic conductivity tests were conducted at an effective confining stress of 20 kPa. The hydraulic conductivity of the Na–B GCLs were >1 × 10−10 m/s when permeated with all six CCP leachates, whereas the B–P GCLs with sufficient polymer loading maintained low hydraulic conductivity to synthetic CCP leachates. All the B–P GCLs showed low hydraulic conductivity (<1 × 10−10 m/s) to low ionic strength leachates (TCCP-50, I = 50 mM and LRMD-96, I = 96 mM). B–P GCLs with P > 5% showed low hydraulic conductivity (<1 × 10−10 m/s) up to HI-3179 leachates. These results suggest that B–P GCLs with sufficient polymer loading can be used to manage aggressive CCP leachates. • Hydraulic conductivity of Na-B and six B-P GCLs to six synthetic CCP leachates. • Six synthetic leachates were created in this study to mimic typical to worst field scenarios. • B-P GCL with > 5% polymer loading can maintain low hydraulic conductivity to CCP leachates. • B-P GCL with sufficient polymer loading can be used to manage aggressive CCP leachates. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=151632293&site=ehost-live
648,Radiation dose and antioxidant depletion in a HDPE geomembrane.,Kuo Tian,Geotextiles & Geomembranes,02661144,,Aug2018,46,4,426,10.0,129589460,10.1016/j.geotexmem.2018.03.003,Elsevier B.V.,Article,ANTIOXIDANTS; HIGH density polyethylene; GEOMEMBRANES; RADIOACTIVE wastes; LEACHATE; Waste treatment and disposal; Hazardous Waste Treatment and Disposal,Antioxidant depletion; Dose deposition; HDPE geomembrane; Low-level radioactive waste; Radiation; Service life,"The impact of α and β radiation on antioxidant depletion in smooth high-density polyethylene (HDPE) geomembranes (GMs) is described. Smooth HDPE GMs having different thickness (0.04-mm, 0.1-mm, 0.2-mm) were created by mechanically pulverizing sections of 2-mm-thick smooth HDPE GM and extruding the polymer at different thicknesses using a film blowing machine. The 2-mm-thick smooth HDPE GM was also used in the experiments. HDPE GM specimens were exposed to sealed sources of 241 Am and 99 Tc for 1–50 h to simulate the impact of α and β radiation from U and 99 Tc in low-level radioactive waste (LLW) leachate. Standard oxidative induction time (OIT) tests were conducted to determine antioxidant depletion. No change in OIT occurred in the 2-mm-thick HDPE GM after exposure to sealed sources of 241 Am and 99 Tc for 50 h. In much thinner GMs (e.g., 0.04 mm), however, significant antioxidant depletion occurred after exposure most likely due to penetration of α and β particles. Penetration depth of α and β particles and dose deposition in HDPE GMs were estimated with the GEometry ANd Tracking (GEANT4) program. Predictions from GEANT4 show that maximum dose deposition occurs at the surface of the HDPE GM and decreases with depth. A multilayer model is used to estimate antioxidant depletion in HDPE GMs for depth-dependent doses. These estimates suggest that radiation from LLW leachate has an insignificant effect on antioxidant depletion in HDPE GMs due to the low dose deposition (e.g., 2.42 Gy) expected over a 1000-yr service life, even if the level of activity in LLW leachate increases 10x to 100x the level typical of today. [ABSTRACT FROM AUTHOR] Copyright of Geotextiles & Geomembranes is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129589460&site=ehost-live
650,Fire performance of functionally-graded-material sheathed load bearing thin-walled structural framing.,Girum Urgessa,Fire Safety Journal,03797112,,Oct2021,125,,N.PAG,1.0,152606340,10.1016/j.firesaf.2021.103425,Elsevier B.V.,Article,STRUCTURAL frames; FUNCTIONALLY gradient materials; COLD-formed steel; MECHANICAL properties of condensed matter; FIRE prevention; THERMAL properties; FIRE testing; Finish Carpentry Contractors; Framing Contractors,Cold-formed steel; Fire-resistance rating; Functionally graded materials; Temperature distribution; Thin-walled structure,"This paper presents the fire rating performance of load-bearing thin-walled system sheathed with Functionally Graded Material (FGM) board under standard ISO834 fire through numerical simulation. FGMs are one of the new classes of advanced composite materials that possess continuous variation of material properties within a given direction. The composition of the FGM sheathing is defined by the volume fractions of the constituents' materials (metal/ceramic) based on the power-law (P-FGM) material function. The general rule of mixtures is then used to predict the thermo-mechanical properties of the FGM sheathing. Fire rating analyses for the Cold-Formed Steel (CFS) wall system were conducted under steady-state conditions where the elastic buckling load from bifurcation analysis under fire conditions was first obtained. Then using the RIKS ON algorithm, collapse analysis was performed in a time frame until failure deformation occurred. The effect of non-uniform temperature on mechanical and thermal properties of the wall stud was included at each time frame in both elastic and collapse analysis. From the FE analysis using ABAQUS, it was observed that the use of FGM as a sheathing material for fire protection increases the failure time for all load ratios compared with the traditional gypsum board. The increase in failure time has a significant implication for improving the safety of occupants during fire scenarios. The study also shows that the novel composite material (FGM board), if properly designed, can lead to an alternative fire protection material in the thin-walled system. [ABSTRACT FROM AUTHOR] Copyright of Fire Safety Journal is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=152606340&site=ehost-live
651,"Internal Relative Humidity, Autogenous Shrinkage, and Strength of Cement Mortar Modified with Superabsorbent Polymers.",Girum Urgessa,Polymers (20734360),20734360,,Oct2018,10,10,1074,1.0,132686423,10.3390/polym10101074,MDPI,Article,SUPERABSORBENT polymers; HUMIDITY; THERMAL expansion; CROSSLINKING (Polymerization); SURFACE cracks; DEFORMATIONS (Mechanics),autogenous shrinkage; coefficient of thermal expansion; internal curing; internal relative humidity; strength; superabsorbent polymers,"Laboratory evaluations were performed to investigate the effect of internal curing (IC) by superabsorbent polymers (SAP) on the internal relative humidity (IRH), autogenous shrinkage, coefficient of thermal expansion (CTE), and strength characteristics of low water-cement ratio (w/c) mortars. Four types of SAP with different cross-linking densities and particle sizes were used. Test results showed that the SAP inclusion effectively mitigated the IRH drops due to self-desiccation and corresponding autogenous shrinkage, and the IC effectiveness tended to increase with an increased SAP dosage. The greater the cross-linking density and particle size of SAP, the less the IRH drop and autogenous shrinkage. The trend of autogenous shrinkage developments was in good agreement with that of IRH changes, with nearly linear relationships between them. Both immediate deformation (ID)-based and full response-based CTEs were rarely affected by SAP inclusions. There were no substantial losses in compressive and flexural strengths of SAP-modified mortar compared to reference plain mortar. The findings revealed that SAPs can be effectively used to reduce the shrinkage cracking potential of low w/c cement-based materials at early ages, without compromising mechanical and thermal characteristics. [ABSTRACT FROM AUTHOR] Copyright of Polymers (20734360) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=132686423&site=ehost-live
652,Thermal responses of concrete slabs containing microencapsulated low-transition temperature phase change materials exposed to realistic climate conditions.,Girum Urgessa,Cement & Concrete Composites,09589465,,Nov2019,104,,N.PAG,1.0,140984573,10.1016/j.cemconcomp.2019.103391,Elsevier B.V.,Article,PHASE change materials; CONCRETE slabs; CONCRETE pavements; TRANSITION temperature; FREEZE-thaw cycles; TEMPERATURE; SERVICE life; Structural Steel and Precast Concrete Contractors; All Other Specialty Trade Contractors; All other non-metallic mineral product manufacturing,Concrete slab; Freeze-thaw deterioration; Mechanical properties; Microencapsulated phase change materials; Service life prediction; Thermal response,"This study examines the effect of microencapsulated low-transition temperature phase change material (PCM) additions on the thermal response of concrete slabs subjected to long-term realistic environmental exposure. To prevent direct contact of PCM with cement hydration products and possible leakage upon liquefaction, an inert PCM was encapsulated with a melamine-formaldehyde resin via an emulsification process before being added in concrete mixtures. Temperature monitoring was performed on three 500 × 500 × 150 mm large-scale concrete slabs with and without PCM for about 14 months encompassing two cold winter seasons. Results indicated that the addition of microencapsulated PCM effectively reduced excessive temperature drop and the number of freeze-thaw cycles concrete slabs experience during winter seasons, which may lead to service life extension by up to 5.2%–35.9% based on a freeze-thaw deterioration model. In particular, the effectiveness of PCM was found to be pronounced when the ambient temperature varied around the transition temperature (mild-cold seasons) while it became insignificant under prolonged exposure to extreme climate conditions such as cold winter and summer. The result of a visual condition survey was consistent with that of the model predictions, which verified the potential benefits of low-transition temperature PCM technology in concrete applications. This study also investigated the influence of microencapsulated PCM pellet embedment on the compressive and flexural strength characteristics. [ABSTRACT FROM AUTHOR] Copyright of Cement & Concrete Composites is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140984573&site=ehost-live
654,Emerging clinical applications of text analytics.,Özlem Uzuner,International Journal of Medical Informatics,13865056,,Feb2020,134,,N.PAG,1.0,140937648,10.1016/j.ijmedinf.2019.103974,Elsevier B.V.,editorial,,,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=140937648&site=ehost-live
655,A Microfluidic Platform to Monitor Real-Time Effects of Extracellular Vesicle Exchange between Co-Cultured Cells across Selectively Permeable Barriers.,Remi Veneziano,International Journal of Molecular Sciences,16616596,,Apr2022,23,7,3534,15.0,156291938,10.3390/ijms23073534,MDPI,Article,EXTRACELLULAR vesicles; CELL populations; EXTRACELLULAR matrix; EXOSOMES; CELL separation; MICROFLUIDIC devices,exosomes; extracellular matrix; extracellular vesicles; functional EV assays; intercellular communication; lab-on-a-chip; Matrigel; microfluidic device; PEGDA,"Exosomes and other extracellular vesicles (EVs) play a significant yet poorly understood role in cell–cell communication during homeostasis and various pathological conditions. Conventional in vitro and in vivo approaches for studying exosome/EV function depend on time-consuming and expensive vesicle purification methods to obtain sufficient vesicle populations. Moreover, the existence of various EV subtypes with distinct functional characteristics and submicron size makes their analysis challenging. To help address these challenges, we present here a unique chip-based approach for real-time monitoring of cellular EV exchange between physically separated cell populations. The extracellular matrix (ECM)-mimicking Matrigel is used to physically separate cell populations confined within microchannels, and mimics tissue environments to enable direct study of exosome/EV function. The submicron effective pore size of the Matrigel allows for the selective diffusion of only exosomes and other smaller EVs, in addition to soluble factors, between co-cultured cell populations. Furthermore, the use of PEGDA hydrogel with a very small pore size of 1.2 nm in lieu of Matrigel allows us to block EV migration and, therefore, differentiate EV effects from effects that may be mediated by soluble factors. This versatile platform bridges purely in vitro and in vivo assays by enabling studies of EV-mediated cellular crosstalk under physiologically relevant conditions, enabling future exosome/EV investigations across multiple disciplines through real-time monitoring of vesicle exchange. [ABSTRACT FROM AUTHOR] Copyright of International Journal of Molecular Sciences is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156291938&site=ehost-live
656,DNA Scaffolds for Nanophotonics: Utilizing the Organizational Power of DNA Scaffolds for New Nanophotonic Applications (Advanced Optical Materials 18/2019).,Remi Veneziano,Advanced Optical Materials,21951071,,9/18/2019,7,18,N.PAG,1.0,138689676,10.1002/adom.201970067,Wiley-Blackwell,Article,OPTICAL materials; DNA nanotechnology; NANOPHOTONICS; FLUORESCENCE resonance energy transfer; DNA; SURFACE enhanced Raman effect,chiral properties; DNA scaffolds; Förster resonance energy transfer; metamaterials; nanoscale optical devices; optically active molecules; plasmonic nanomaterials,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138689676&site=ehost-live
657,Rapid DNA origami nanostructure detection and classification using the YOLOv5 deep convolutional neural network.,Qi Wei,Scientific Reports,20452322,,3/9/2022,12,1,1,13.0,155683431,10.1038/s41598-022-07759-3,Springer Nature,Article,CONVOLUTIONAL neural networks; DNA folding; DNA structure; DNA nanotechnology; ATOMIC force microscopes,,"The intra-image identification of DNA structures is essential to rapid prototyping and quality control of self-assembled DNA origami scaffold systems. We postulate that the YOLO modern object detection platform commonly used for facial recognition can be applied to rapidly scour atomic force microscope (AFM) images for identifying correctly formed DNA nanostructures with high fidelity. To make this approach widely available, we use open-source software and provide a straightforward procedure for designing a tailored, intelligent identification platform which can easily be repurposed to fit arbitrary structural geometries beyond AFM images of DNA structures. Here, we describe methods to acquire and generate the necessary components to create this robust system. Beginning with DNA structure design, we detail AFM imaging, data point annotation, data augmentation, model training, and inference. To demonstrate the adaptability of this system, we assembled two distinct DNA origami architectures (triangles and breadboards) for detection in raw AFM images. Using the images acquired of each structure, we trained two separate single class object identification models unique to each architecture. By applying these models in sequence, we correctly identified 3470 structures from a total population of 3617 using images that sometimes included a third DNA origami structure as well as other impurities. Analysis was completed in under 20 s with results yielding an F1 score of 0.96 using our approach. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155683431&site=ehost-live
657,Rapid DNA origami nanostructure detection and classification using the YOLOv5 deep convolutional neural network.,Remi Veneziano,Scientific Reports,20452322,,3/9/2022,12,1,1,13.0,155683431,10.1038/s41598-022-07759-3,Springer Nature,Article,CONVOLUTIONAL neural networks; DNA folding; DNA structure; DNA nanotechnology; ATOMIC force microscopes,,"The intra-image identification of DNA structures is essential to rapid prototyping and quality control of self-assembled DNA origami scaffold systems. We postulate that the YOLO modern object detection platform commonly used for facial recognition can be applied to rapidly scour atomic force microscope (AFM) images for identifying correctly formed DNA nanostructures with high fidelity. To make this approach widely available, we use open-source software and provide a straightforward procedure for designing a tailored, intelligent identification platform which can easily be repurposed to fit arbitrary structural geometries beyond AFM images of DNA structures. Here, we describe methods to acquire and generate the necessary components to create this robust system. Beginning with DNA structure design, we detail AFM imaging, data point annotation, data augmentation, model training, and inference. To demonstrate the adaptability of this system, we assembled two distinct DNA origami architectures (triangles and breadboards) for detection in raw AFM images. Using the images acquired of each structure, we trained two separate single class object identification models unique to each architecture. By applying these models in sequence, we correctly identified 3470 structures from a total population of 3617 using images that sometimes included a third DNA origami structure as well as other impurities. Analysis was completed in under 20 s with results yielding an F1 score of 0.96 using our approach. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=155683431&site=ehost-live
658,Synthesis of DNA Origami Scaffolds: Current and Emerging Strategies.,Remi Veneziano,Molecules,14203049,,Aug2020,25,15,3386,1.0,144990075,10.3390/molecules25153386,MDPI,Article,DNA folding; DNA synthesis; SINGLE-stranded DNA; CIRCULAR DNA; CANCER treatment; DNA nanotechnology,DNA amplification; DNA origami; DNA scaffolds; DNA Synthesis; nucleic acid nanoparticles; single-stranded DNA,"DNA origami nanocarriers have emerged as a promising tool for many biomedical applications, such as biosensing, targeted drug delivery, and cancer immunotherapy. These highly programmable nanoarchitectures are assembled into any shape or size with nanoscale precision by folding a single-stranded DNA scaffold with short complementary oligonucleotides. The standard scaffold strand used to fold DNA origami nanocarriers is usually the M13mp18 bacteriophage's circular single-stranded DNA genome with limited design flexibility in terms of the sequence and size of the final objects. However, with the recent progress in automated DNA origami design—allowing for increasing structural complexity—and the growing number of applications, the need for scalable methods to produce custom scaffolds has become crucial to overcome the limitations of traditional methods for scaffold production. Improved scaffold synthesis strategies will help to broaden the use of DNA origami for more biomedical applications. To this end, several techniques have been developed in recent years for the scalable synthesis of single stranded DNA scaffolds with custom lengths and sequences. This review focuses on these methods and the progress that has been made to address the challenges confronting custom scaffold production for large-scale DNA origami assembly. [ABSTRACT FROM AUTHOR] Copyright of Molecules is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=144990075&site=ehost-live
659,Utilizing the Organizational Power of DNA Scaffolds for New Nanophotonic Applications.,Remi Veneziano,Advanced Optical Materials,21951071,,9/18/2019,7,18,N.PAG,1.0,138689669,10.1002/adom.201900562,Wiley-Blackwell,Article,FLUORESCENCE resonance energy transfer; NANOWIRES; DNA structure; DNA; NANOELECTROMECHANICAL systems; SOFT errors,chiral properties; DNA scaffolds; Förster resonance energy transfer; metamaterials; nanoscale optical devices; optically active molecules; plasmonic nanomaterials,"Rapid development of DNA technology has provided a feasible route to creating nanoscale materials. DNA acts as a self‐assembled nanoscaffold capable of assuming any three‐dimensional shape. The ability to integrate dyes and new optical materials such as quantum dots and plasmonic nanoparticles precisely onto these architectures provides new ways to exploit their near‐ and far‐field interactions. A fundamental understanding of these optical processes will help drive development of next‐generation photonic nanomaterials. This review is focused on latest progress in DNA‐based photonic materials and highlights DNA scaffolds for rapidly assembling and prototyping nanoscale optical devices. Three areas are discussed including intrinsically active DNA structures displaying chiral properties, DNA scaffolds hosting plasmonic nanomaterials, and fluorophore‐labeled DNAs that engage in Förster resonance energy transfer and give rise to complex molecular photonic wires. An explanation of what is desired from these optical processes when harnessed sets the tone for what DNA scaffolds are providing toward each focus. Examples from the literature illustrate current progress along with a discussion of challenges to overcome for further improvements. Opportunities to integrate diverse classes of optically active molecules including light‐generating enzymes, fluorescent proteins, nanoclusters, and metal–chelates in new structural combinations on DNA scaffolds are also highlighted. [ABSTRACT FROM AUTHOR] Copyright of Advanced Optical Materials is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138689669&site=ehost-live
660,Deriving performance measures for transportation planning using ITS archived data.,Mohan Venigalla,Civil Engineering & Environmental Systems,10286608,,Sep2005,22,3,171,18.0,18926515,10.1080/10286600500279998,Taylor & Francis Ltd,Article,INTELLIGENT transportation systems; ELECTRONICS in transportation; TRANSPORTATION; TRAFFIC flow; TRAFFIC surveys; DATABASES; ELECTRONIC information resources; Other support activities for transportation; All Other Support Activities for Transportation,Air quality; Archived data management system; Archived data management systems; Intelligent transportation systems; Transportation planning,"Various modern sensor technologies deployed under the auspices of intelligent transportation systems (ITS) for data collection and archiving have helped in accumulating a wealth of transportation data in the form of data archives. Archiving of transportation data obtained from intelligent sources is practiced in most parts of the US under the auspices of the states' departments of transportation. However, recently there is a shift in the focus of archived data management systems (ADMS) from data collection and archiving to data analysis and distribution to stakeholders. This article discusses the use of archived ITS data for the development of performance measures for transportation planning and air quality support service. This service is packaged within a larger ADMS effort called Traffic Management Centers (TMC) Applications of Archived Data, which is also known by its working title ‘ADMS Virginia’. Nine sub-services for computing various performance measures at different spatial and temporal levels of aggregation are available within the transportation planning and air quality service. The nine performance measures provided are traffic speed, volume, density, vehicle miles traveled (VMT), percent VMT by time of day, travel time, volume-to-capacity ratio, peak hour factor, and average daily traffic (ADT). The service integrates a subset of a regional transportation planning network with the traffic flow data in the archived databases. The performance measures developed in this study have a broad spectrum of uses ranging from long- and short-range transportation planning, transportation system monitoring, regional air quality monitoring and air quality conformity, development of forecasting and simulation models, and establishment of growth impact policies. The concepts of operations of this system are discussed along with functional requirements, data model, and algorithms for deriving the performance measures. The methodology and procedures discussed in this article are portable and can easily be adopted by other ADMS efforts. [ABSTRACT FROM AUTHOR] Copyright of Civil Engineering & Environmental Systems is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18926515&site=ehost-live
662,Innovations in Geographic Information Systems Applications for Civil Engineering.,Mohan Venigalla,Journal of Computing in Civil Engineering,08873801,,Nov2006,20,6,375,2.0,22741548,10.1061/(ASCE)0887-3801(2006)20:6(375),American Society of Civil Engineers,Article,GEOGRAPHIC information systems; CIVIL engineering; ENGINEERING; TRANSPORTATION; ENVIRONMENTAL engineering; WATER supply; Engineering Services; All Other Support Activities for Transportation; Other support activities for transportation; Water Supply and Irrigation Systems,,"The article focuses on the application of geographic information systems (GIS) for civil engineering. The article discusses GIS-based solutions for engineering problems such as transportation, environmental and water resources. It also presents topics on engineering for submission to the journal such as infrastructure security.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=22741548&site=ehost-live
674,Serum Strongylus vulgaris-specific antibody responses to anthelmintic treatment in naturally infected horses.,Anand Vidyashankar,Parasitology Research,09320113,,Feb2015,114,2,445,7.0,100576391,10.1007/s00436-014-4201-5,Springer Nature,Article,HELMINTHS; PARASITES; ENDARTERITIS; THROMBOEMBOLISM; INFARCTION; ENZYME-linked immunosorbent assay,ELISA; Ivermectin; Pyrantel tartrate; Strongylus vulgaris; SvSXP,"Strongylus vulgaris is the most pathogenic helminth parasite of horses, causing verminous endarteritis with thromboembolism and infarction. A serum enzyme-linked immunosorbent assay (ELISA) has been validated for detection of antibodies to an antigen produced by migrating larvae of this parasite. The aim was to evaluate ELISA responses to anthelmintic treatment in cohorts of naturally infected horses. Fifteen healthy horses harboring patent S. vulgaris infections were turned out for communal grazing in May 2013 (day 0). On day 55, horses were ranked according to ELISA titers and randomly allocated to the following three groups: no treatment followed by placebo pellets daily; ivermectin on day 60 followed by placebo pellets daily; or ivermectin on day 60 followed by daily pyrantel tartrate. Fecal and serum samples were collected at ∼28-day intervals until study termination on day 231. Increased ELISA values were observed for the first 53 days following ivermectin treatment. Titers were significantly reduced 80 days after ivermectin treatment. Horses receiving daily pyrantel tartrate maintained lower ELISA values from 137 days post ivermectin treatment until trial termination. These results illustrate that a positive ELISA result is indicative of either current or prior exposure to larval S. vulgaris infection within the previous 5 months. [ABSTRACT FROM AUTHOR] Copyright of Parasitology Research is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100576391&site=ehost-live
684,High-throughput profiling of histone post-translational modifications and chromatin modifying proteins by reverse phase protein array.,Xuan Wang,Journal of Proteomics,18743919,,Jun2022,262,,N.PAG,1.0,156999945,10.1016/j.jprot.2022.104596,Elsevier B.V.,Article,PROTEIN microarrays; POST-translational modification; HUMAN biology; DEVELOPMENTAL biology; CELL culture; PLURIPOTENT stem cells; HISTONES,Breast cancer; Epigenetics; High-throughput; Induced pluripotent stem cells; Post-translational modifications; RPPA,"Epigenetic variation plays a significant role in normal development and human diseases including cancer, in part through post-translational modifications (PTMs) of histones. Identification and profiling of changes in histone PTMs, and in proteins regulating PTMs, are crucial to understanding diseases, and for discovery of epigenetic therapeutic agents. In this study, we have adapted and validated an antibody-based reverse phase protein array (RPPA) platform for profiling 20 histone PTMs and expression of 40 proteins that modify histones and other epigenomic regulators. The specificity of the RPPA assay for histone PTMs was validated with synthetic peptides corresponding to histone PTMs and by detection of histone PTM changes in response to inhibitors of histone modifier proteins in cell cultures. The useful application of the RPPA platform was demonstrated with two models: induction of pluripotent stem cells and a mouse mammary tumor progression model. Described here is a robust platform that includes a rapid microscale method for histone isolation and partially automated workflows for analysis of histone PTMs and histone modifiers that can be performed in a high-throughput manner with hundreds of samples. This RPPA platform has potential for translational applications through the discovery and validation of epigenetic states as therapeutic targets and biomarkers. Our study has established an antibody-based reverse phase protein array platform for global profiling of a wide range of post-translational modifications of histones and histone modifier proteins. The high-throughput platform provides comprehensive analyses of epigenetics for biological research and disease studies and may serve as screening assay for diagnostic purpose or therapy development. [Display omitted] • Reverse Phase Protein Array (RPPA) was adapted to profile histone PTMs. • RPPA simultaneously profiles expression level of proteins modifying histones. • RPPA is reproducible, high-throughput, and scalable for all histone modifications. • RPPA provides a valuable tool for study of developmental biology and human disease. [ABSTRACT FROM AUTHOR] Copyright of Journal of Proteomics is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156999945&site=ehost-live
685,Adaptive modulation and coding in underwater acoustic communications: a machine learning perspective.,Yue Wang,EURASIP Journal on Wireless Communications & Networking,16871472,,10/17/2020,2020,1,N.PAG,1.0,146494374,10.1186/s13638-020-01818-x,Springer Nature,Article,UNDERWATER acoustic communication; ADAPTIVE modulation; MODULATION coding; WIRELESS channels; MACHINE learning; MARINE resources,Adaptive modulation and coding (AMC); Harsh oceanic environment; Machine learning (ML); Underwater acoustic communication (UAC),"The increasing demand for exploring and managing the vast marine resources of the planet has underscored the importance of research on advanced underwater acoustic communication (UAC) technologies. However, owing to the severe characteristics of the oceanic environment, underwater acoustic (UWA) propagation experiences nearly the harshest wireless channels in nature. This article resorts to the perspective of machine learning (ML) to cope with the major challenges of adaptive modulation and coding (AMC) design in UACs. First, we present an ML AMC framework for UACs. Then, we propose an attention-aided k-nearest neighbor (A-kNN) algorithm with simplicity and robustness, based on which an ML AMC approach is designed with immunity to channel modeling uncertainty. Leveraging its online learning ability, such A-kNN-based AMC classifier offers salient capabilities of both sustainable self-enhancement and broad applicability to various operation scenarios. Next, aiming at higher implementation efficiency, we take strategies of complexity reduction and present a dimensionality-reduced and data-clustered A-kNN (DRDC-A-kNN) AMC classifier. Finally, we demonstrate that these proposed ML approaches have superior performance over traditional model-based methods by simulations using actual data collected from three lake experiments. [ABSTRACT FROM AUTHOR] Copyright of EURASIP Journal on Wireless Communications & Networking is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146494374&site=ehost-live
686,Efficient two-dimensional line spectrum estimation based on decoupled atomic norm minimization.,Yue Wang,Signal Processing,01651684,,Oct2019,163,,95,12.0,136840664,10.1016/j.sigpro.2019.04.024,Elsevier B.V.,Article,TOEPLITZ matrices; RADAR signal processing; RADIO astronomy; MAGNITUDE (Mathematics); MATHEMATICAL optimization; COMPUTATIONAL complexity,Atomic norm minimization; Decoupled ANM; Line spectrum estimation; Semi-definite programming; Two-dimensional,"This paper presents an efficient optimization technique for gridless 2-D line spectrum estimation, named decoupled atomic norm minimization (D-ANM). The framework of atomic norm minimization (ANM) is considered, which has been successfully applied in 1-D problems to allow super-resolution frequency estimation for correlated sources even when the number of snapshots is highly limited. The state-of-the-art 2-D ANM approach vectorizes the 2-D measurements to their 1-D equivalence, which incurs huge computational cost and may become too costly for practical applications. We develop a novel decoupled approach of 2-D ANM via semi-definite programming (SDP), which introduces a new matrix-form atom set to naturally decouple the joint observations in both dimensions without loss of optimality. Accordingly, the original large-scale 2-D problem is equivalently reformulated via two decoupled one-level Toeplitz matrices, which can be solved by simple 1-D frequency estimation with pairing. Compared with the conventional vectorized approach, the proposed D-ANM technique reduces the computational complexity by several orders of magnitude with respect to the problem size, at no loss of optimality. It also retains the benefits of ANM in terms of precise signal recovery, small number of required measurements, and robustness to source correlation. The complexity benefits are particularly attractive for large-scale antenna systems such as massive MIMO, radar signal processing and radio astronomy. [ABSTRACT FROM AUTHOR] Copyright of Signal Processing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136840664&site=ehost-live
687,Mice lacking the transcriptional regulator Bhlhe40 have enhanced neuronal excitability and impaired synaptic plasticity in the hippocampus.,Yue Wang,PLoS ONE,19326203,,1/5/2018,13,5,1,22.0,129375500,10.1371/journal.pone.0196223,Public Library of Science,Article,TRANSCRIPTION factors; NEUROPLASTICITY; HIPPOCAMPUS physiology; EXCITATORY postsynaptic potential; DNA-protein interactions,Amniotes; Anatomy; Animal genomics; Animal models; Animals; Biochemistry; Biology and life sciences; Brain; Cellular neuroscience; Developmental neuroscience; Diabetic endocrinology; Endocrinology; Eukaryota; Experimental organism systems; Gene expression; Genetics; Genomics; Hippocampus; Hormones; Insulin; Mammalian genomics; Mammals; Medicine and health sciences; Mice; Model organisms; Mouse models; Neuronal plasticity; Neuroscience; Organisms; Research and analysis methods; Research Article; Rodents; Synaptic plasticity; Vertebrates,"Bhlhe40 is a transcription factor that is highly expressed in the hippocampus; however, its role in neuronal function is not well understood. Here, we used Bhlhe40 null mice on a congenic C57Bl6/J background (Bhlhe40 KO) to investigate the impact of Bhlhe40 on neuronal excitability and synaptic plasticity in the hippocampus. Bhlhe40 KO CA1 neurons had increased miniature excitatory post-synaptic current amplitude and decreased inhibitory post-synaptic current amplitude, indicating CA1 neuronal hyperexcitability. Increased CA1 neuronal excitability was not associated with increased seizure severity as Bhlhe40 KO relative to +/+ (WT) control mice injected with the convulsant kainic acid. However, significant reductions in long term potentiation and long term depression at CA1 synapses were observed in Bhlhe40 KO mice, indicating impaired hippocampal synaptic plasticity. Behavioral testing for spatial learning and memory on the Morris Water Maze (MWM) revealed that while Bhlhe40 KO mice performed similarly to WT controls initially, when the hidden platform was moved to the opposite quadrant Bhlhe40 KO mice showed impairments in relearning, consistent with decreased hippocampal synaptic plasticity. To investigate possible mechanisms for increased neuronal excitability and decreased synaptic plasticity, a whole genome mRNA expression profile of Bhlhe40 KO hippocampus was performed followed by a chromatin immunoprecipitation sequencing (ChIP-Seq) screen of the validated candidate genes for Bhlhe40 protein-DNA interactions consistent with transcriptional regulation. Of the validated genes identified from mRNA expression analysis, insulin degrading enzyme (Ide) had the most significantly altered expression in hippocampus and was significantly downregulated on the RNA and protein levels; although Bhlhe40 did not occupy the Ide gene by ChIP-Seq. Together, these findings support a role for Bhlhe40 in regulating neuronal excitability and synaptic plasticity in the hippocampus and that indirect regulation of Ide transcription may be involved in these phenotypes. [ABSTRACT FROM AUTHOR] Copyright of PLoS ONE is the property of Public Library of Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=129375500&site=ehost-live
688,Performance bounds of compressive classification under perturbation.,Yue Wang,Signal Processing,01651684,,Mar2021,180,,N.PAG,1.0,147405032,10.1016/j.sigpro.2020.107855,Elsevier B.V.,Article,CLASSIFICATION; MOTIVATION (Psychology); GAUSSIAN distribution,Classification; Compressive sensing; Performance bound; Perturbation,"• The upper bound and the lower bound on the possibility of misclassification of the compressive classification under measurement perturbation are derived. • Our analysis reveals that the performance of compressive classification is dependent on the number of measurements, the variance of the noise, and the measurement perturbation. • When the measurement perturbations for different hypotheses are different, the performance of compressive classification is better than the one with the same measurement perturbation. • We propose an improved sparse representation classification framework, which models the perturbation factor in the training samples as the perturbation term in the dictionary. Recently, compressive sensing based classification, which is called compressive classification, has drawn a lot of attention, since it works directly in the compressive domain with low complexity. However, existing literatures assume perfectly known measurement matrix during compressive classification, which is impossible in many practical situations. In this paper, we focus on studying the performance of classification based on the compressive measurements under the perturbation, where the perturbation models the uncertainty of the measurement matrix. The upper and the lower bounds on the probability of misclassification of the compressive classification are evaluated by utilizing the Kullback-Leibler and Chernoff distances. Our results indicate that the performance depends on the variance of the perturbation when the perturbation obeys Gaussian distribution with zero mean. Moreover, compared with the one without perturbation, the performance of the compressive classification can be improved when the perturbation in each hypothesis is different from each other. Motivated by these observations, we design an improved sparse representation classification (SRC) framework by incorporating the perturbation item into the SRC framework and propose several enhanced SRC schemes for performance improvement. The experiments on MNIST datasets validate that the proposed SRC schemes outperform the existing standard SRC scheme. [ABSTRACT FROM AUTHOR] Copyright of Signal Processing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147405032&site=ehost-live
689,Performance limits of one-bit compressive classification.,Yue Wang,Signal Processing,01651684,,Jan2021,178,,N.PAG,1.0,146398367,10.1016/j.sigpro.2020.107808,Elsevier B.V.,Article,PROBABILITY density function; SIGNAL classification; SIGNAL processing; GAUSSIAN distribution,Compressive classification (CC); Compressive sensing (CS); One-bit CC; One-bit CS; Performance limits,"• When the signals are not necessarily sparse, we derive the upper bound on the probability of misclassification in the high-dimensional setting. • For the signals that are not necessarily sparse, we derive the lower bound on the probability of misclassification in the high-dimensional setting. • For high-dimensional sparse signals, we derive the performance limits on the probability of misclassification when the nonzero entries follow the first order Gaussian distribution. Classification is an important task in the fields of signal processing and machine learning. Recently, compressive classification (CC) appears to enable signal classification directly with the compressive measurements, which are acquired by the technique of compressive sensing (CS). However, the existing works of CC ignore the practical quantization operation. This paper studies the one-bit CC based on one-bit quantized compressive measurements, which is appealing in saving the transmission and storage bits in many applications when the bandwidth and energy are constrained. For the signals that are not necessarily sparse as well as exactly sparse, we provide the performance limits of one-bit CC. We first analyze the Chernoff distance and Kullback–Leibler (KL) distance between two probability density functions under any two hypotheses. The results are then used to derive the upper and lower bounds on the probability of misclassification in the high-dimensional setting. The analytical results show that one-bit quantized CC has the same performance trend as conventional not-quantized CC when the number of measurements increases. [ABSTRACT FROM AUTHOR] Copyright of Signal Processing is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=146398367&site=ehost-live
690,A Martingale Framework for Detecting Changes in Data Streams by Testing Exchangeability.,Harry Wechsler,IEEE Transactions on Pattern Analysis & Machine Intelligence,01628828,,Dec2010,32,12,2113,0.0,54885885,10.1109/TPAMI.2010.48,IEEE,Article,,Change detection; classification; clustering; Data models; data stream; exchangeability; Games; hypothesis testing; Manganese; martingale; Random variables; regression; support vector machine; Support vector machines; Testing; Training,"In a data streaming setting, data points are observed sequentially. The data generating model may change as the data are streaming. In this paper, we propose detecting this change in data streams by testing the exchangeability property of the observed data. Our martingale approach is an efficient, nonparametric, one-pass algorithm that is effective on the classification, cluster, and regression data generating models. Experimental results show the feasibility and effectiveness of the martingale methodology in detecting changes in the data generating model for time-varying data streams. Moreover, we also show that: 1) An adaptive support vector machine (SVM) utilizing the martingale methodology compares favorably against an adaptive SVM utilizing a sliding window, and 2) a multiple martingale video-shot change detector compares favorably against standard shot-change detection algorithms. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=54885885&site=ehost-live
691,Biometric Security and Privacy Using Smart Identity Management and Interoperability: Validation and Vulnerabilities of Various Techniques.,Harry Wechsler,Review of Policy Research,1541132X,,Jan2012,29,1,63,24.0,70469404,10.1111/j.1541-1338.2011.00538.x,Wiley-Blackwell,Article,ETHICS; BIOMETRIC identification; DATA mining; RIGHT of privacy; ANONYMITY; IDENTIFICATION; FORENSIC sciences; RELIABILITY (Personality trait); MASS surveillance,,"The central position of this article is that validation and interoperability are paramount for the effective and ethical use of biometrics. Illuminating the relevance for policymakers of the science underlying the security and privacy aspects of biometrics, this article calls for adequate and enforceable performance metrics that can be independently corroborated. Accordingly, the article considers biometrics and forensics for the dual challenges of addressing security and privacy using smart identity management. The discussion revolves around the concepts of 'personally identifiable information' (PII) and interoperability with emphasis on quantitative performance analysis and validation for uncontrolled operational settings, variable demographics, and distributed and federated operations. Validation metrics includes expected rates of identification/misidentification, precision, and recall. The complementary concepts of identity and anonymity are addressed in terms of expected performance, functionality, law and ethics, forensics, and statistical learning. Biometrics encompasses appearance, behavior, and cognitive state or intent. Modes of deployment and performance evaluation for biometrics are detailed, with operational and adversarial challenges for both security and privacy described in terms of trustworthiness, vulnerabilities, functional creep, and feasibility of safeguards. The article underscores how lack of interoperability is mostly due to overfitting and tuning to well-controlled settings, so that validation merely confirms 'teaching to the test' rather than preparation for real-world deployment. Most important for validation is reproducibility of results including full information on the experimental design used, that forensic exclusion is allowed, and that scientific methods for performance evaluation are followed. The article concludes with expected developments regarding technology use and advancements that bear on security and privacy, including data streams and video, de-anonymization and reidentification, social media analytics and cyber security, and smart camera networks and surveillance. [ABSTRACT FROM AUTHOR] Copyright of Review of Policy Research is the property of Wiley-Blackwell and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=70469404&site=ehost-live
692,Biometric surveillance using visual question answering.,Harry Wechsler,Pattern Recognition Letters,01678655,,Sep2019,126,,111,8.0,138726453,10.1016/j.patrec.2018.02.013,Elsevier B.V.,Article,EYE; QUESTIONING; TURING test; QUERY (Information retrieval system); VIDEO compression,Biometrics; Deep learning; Forensics; Question relevance; Surveillance; Visual question answering; Visual turing test,"• Novel biometric surveillance system based on Visual Question Answering. • Reuse of pre-trained components without fine tuning to demonstrate system robustness. • Introduction of novel models for biometric-focused question image relevance. • New datasets for biometric-based surveillance tasks. Surveillance of individuals using visual data requires human-level capabilities for understanding the characteristics that differentiate one person from another. However, because the influx of both video and imagery is increasing at a greater rate than humans can cope with, biometric-based surveillance systems are required to assist with the triage of information based on human-generated queries. Unfortunately, current systems are not robust enough to tackle new tasks, as they involve specialized models that do not leverage existing, pre-trained components. To mitigate these issues, we propose a novel system for biometric-based surveillance that utilizes models that are relevance-aware to triage images and videos based on interaction with single or multiple users. As the system is initially focused on detection of people via their appearance and clothing, we have named the system Context and Collaborative (C2) Visual Question Answering (VQA) for Biometric Object-Attribute Relevance and Surveillance (C2VQA-BOARS). To validate the usefulness of C2VQA-BOARS in real-world scenarios, we provide an implementation of two novel components (Relevance and Triage) and apply them in tasks against two datasets created for biometric surveillance. Our results outperform baseline approaches, proving that a system with a minimal amount of fine-tuned components can robustly handle new datasets and problems as needed. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138726453&site=ehost-live
694,Eye movement analysis for human authentication: a critical survey.,Harry Wechsler,Pattern Recognition Letters,01678655,,Dec2016,84,,272,12.0,119777393,10.1016/j.patrec.2016.11.002,Elsevier B.V.,Article,EYE tracking; BIOMETRIC identification; INDIVIDUALIZED medicine; MEDICAL care; PERFORMANCE evaluation,Active and dynamic biometrics; Eye movements; GANT; Gaze analysis; Gaze scan paths; HCI; Identity management; Performance evaluation; Uncontrolled settings,"This paper addresses the active and dynamic nature of biometrics, in general, and gaze analysis, in particular, including motivation and background. The paper includes a critical survey of existing gaze analysis methods, challenges due to uncontrolled settings and lack of standards, and outlines promising future R&D directions. Criteria for performance evaluation are proposed, and state-of-the art gaze analysis methods are compared on the same database set. Performance improvement would come from richer stimuli including task dependent user profiles, with applications going much beyond identity management to include personalized medical care and rehabilitation, privacy, marketing, and education. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119777393&site=ehost-live
696,Linguistics and face recognition,Harry Wechsler,Journal of Visual Languages & Computing,1045926X,,Jun2009,20,3,145,11.0,39776716,10.1016/j.jvlc.2009.01.001,Academic Press Inc.,Article,FACE perception; VISUAL perception; BIOMETRY; SEMANTICS,Authentication; Biometrics; Boosting; Clustering; Cross-validation; Data fusion; Face recognition; Feature selection; FERET; Forensics; FRGC; ICA; k Nearest neighbor; Likelihood ratio; Linguistics; Margin; MDL; Multimodal integration; Neyman–Pearson; Occlusion; p-Values; Parsing; Random deficiency; Ranking; Recognition; Recognition-by-parts; Segmentation; SIFT; Strangeness; Surveillance; Transduction; Typicality,"Abstract: We describe in this paper a novel biometric methodology for face recognition suitable to address pose, illumination, and expression (PIE) image variability, temporal change, flexible matching, and last but not least occlusion and disguise that are usually referred to as denial and deception. The adverse conditions listed above affect the scope and performance of biometric analysis vis-à-vis both training and testing. The conceptual framework proposed here draws support from discriminative methods using likelihood ratios. At the conceptual level it links forensics and biometrics, while at the implementation level it links the Bayesian framework and statistical learning theory. As many of the concerns listed usually affect only parts of the face, a non-parametric recognition-by-part approach is advanced here for the purpose of reliable face recognition. Recognition-by-parts facilitates authentication because it does not seek for explicit invariance. Instead, it handles variability using component-based configurations that are flexible enough to compensate among others for limited pose changes, if any, and limited occlusion and disguise. The recognition-by-parts approach proposed here supports incremental and progressive processing. It is similar in nature to modern linguistics and practical intelligence with the emphasis on semantics and pragmatics. Layered categorization starts with face detection using implicit rather than explicit segmentation. It proceeds with face authentication that involves feature selection of local patch instances including dimensionality reduction, exemplar-based clustering of patches into parts, and data fusion for matching using boosting driven by parts that play the role of weak learners. The implementation, driven by transduction, employs proximity and typicality (ranking) realized using strangeness and random deficiency p-values, respectively. The feasibility and reliability of the proposed architecture has been validated using FERET and FRGC data. The paper concludes with suggestions for augmenting and enhancing the scope and utility of the recognition-by-parts architecture. [Copyright &y& Elsevier] Copyright of Journal of Visual Languages & Computing is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=39776716&site=ehost-live
697,"Micro-Doppler Effect in Radar: Phenomenon, Model, and Simulation Study.",Harry Wechsler,IEEE Transactions on Aerospace & Electronic Systems,00189251,,Jan2006,42,1,2,20.0,20383039,10.1109/TAES.2006.1603402,IEEE,Article,DOPPLER radar; DOPPLER effect; RADAR targets; ELECTRONIC modulation; ELECTRONIC pulse techniques; RADAR simulation,,"When, in addition to the constant Doppler frequency shift induced by the bulk motion of a radar target, the target or any structure on the target undergoes micro-motion dynamics, such as mechanical vibrations or rotations, the micro-motion dynamics induce Doppler modulations on the returned signal, referred to as the micro-Doppler effect. We introduce the micro-Doppler phenomenon in radar, develop a model of Doppler modulations, derive formulas of micro-Doppler induced by targets with vibration, rotation, tumbling and coning motions, and verify them by simulation studies, analyze time-varying micro-Doppler features using high-resolution time-frequency transforms, and demonstrate the micro-Doppler effect observed in real radar data. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Aerospace & Electronic Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=20383039&site=ehost-live
698,"Mobile Iris Challenge Evaluation (MICHE)-I, biometric iris dataset and protocols.",Harry Wechsler,Pattern Recognition Letters,01678655,,May2015,57,,17,7.0,102208391,10.1016/j.patrec.2015.02.009,Elsevier B.V.,Article,BIOMETRIC identification; DATABASES; COMPUTER network protocols; COMPUTER simulation; IRIS recognition,Iris biometric; Iris challenge; Mobile devices,"We introduce and describe here MICHE-I, a new iris biometric dataset captured under uncontrolled settings using mobile devices. The key features of the MICHE-I dataset are a wide and diverse population of subjects, the use of different mobile devices for iris acquisition, realistic simulation of the acquisition process (including noise), several data capture sessions separated in time, and image annotation using metadata. The aim of MICHE-I dataset is to make up the starting core of a wider dataset that we plan to collect, with the further aim to address interoperability, both in the sense of matching samples acquired with different devices and of assessing the robustness of algorithms to the use of devices with different characteristics. We discuss throughout the merits of MICHE-I with regard to biometric dimensions of interest including uncontrolled settings, demographics, interoperability, and real-world applications. We also consider the potential for MICHE-I to assist with developing continuous authentication aimed to counter adversarial spoofing and impersonation, when the bar for uncontrolled settings raises even higher for proper and effective defensive measures. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=102208391&site=ehost-live
699,Modern art challenges face detection.,Harry Wechsler,Pattern Recognition Letters,01678655,,Sep2019,126,,3,8.0,138726454,10.1016/j.patrec.2018.02.014,Elsevier B.V.,Article,MODERN art; POP art; HUMAN facial recognition software; TURING test; COMPUTER vision; FUSIFORM gyrus,Biometrics; Face detection; Forensics; Interoperability; Modern art; Visual Turing test,"• Creation of a new face detection dataset (MAFD-150) based on modern art. • Describes face detection on modern art as a challenging task for Visual Turing test. • Analysis of the characteristics of modern art that make it challenging for algorithms. • Challenge to claims of ""human-level performance"" on face detection and recognition. • Performance evaluation for face detection on modern art. There is a widely held belief that computer vision, in general, and face authentication, in particular, are to a large extent solved problems. This paper challenges this belief regarding face authentication using examples from modern art that significantly confound face detection. The challenges are made concrete using a new MAFD-150 dataset (M odern A rt F ace D etection) composed mostly of modern art examples that cover much diversity in style and artists. MAFD-150 challenges the belief that singleton and crowd face detection is an almost solved problem, and provides baselines and preliminary results that highlight the inadequacy of current expertise and methods to address face detection. In particular, we show that well-known face detection algorithms are only able to achieve an F1 score of less than 35% overall across the new dataset. Additionally, we discuss the performance of the selected face detectors on varying art categories (such as Impressionism, Pop Art, et al.) to show how style and face representation may impact these algorithms. The paper concludes with suggestions on how to advance face processing by leveraging the complementarity between Show-and-Tell-like methods and a context and cooperative driven visual question answering framework using relevance-based triage. The very challenges detailed throughout are then shown to be helpful with developing novel, robust, and secure access protocols that combine text and modern art images using the visual question answering framework. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=138726454&site=ehost-live
700,Motion Estimation Using Statistical Learning Theory.,Harry Wechsler,IEEE Transactions on Pattern Analysis & Machine Intelligence,01628828,,Apr2004,26,4,466,13.0,12798701,10.1109/TPAMI.2004.1265862,IEEE,Article,MOTION; THEORY; LEARNING; STATISTICS; MATHEMATICAL models; IMAGING systems; Photographic and Photocopying Equipment Manufacturing; Computer Terminal and Other Computer Peripheral Equipment Manufacturing,,"This paper describes a novel application of Statistical Learning Theory (SLT) to single motion estimation and tracking. The problem of motion estimation can be related to statistical model selection, where the goal is to select one (correct) motion model from several possible motion models, given finite noisy samples. SLT, also known as Vapnik-Chervonenkis (VC), theory provides analytic generalization bounds for model selection, which have been used successfully for practical model selection. This paper describes a successful application of an SLT-based model selection approach to the challenging problem of estimating optimal motion models from small data sets of image measurements (flow). We present results of experiments on both synthetic and real image sequences for motion interpolation and extrapolation; these results demonstrate the feasibility and strength of our approach. Our experimental results show that for motion estimation applications, SLT-based model selection compares favorably against alternative model selection methods, such as the Akaike's fpe, Schwartz' criterion (sc), Generalized Cross-Validation (gcv), and Shibata's Model Selector (sms). The paper also shows how to address the aperture problem using SLT-based model selection for penalized linear (ridge regression) formulation. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=12798701&site=ehost-live
701,Open Set Face Recognition Using Transduction.,Harry Wechsler,IEEE Transactions on Pattern Analysis & Machine Intelligence,01628828,,Nov2005,27,11,1686,12.0,18530864,10.1109/TPAMI.2005.224,IEEE,Article,"BIOMETRY; KOLMOGOROV complexity; NUMERICAL analysis; MATHEMATICAL statistics; ELECTRONIC data processing; IDENTIFICATION documents; Data Processing, Hosting, and Related Services",(multiclass) transduction; Biometrics; clustering; confidence; credibility; data fusion; face recognition; face surveillance; information quality; Kolmogorov complexity; open set recognition; outlier detection; performance evaluation; PSEI (pattern specific error inhomogeneities); randomness deficiency; strangeness; watch list,"This paper motivates and describes a novel realization of transductive inference that can address the Open Set face recognition task. Open Set operates under the assumption that not all the test probes have mates in the gallery. It either detects the presence of some biometric signature within the gallery and finds its identity or rejects it, i.e., it provides for the ""none of the above"" answer. The main contribution of the paper is Open Set TCM-kNN (Transduction Confidence Machine-k Nearest Neighbors), which is suitable for multiclass authentication operational scenarios that have to include a rejection option for classes never enrolled in the gallery. Open Set TCM-kNN, driven by the relation between transduction and Kolmogorov complexity, provides a local estimation of the likelihood ratio needed for detection tasks. We provide extensive experimental data to show the feasibility, robustness, and comparative advantages of Open Set TCM-kNN on Open Set identification and watch list (surveillance) tasks using challenging FERET data. Last, we analyze the error structure driven by the fact that most of the errors in identification are due to a relatively small number of face patterns. Open Set TCM-kNN is shown to be suitable for PSEI (pattern specific error inhomogeneities) error analysis in order to identify difficult to recognize faces. PSEI analysis improves biometric performance by removing a small number of those difficult to recognize faces responsible for much of the original error in performance and/or by using data fusion. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18530864&site=ehost-live
702,Pattern recognition: Historical perspective and future directions.,Harry Wechsler,International Journal of Imaging Systems & Technology,08999457,,Mar2000,11,2,101,16.0,13509961,10.1002/1098-1098(2000)11:2<101::AID-IMA1>3.0.CO;2-J,Wiley-Blackwell,Article,"PATTERN recognition systems; PATTERN perception; FORM perception; ARTIFICIAL intelligence; COMPUTER science; ELECTRICAL engineering; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology); Engineering Services",categorization; classification; feature extraction; feature selection; functional approximation; induction; pattern recognition; performance evaluation; predictive learning; representation,"“What being walks sometimes on two feet, sometimes on three, and sometimes on four, and is weakest when it has the most?” —The Sphinx's Riddle Pattern recognition is one of the most important functionalities for intelligent behavior and is displayed by both biological and artificial systems. Pattern recognition systems have four major components: data acquisition and collection, feature extraction and representation, similarity detection and pattern classifier design, and performance evaluation. In addition, pattern recognition systems are successful to the extent that they can continuously adapt and learn from examples; the underlying framework for building such systems is predictive learning. The pattern recognition problem is a special case of the more general problem of statistical regression; it seeks an approximating function that minimizes the probability of misclassification. In this framework, data representation requires the specification of a basis set of approximating functions. Classification requires an inductive principle to design and model the classifier and an optimization or learning procedure for classifier parameter estimation. Pattern recognition also involves categorization: making sense of patterns not previously seen. The sections of this paper deal with the categorization and functional approximation problems; the four components of a pattern recognition system; and trends in predictive learning, feature selection using “natural” bases, and the use of mixtures of experts in classification. © 2000 John Wiley & Sons, Inc. Int J Imaging Syst Technol 11, 101–116, 2000 [ABSTRACT FROM AUTHOR] Copyright of International Journal of Imaging Systems & Technology is the property of John Wiley & Sons, Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=13509961&site=ehost-live
703,Query by transduction.,Harry Wechsler,IEEE Transactions on Pattern Analysis & Machine Intelligence,01628828,,Sep2008,30,9,1557,15.0,34230536,10.1109/TPAMI.2007.70811,IEEE,Article,INFORMATION storage & retrieval systems; EXPERIENTIAL learning; CLASSIFICATION; COMPUTER algorithms; INFORMATION retrieval; COMPUTER systems; QUERY (Information retrieval system); Computer Systems Design Services; Computer systems design and related services (except video game design and development),Active learning; hypothesis testing; Kolmogorov complexity; support vector machine; transductive inference,"There has recently been a growing interest in the use of transductive inference for learning. We expand here the scope of transductive inference to active learning in a stream-based setting. Toward that end, this paper proposes Query-by-Transduction (QBT) as a novel active learning algorithm. QBT queries the label of an example based on the p-values obtained using transduction. We show that QBT is closely related to Query-by-Committee (QBC) using relations between transduction, Bayesian statistical testing, Kuliback-Leibler divergence, and Shannon information. The feasibility and utility of QBT is shown on both binary and multiclass classification tasks using a support vector machine (SVM) as the choice classifier. Our experimental results show that QBT compares favorably, in terms of mean generalization, against random sampling, committee-based active learning, margin-based active learning, and QBC in the stream-based setting. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Pattern Analysis & Machine Intelligence is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=34230536&site=ehost-live
704,Robust face recognition after plastic surgery using region-based approaches.,Harry Wechsler,Pattern Recognition,00313203,,Apr2015,48,4,1257,16.0,100157071,10.1016/j.patcog.2014.10.004,Elsevier B.V.,Article,HUMAN facial recognition software; PLASTIC surgery; ROBUST control; FACIAL expression; CODING theory; COMPUTER reliability,Beautification; Face recognition; Local features; Plastic surgery; Regions of interest (ROI); Response reliability,"This paper advances the use of region-based strategies for addressing the problem of face recognition after plastic surgery. The proposed methods implement the region-based approach in several ways. FARO (FAce Recognition against Occlusions and Expression Variations) divides the face into relevant regions (left eye, right eye, nose and mouth) and then codes them independently using Partitioned Iterated Function System (PIFS) processing. FACE (Face Analysis for Commercial Entities) applies a localized version of image correlation index. Finally, the Split Face Architecture (SFA), adaptive and integrative in nature, can leverage any known recognition method, from PCA to most recent ones (including FARO and FACE), provided that it is possible to divide the face into regions. Experimental results, compared with those available from recent experiments reported in literature, show that our methods yield much better performance than state-of-the art algorithms, both holistic and region based. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=100157071&site=ehost-live
705,Robust human authentication using appearance and holistic anthropometric features,Harry Wechsler,Pattern Recognition Letters,01678655,,Nov2010,31,15,2425,11.0,53405253,10.1016/j.patrec.2010.07.011,Elsevier B.V.,Article,BIOMETRIC identification; ANTHROPOMETRY; DECISION making; STATISTICAL matching; FEATURE extraction; HUMAN facial recognition software; ROBUST control,Anthropometry; Biometrics; Face recognition; Feature selection; Occlusion and disguise; Soft biometrics,"Abstract: We propose here decision-level fusion using neural networks and feature-level fusion using boosting for the purpose of robust human authentication vis-à-vis face occlusion and disguise. Holistic anthropometric and appearance-based features feed the data fusion stage. In addition to standard head and face geometric measurements, the proposed holistic anthropometric features include additional measurements below the face to describe the neck and shoulder and their contextual relations to head and face. The appearance-based features include standard PCA or Fisherfaces. Experimental data shows the feasibility and utility of the proposed hybrid (extended geometry+appearance) approach for robust human authentication vis-à-vis occluded and/or degraded face biometrics. The authentication results presented compare favorably against both appearance-based methods and hybrid methods with anthropometric features confined to face and head. The methods proposed can train on clean data and authenticate on corrupt data, or train on corrupt data and authenticate on clean data. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=53405253&site=ehost-live
706,Robust re-identification using randomness and statistical learning: Quo vadis,Harry Wechsler,Pattern Recognition Letters,01678655,,Oct2012,33,14,1820,8.0,79559551,10.1016/j.patrec.2012.02.005,Elsevier B.V.,Article,"ROBUST control; STATISTICS; BIOMETRIC identification; ELECTRONIC data processing; COMBINATORIAL analysis; COMPUTER architecture; Data Processing, Hosting, and Related Services",Biometrics; Evidence-based management; Face recognition; Identity management; Re-identification; Statistical learning theory,"Abstract: The re-identification problem is to match objects across multiple but possibly disjoint fields of view for the purpose of sequential authentication over space and time. Detection and seeding for initialization do not presume known identity and allow for re-identification of objects and/or faces whose identity might remain unknown. Specific functionalities involved in re-identification include clustering and selection, recognition-by-parts, anomaly and change detection, sampling and tracking, fast indexing and search, sensitivity analysis, and their integration for the purpose of identity management. As re-identification processes data streams and involves change detection and on-line adaptation three complementary statistical learning frameworks, driven by randomness for the purpose of robust prediction, are advanced here to support the functionalities listed earlier and their combination thereof. The intertwined learning frameworks employed are those of (a) semi-supervised learning (SSL); (b) transduction; and (c) conformal prediction. The overall architecture proposed is data-driven and modular, on one side, and discriminative and progressive, on the other side. The architecture is built around autonomic computing and W5+. Autonomic computing or self-management provides for closed-loop control. W5+ answers questions related to What data to consider for sampling and collection, When to capture the data and from Where, and How to best process the data. The Who (is) query is about identity for biometrics, and the Why question for explanation purposes. The challenge addressed throughout is that of evidence-based management to progressively collect and add value to data in order to generate knowledge that leads to purposeful and gainful action including active learning for the overall purpose of re-identification. A venue for future research includes adversarial learning when re-identification is possibly “distracted” using deliberate corrupt information. [Copyright &y& Elsevier] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=79559551&site=ehost-live
707,Spam detection using Random Boost,Harry Wechsler,Pattern Recognition Letters,01678655,,Jul2012,33,10,1237,8.0,76157688,10.1016/j.patrec.2012.03.012,Elsevier B.V.,Article,RANDOM variables; DETECTORS; SPAM email; ROBUST control; PERFORMANCE evaluation; COMPUTATIONAL complexity; RANDOM projection method,Logit Boost; Random Boost; Random Forest; Random projection; Robust learning; Spam detection,"Abstract: This paper proposes two alternative methods of random projections and compares their performance for robust and efficient spam detection when trained using a small number of examples. Robustness refers to learning and adaptation leading to a high level of performance despite data variability, while efficiency is concerned with (i) the complexity of the detection method employed; and (ii) the amount of training resources used for training and retraining. The first method, Random Project, employs a random projection matrix to produce linear combinations of input features, while the second method, Random Boost, employs random feature selection to enhance the performance of the Logit Boost algorithm. Random Boost is, in fact, a combination of Logit Boost and Random Forest. Experimental results, using TREC and CEAS as challenging spam benchmark sets, show that the Random Boost method significantly improves the performance of the spam filter compared to the Logit Boost algorithm (e.g., a 5% increase in AUC, which is the area under the Receiver Operating Characteristic curve), and yields similar classification accuracy compared to the Random Forest method but using only one fourth the runtime complexity of the Random Forest algorithm. Additionally, the Random Boost algorithm also reduces training time by two orders of magnitude compared to Logit Boost, which becomes important during retraining on the ever changing data streams, including adapting to adversarial tactics and “noise” injected by spammers. [Copyright &y& Elsevier] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=76157688&site=ehost-live
708,Towards demographic categorization using gaze analysis.,Harry Wechsler,Pattern Recognition Letters,01678655,,Oct2016 Part 2,82,,226,6.0,119158561,10.1016/j.patrec.2015.08.018,Elsevier B.V.,Article,EYE tracking; MEANS of communication for people with disabilities; AMYOTROPHIC lateral sclerosis; BIOMETRIC eye scanning systems; DEMOGRAPHIC surveys; SUPPORT vector machines; FEATURE extraction,Adaboost; GANT; Gaze analysis; Gender and age categorization; K-fold cross validation; SVM,"Current use of gaze analysis, which is mostly restricted to eye gaze tracking for augmentative and alternative communication (AAC) medium, can benefit people afflicted with amyotrophic lateral sclerosis (ALS). This paper advances the use of gaze analysis for biometrics purposes related to gender and age demographics to benefit applications related to retail space for targeted advertising, behavioral biometrics to benefit health care, and surveillance applications. Towards that end, this paper expands on the recently introduced Gaze ANalysis Technique (GANT) for human identification to combine the length of time spent on observing patterns of interest and the scanning patterns for biometric representation with AdaBoost and super vector machines (SVM) subsequently used for biometric categorization. The experiments conducted show that while the initial results are promising further innovation and development is required to make gaze analysis a viable alternative for demographics categorization, on its own, or together with other biometrics. Further improvements on performance are expected from the derivation, extraction, and use of alternative and novel gaze driven features. This will include among others additional information that is already available about the arc features connecting the fixation points and the dynamics they encode about the roving gaze. [ABSTRACT FROM AUTHOR] Copyright of Pattern Recognition Letters is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=119158561&site=ehost-live
709,Biomechanical modeling of actively controlled rectus extraocular muscle pulleys.,Qi Wei,Scientific Reports,20452322,,4/6/2022,12,1,1,8.0,156155241,10.1038/s41598-022-09220-x,Springer Nature,Article,"PULLEYS; EYE muscles; MAGNETIC resonance imaging; GAZE; AXIOMS; ROTATIONAL motion; Diagnostic Imaging Centers; Overhead Traveling Crane, Hoist, and Monorail System Manufacturing; Material handling equipment manufacturing",,"The Active Pulley Hypothesis (APH) is based on modern functional anatomical descriptions of the oculomotor plant, and postulates behaviors of the orbital pulleys proposed to be positioned by the extraocular muscles (EOMs). A computational model is needed to understand this schema quantitatively. We developed and evaluated a novel biomechanical model of active horizontal rectus pulleys. The orbital (OL) and global (GL) layers of the horizontal rectus EOMs were implemented as separate musculoskeletal strands. Pulley sleeves were modeled as tube-like structures receiving the OL insertion and suspended by elastic strands. Stiffnesses and orientations of pulley suspensions were determined empirically to limit horizontal rectus EOM side-slip while allowing anteroposterior pulley travel. Independent neural drives of the OL greater than GL were assumed. The model was iteratively refined in secondary gazes to implement realistic behavior using the simplest mechanical configuration and neural control strategy. Simulated horizontal rectus EOM paths and pulley positions during secondary gazes were consistent with published MRI measurements. Estimated EOM tensions were consistent with the range of experimentally measured tensions. This model is consistent with postulated bilaminar activity of the EOMs, and the separate roles of the GL in ocular rotation, and OL in pulley positioning. [ABSTRACT FROM AUTHOR] Copyright of Scientific Reports is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=156155241&site=ehost-live
710,Correlation between pelvic floor ultrasound parameters and vaginal pressures in nulliparous women: a subanalysis of the SUM-AN study.,Qi Wei,International Urogynecology Journal,09373462,,Jun2022,33,6,1481,7.0,157529319,10.1007/s00192-022-05117-5,Springer Nature,Article,PELVIC floor; ULTRASONIC imaging; PUBIC symphysis; LOGISTIC regression analysis,Levator ani muscle hiatus; Nulliparous; Pelvic floor hiatus; Three-dimensional endovaginal manometry; Three-dimensional endovaginal ultrasound; Vaginal pressures,"Introduction and hypothesis: Pelvic floor ultrasound is used as a validated technique for measuring levator ani dimensions. Vaginal manometry has been used in the past as a method to assess levator ani muscle (LAM) strength. Whether the combination of both methods can contribute to our understanding of pelvic floor pathophysiology has not yet been described. We hypothesized that as female pelvic floor muscular hiatus increases, the vaginal pressure and strength decrease. Methods: We recruited 20 asymptomatic nulliparous women ages 18–85 years. Minimal levator hiatus (MLH) area, anteroposterior/left-right (AP/LR) diameter ratio, the distance between levator plate and the pubic symphysis (LP-PS) while at rest and squeeze were measured using endovaginal ultrasound (US). Vaginal pressure at rest, squeeze (Kegel) and Valsalva were measured using 3D manometry. Logistic and linear regression analysis was performed to assess correlations. Results: MLH area was negatively correlated with the sum of all the squeeze pressures produced on the four walls of the vagina (p = 0.049, R2 = 0.197). There was also a borderline negative correlation between MLH and the sum of rest pressures (p = 0.09, R2 = 0.15). AP/LR ratio was negatively correlated with the sum of squeeze pressures (p = 0.056, R2 = 0.197). LP-PS distances, both while at rest and during squeeze, were negatively correlated with the vaginal squeeze pressure (p = 0.046, R2 = 0.21; p = 0.011, R2 = 0.31, respectively). LP-V distance, both at rest and during squeeze, was negatively correlated with the sum of squeeze pressures on four vaginal walls (p = 0.02, R2 = 0.25; p = 0.005, R2 = 0.36, respectively). Conclusions: Stronger levator ani muscles, smaller MLH area and a more oval shape of pelvic floor hiatus as assessed by pelvic floor ultrasound are associated with higher squeeze vaginal pressures as assessed by 3D manometry. [ABSTRACT FROM AUTHOR] Copyright of International Urogynecology Journal is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157529319&site=ehost-live
711,Fast ray-tracing of human eye optics on Graphics Processing Units.,Qi Wei,Computer Methods & Programs in Biomedicine,01692607,,May2014,114,3,302,13.0,95713419,10.1016/j.cmpb.2014.02.003,Elsevier B.V.,Article,VISION disorders; RETINAL anatomy; DIAGNOSTIC imaging; MEDICAL screening; COMPUTER simulation; GRAPHICS processing units; Diagnostic Imaging Centers; Other Electronic and Precision Equipment Repair and Maintenance; All Other Miscellaneous Ambulatory Health Care Services,Computational simulation; GPU programming; Human eye optics; Patient specific modeling and simulation; Ray tracing; Vision defects,"Abstract: We present a new technique for simulating retinal image formation by tracing a large number of rays from objects in three dimensions as they pass through the optic apparatus of the eye to objects. Simulating human optics is useful for understanding basic questions of vision science and for studying vision defects and their corrections. Because of the complexity of computing such simulations accurately, most previous efforts used simplified analytical models of the normal eye. This makes them less effective in modeling vision disorders associated with abnormal shapes of the ocular structures which are hard to be precisely represented by analytical surfaces. We have developed a computer simulator that can simulate ocular structures of arbitrary shapes, for instance represented by polygon meshes. Topographic and geometric measurements of the cornea, lens, and retina from keratometer or medical imaging data can be integrated for individualized examination. We utilize parallel processing using modern Graphics Processing Units (GPUs) to efficiently compute retinal images by tracing millions of rays. A stable retinal image can be generated within minutes. We simulated depth-of-field, accommodation, chromatic aberrations, as well as astigmatism and correction. We also show application of the technique in patient specific vision correction by incorporating geometric models of the orbit reconstructed from clinical medical images. [Copyright &y& Elsevier] Copyright of Computer Methods & Programs in Biomedicine is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=95713419&site=ehost-live
712,Levator ani muscle volume and architecture in normal vs. muscle damage patients using 3D endovaginal ultrasound: a pilot study.,Qi Wei,International Urogynecology Journal,09373462,,Sep2022,,,1,,159395461,10.1007/s00192-022-05366-4,Springer Nature,Article,,3D endovaginal ultrasound; Avulsion; Levator ani; Muscle damage,"Introduction and hypothesis: This study aimed to compare the difference in levator ani muscle (LAM) volumes between 'normal' and those with sonographically visualized LAM defects. We hypothesized that the 'muscle damage' group would have a significantly lower muscle volume.The study included patients who had undergone a 3D endovaginal ultrasound. The normal (NM) and damage (DM) muscle groups’ architectural changes were evaluated based on anterior-posterior (AP), left-right (LR) diameter, and minimal levator hiatus (MLH) area. The puboanalis-puboperinealis (PA), puborectalis (PR), and pubococcygeus-iliococcygeus (PC) were manually segmented using 2.5 vs. 1.0 mm to find the optimal sequence and to compare the volumes between NM and DM groups. POPQs were compared between the NM and DM groups.The 1.0-mm segmentation volumes created superior volume analysis. Comparing NM to the DM group showed no significant difference in LAM volume. Respectively, the mean total LAM volumes were 17.27 cm3 (SD = 3.97) and 17.04 cm3 (SD = 4.32), <italic>p</italic> = 0.79. The mean MLH measurements for both groups respectively were 10.06 cm2 (SD = 2.93) and 12.18 cm2 (SD = 2.93), indicating a significant difference (<italic>p</italic> = 0.01). POPQ analysis demonstrated statistically significant differences at Ba and Bp parameters suggesting that the DM group had worse prolapse (<italic>p</italic> = 0.05, 0.01, respectively).While LAM volumes are similar, there is a significant difference in the physical architecture of the LAM and the POPQ parameters in muscle-damaged patients compared to the normal group.Methods: This study aimed to compare the difference in levator ani muscle (LAM) volumes between 'normal' and those with sonographically visualized LAM defects. We hypothesized that the 'muscle damage' group would have a significantly lower muscle volume.The study included patients who had undergone a 3D endovaginal ultrasound. The normal (NM) and damage (DM) muscle groups’ architectural changes were evaluated based on anterior-posterior (AP), left-right (LR) diameter, and minimal levator hiatus (MLH) area. The puboanalis-puboperinealis (PA), puborectalis (PR), and pubococcygeus-iliococcygeus (PC) were manually segmented using 2.5 vs. 1.0 mm to find the optimal sequence and to compare the volumes between NM and DM groups. POPQs were compared between the NM and DM groups.The 1.0-mm segmentation volumes created superior volume analysis. Comparing NM to the DM group showed no significant difference in LAM volume. Respectively, the mean total LAM volumes were 17.27 cm3 (SD = 3.97) and 17.04 cm3 (SD = 4.32), <italic>p</italic> = 0.79. The mean MLH measurements for both groups respectively were 10.06 cm2 (SD = 2.93) and 12.18 cm2 (SD = 2.93), indicating a significant difference (<italic>p</italic> = 0.01). POPQ analysis demonstrated statistically significant differences at Ba and Bp parameters suggesting that the DM group had worse prolapse (<italic>p</italic> = 0.05, 0.01, respectively).While LAM volumes are similar, there is a significant difference in the physical architecture of the LAM and the POPQ parameters in muscle-damaged patients compared to the normal group.Results: This study aimed to compare the difference in levator ani muscle (LAM) volumes between 'normal' and those with sonographically visualized LAM defects. We hypothesized that the 'muscle damage' group would have a significantly lower muscle volume.The study included patients who had undergone a 3D endovaginal ultrasound. The normal (NM) and damage (DM) muscle groups’ architectural changes were evaluated based on anterior-posterior (AP), left-right (LR) diameter, and minimal levator hiatus (MLH) area. The puboanalis-puboperinealis (PA), puborectalis (PR), and pubococcygeus-iliococcygeus (PC) were manually segmented using 2.5 vs. 1.0 mm to find the optimal sequence and to compare the volumes between NM and DM groups. POPQs were compared between the NM and DM groups.The 1.0-mm segmentation volumes created superior volume analysis. Comparing NM to the DM group showed no significant difference in LAM volume. Respectively, the mean total LAM volumes were 17.27 cm3 (SD = 3.97) and 17.04 cm3 (SD = 4.32), <italic>p</italic> = 0.79. The mean MLH measurements for both groups respectively were 10.06 cm2 (SD = 2.93) and 12.18 cm2 (SD = 2.93), indicating a significant difference (<italic>p</italic> = 0.01). POPQ analysis demonstrated statistically significant differences at Ba and Bp parameters suggesting that the DM group had worse prolapse (<italic>p</italic> = 0.05, 0.01, respectively).While LAM volumes are similar, there is a significant difference in the physical architecture of the LAM and the POPQ parameters in muscle-damaged patients compared to the normal group.Conclusions: This study aimed to compare the difference in levator ani muscle (LAM) volumes between 'normal' and those with sonographically visualized LAM defects. We hypothesized that the 'muscle damage' group would have a significantly lower muscle volume.The study included patients who had undergone a 3D endovaginal ultrasound. The normal (NM) and damage (DM) muscle groups’ architectural changes were evaluated based on anterior-posterior (AP), left-right (LR) diameter, and minimal levator hiatus (MLH) area. The puboanalis-puboperinealis (PA), puborectalis (PR), and pubococcygeus-iliococcygeus (PC) were manually segmented using 2.5 vs. 1.0 mm to find the optimal sequence and to compare the volumes between NM and DM groups. POPQs were compared between the NM and DM groups.The 1.0-mm segmentation volumes created superior volume analysis. Comparing NM to the DM group showed no significant difference in LAM volume. Respectively, the mean total LAM volumes were 17.27 cm3 (SD = 3.97) and 17.04 cm3 (SD = 4.32), <italic>p</italic> = 0.79. The mean MLH measurements for both groups respectively were 10.06 cm2 (SD = 2.93) and 12.18 cm2 (SD = 2.93), indicating a significant difference (<italic>p</italic> = 0.01). POPQ analysis demonstrated statistically significant differences at Ba and Bp parameters suggesting that the DM group had worse prolapse (<italic>p</italic> = 0.05, 0.01, respectively).While LAM volumes are similar, there is a significant difference in the physical architecture of the LAM and the POPQ parameters in muscle-damaged patients compared to the normal group. [ABSTRACT FROM AUTHOR] Copyright of International Urogynecology Journal is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=159395461&site=ehost-live
713,"Posterior perineal translevator hernia: application of MRI, 3D ultrasound, and computerized modeling.",Qi Wei,International Urogynecology Journal,09373462,,Oct2018,29,10,1559,3.0,131820451,10.1007/s00192-018-3714-3,Springer Nature,Article,PERINEAL care; PELVIC floor; PELVIC surgery; ULTRASONIC imaging; MAGNETIC resonance imaging; DISEASES; Diagnostic Imaging Centers,Pelvic floor MRI; Pelvic floor ultrasound; Translevator hernia,,http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=131820451&site=ehost-live
714,Uncertainty in Limb Configuration Makes Minimal Contribution to Errors Between Observed and Predicted Forces in a Musculoskeletal Model of the Rat Hindlimb.,Qi Wei,IEEE Transactions on Biomedical Engineering,00189294,,Feb2018,65,2,469,8.0,127409149,10.1109/TBME.2017.2775598,IEEE,Article,MUSCULOSKELETAL system; LOCOMOTION; CALIBRATION; GROUND reaction forces (Biomechanics); ROBOTICS; MONTE Carlo method,Biomechanical simulation; Biomedical measurement; Force; Force measurement; Monte Carlo simulation; Muscles; musculoskeletal model; Predictive models; rat hindlimb; Rats,"Subject-specific musculoskeletal models are increasingly used in biomedical applications to predict endpoint forces due to muscle activation, matching predicted forces to experimentally observed forces at a specific limb configuration. However, it is difficult to precisely measure the limb configuration at which these forces are observed. The consequent uncertainty in limb configuration might contribute to errors in model predictions. We therefore evaluated how uncertainties in limb configuration measurement contributed to errors in force prediction, using data from in vivo measurements in the rat hindlimb. We used a data-driven approach to estimate the uncertainty in estimated limb configuration and then used this configuration uncertainty to evaluate the consequent uncertainty in force predictions, using Monte Carlo simulations. We used subject-specific models of joint structures (i.e., centers and axes of rotation) in order to estimate limb configurations for each animal. The standard deviation of the distribution of predicted force directions resulting from configuration uncertainty was small, ranging between 0.27° and 3.05° across muscles. For most muscles, this standard deviation was considerably smaller than the error between observed and predicted forces (between 0.57° and 70.96°), suggesting that uncertainty in limb configuration could not explain inaccuracies in model predictions. Instead, our results suggest that inaccuracies in muscle model parameters, most likely in parameters specifying muscle moment arms, are the main source of prediction errors by musculoskeletal models in the rat hindlimb. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Biomedical Engineering is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=127409149&site=ehost-live
715,Zebrafish larvae heartbeat detection from body deformation in low resolution and low frequency video.,Qi Wei,Medical & Biological Engineering & Computing,01400118,,Dec2018,56,12,2353,13.0,133056270,10.1007/s11517-018-1863-7,Springer Nature,journal article,ZEBRA danio; FISH larvae; FISH genetics; HEART beat; DEFORMATIONS (Mechanics),Dense optical flow; Motion tracking; Multiresolution; Principal component analysis; Zebrafish heartbeat,"Zebrafish (Danio rerio) is a powerful animal model used in many areas of genetics and disease research. Despite its advantages for cardiac research, the heartbeat pattern of zebrafish larvae under different stress conditions is not well documented quantitatively. Several effective automated heartbeat detection methods have been developed to reduce the workload for larva heartbeat analysis. However, most require complex experimental setups and necessitate direct observation of the larva heart. In this paper, we propose the Zebrafish Heart Rate Automatic Method (Z-HRAM), which detects and tracks the heartbeats of immobilized, ventrally positioned zebrafish larvae without direct larva heart observation. Z-HRAM tracks localized larva body deformation that is highly correlated with heart movement. Multiresolution dense optical flow-based motion tracking and principal component analysis are used to identify heartbeats. Here, we present results of Z-HRAM on estimating heart rate from video recordings of seizure-induced larvae, which were of low resolution (1024 × 760) and low frame rate (3 to 4 fps). Heartbeats detected from Z-HRAM were shown to correlate reliably with those determined through corresponding electrocardiogram and manual video inspection. We conclude that Z-HRAM is a robust, computationally efficient, and easily applicable tool for studying larva cardiac function in general laboratory conditions. Graphical abstract Flowchart of the automatic zebrafish heartbeat detection. [ABSTRACT FROM AUTHOR] Copyright of Medical & Biological Engineering & Computing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=133056270&site=ehost-live
716,An Attack Vector Taxonomy for Mobile Telephony Security Vulnerabilities.,Duminda Wijesekera,Computer (00189162),00189162,,Apr2021,54,4,76,9.0,149806637,10.1109/MC.2021.3057059,IEEE,Article,TAXONOMY; SECURITY management; CELL phone systems; Wireless Telecommunications Carriers (except Satellite),,"A simplified cybersecurity threat matrix may provide a unifying way to define the security risk posed by current and future generations of mobile telephony. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=149806637&site=ehost-live
717,An authorization model for multimedia digital libraries.,Duminda Wijesekera,International Journal on Digital Libraries,14325012,,2004,4,3,139,17.0,14910124,10.1007/s00799-004-0080-1,Springer Nature,Article,MULTIMEDIA systems; DIGITAL libraries; ACCESS control; METADATA; SEMANTICS; Libraries and Archives,Access control; Continuous media; Digital library; Metadata; Semantics,"In this paper we present ageneralized authorization modelfor multimedia digital libraries. Our aim is to support the enforcement of access control requirements of the original data sources without the need to create a new, unified model for the digital library. We integrate the three most widely used access control models (i.e., mandatory, discretionary, and role-based) within a single framework, allowing seamless accesses to data protected by these security models. In particular, we address the access control needs ofcontinuous media datawhile supporting quality of service (QoS) requirements and preserving operational semantics. The technical core of the paper focuses on the development ofmetadataand the correspondingmetastructureto represent authorization policies and QoS requirements and shows their applicabilty to continuous media. We define our security objects based on the Synchronized Multimedia Integration Language (SMIL), which controls multimedia presentations. Following the synchronization constructs <par> and <seq> of SMIL, we define a normal form for multimedia streams, calledSMIL normal form. SMIL normal form provides a syntax-independent representation of semantically equivalent multimedia data. SMIL normal form compositions are extended (decorated) with RDF statements, representing security and QoS metadata. Interpretation of these statements and, therefore, the authorization and QoS requirements of the decorated multimedia object are defined by the metastructure, represented as a DAML+OIL ontology. We propose the concept ofgeneralized subjectthat encompasses all access permissions of a given user regardless of the multiple permissions in different access control models. Finally, we develop methods to generate secure views for each generalized subject and retrieve them using a secure multimedia server. [ABSTRACT FROM AUTHOR] Copyright of International Journal on Digital Libraries is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=14910124&site=ehost-live
718,An ontology-based distributed whiteboard to determine legal responses to online cyber attacks.,Duminda Wijesekera,Internet Research,10662243,,2006,16,5,475,16.0,24638211,10.1108/10662240610710969,Emerald Publishing Limited,Article,DATA protection; DECISION making; CYBERSPACE; COMPUTER crimes; COMPUTER systems; CYBERTERRORISM; LEGAL documents; INFORMATION services; PEACE officers; All Other Information Services; Computer systems design and related services (except video game design and development); Computer Systems Design Services,Decision support systems; Law; Systems theory; Worldwide web,"Purpose — This paper aims to assist investigators and attorneys addressing the legal aspects of cyber incidents, and allow them to determine the legality of a response to cyber attacks by using the Worldwide web securely. Design/methodology/approach — Develop a decision support legal whiteboard that graphically constructs legal arguments as a decision tree. The tree is constructed using a tree of questions and appending legal documents to substantiate the answers that are known to hold in anticipated legal challenges. Findings — The tool allows participating group of attorneys to meet in cyberspace in real time and construct a legal argument graphically by using a decision tree. They can construct sub-parts of the tree from their own legal domains. Because diverse legal domains use different nomenclatures, this tool provides the user the capability to index and search legal documents using a complex international legal ontology that goes beyond the traditional LexisNexis-like legal databases. This ontology itself can be created using the tool from distributed locations. Originality/value — This tool has been fine-tuned through numerous interviews with attorneys teaching and practicing in the area of cyber crime, cyber espionage, and military operations in cyberspace. It can be used to guide forensic experts and law enforcement personnel during their active responses and off-line examinations. [ABSTRACT FROM AUTHOR] Copyright of Internet Research is the property of Emerald Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=24638211&site=ehost-live
719,Can You Trust Zero Trust?,Duminda Wijesekera,Computer (00189162),00189162,,Aug2022,55,8,103,3.0,158333510,10.1109/MC.2022.3178813,IEEE,Article,TRUST; COMPUTER architecture,Computer architecture,"Developing and sustaining a ""zero-trust architecture"" is essentially impossible today. The concept is currently a moving target, and its meaning is in the eye of the beholder. One thing we know for certain is that it's a misnomer. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158333510&site=ehost-live
720,Detecting VoIP Floods Using the Hellinger Distance.,Duminda Wijesekera,IEEE Transactions on Parallel & Distributed Systems,10459219,,Jun2008,19,6,794,12.0,32522747,10.1109/TPDS.2007.70786,IEEE,Article,INTERNET telephony; COMPUTER network protocols; MARKET share; INTERNET; DISTRIBUTION (Probability theory); EXPERIMENTS; TRAFFIC flow; DIFFERENCES; DATA packeting; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals; All Other Telecommunications,flooding attacks; Hellinger distance; VOIP,"Voice over IP (VoIP), also known as Internet telephony, is gaining market share rapidly and now competes favorably as one of the visible applications of the Internet. Nevertheless, being an application running over the TCP/IP suite, it is susceptible to flooding attacks. If flooded, as a time-sensitive service, VoIP may show noticeable service degradation and even encounter sudden service disruptions. Because multiple protocols are involved in a VoIP service and most of them are susceptible to flooding, an effective solution must be able to detect and overcome hybrid floods. As a solution, we offer the VoIP Flooding Detection System (vFDS)—an online statistical anomaly detection framework that generates alerts based on abnormal variations in a selected hybrid collection of traffic flows. It does so by viewing collections of related packet streams as evolving probability distributions and measuring abnormal variations in their relationships based on the Hellinger distance—a measure of variability between two probability distributions. Experimental results show that vFDS is fast and accurate in detecting flooding attacks, without noticeably increasing call setup times or introducing jitter into the voice streams. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Parallel & Distributed Systems is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=32522747&site=ehost-live
721,Modeling Human-in-the-Loop Security Analysis and Decision-Making Processes.,Duminda Wijesekera,IEEE Transactions on Software Engineering,00985589,,Jan2014,40,2,154,13.0,94842622,10.1109/TSE.2014.2302433,IEEE,Article,MATHEMATICAL models of decision making; WORKFLOW management; SOFTWARE engineering; COMPUTER security; PRODUCTION engineering,Analytical models; Business; Formal methods; Formal specifications; information assurance; Object oriented modeling; process modeling; Runtime; Software; software engineering; statechart assertions; Unified modeling language; verification and validation,"This paper presents a novel application of computer-assisted formal methods for systematically specifying, documenting, statically and dynamically checking, and maintaining human-centered workflow processes. This approach provides for end-to-end verification and validation of process workflows, which is needed for process workflows that are intended for use in developing and maintaining high-integrity systems. We demonstrate the technical feasibility of our approach by applying it on the development of the US government's process workflow for implementing, certifying, and accrediting cross-domain computer security solutions. Our approach involves identifying human-in-the-loop decision points in the process activities and then modeling these via statechart assertions. We developed techniques to specify and enforce workflow hierarchies, which was a challenge due to the existence of concurrent activities within complex workflow processes. Some of the key advantages of our approach are: it results in development of a model that is executable, supporting both upfront and runtime checking of process-workflow requirements; aids comprehension and communication among stakeholders and process engineers; and provides for incorporating accountability and risk management into the engineering of process workflows. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Software Engineering is the property of IEEE Computer Society and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=94842622&site=ehost-live
722,Reasoning with advanced policy rules and its application to access control.,Duminda Wijesekera,International Journal on Digital Libraries,14325012,,2004,4,3,156,15.0,14910126,10.1007/s00799-004-0078-8,Springer Nature,Article,ACCESS control; COMPUTER security; INFORMATION storage & retrieval systems; ELECTRONIC information resources; DIGITAL libraries; Libraries and Archives,Access control; Obligations; Policies; Policy rule evaluation; Provisions,"This paper presents a formal framework to represent and manage advanced policy rules, which incorporate the notions ofprovisionandobligation. Provisions are those conditions that need to be satisfied or actions that must be performed by a user or an agent before a decision is rendered, while obligations are those conditions or actions that must be fulfilled by either the user or agent or by the system itself within a certain period of time after the decision. This paper proposes a specific formalism to express provisions and obligations within a policy and investigates a reasoning mechanism within this framework. A policy decision may be supported by more than one rule-based derivation, each associated with a potentially different set of provisions and obligations (called a global PO set). The reasoning mechanism can derive all the global PO sets for each specific policy decision and facilitates the selection of the best one based on numerical weights assigned to provisions and obligations as well as on semantic relationships among them. The formal results presented in the paper hold for many applications requiring the specification of policies, but this paper illustrates the use of the proposed policy framework in the security domain only. [ABSTRACT FROM AUTHOR] Copyright of International Journal on Digital Libraries is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=14910126&site=ehost-live
723,Removing Permissions in the Flexible Authorization Framework.,Duminda Wijesekera,ACM Transactions on Database Systems,03625915,,Sep2003,28,3,209,21.0,10885768,10.1145/937598.937599,Association for Computing Machinery,Article,COMPUTER security; SECURITY systems; DATA protection; LOGIC programming; COMPUTER programming; ACCESS control; Computer systems design and related services (except video game design and development); Other Computer Related Services; Custom Computer Programming Services; Security Systems Services (except Locksmiths),Access control policy; authorization; logic programming,"The Flexible Authorization Framework (FAF) defined by Jajodia et al. [2001] provides a stratified-logic-programming-based framework for specifying access control policies that is expressive enough to specify many known access control policies. Although the original formulation of FAF indicated how rules could be added to or deleted from a FAF specification, it did not address the removal of access permissions from users. We present two options for removing permissions in FAF and provide details on the option which is representation independent. [ABSTRACT FROM AUTHOR] Copyright of ACM Transactions on Database Systems is the property of Association for Computing Machinery and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=10885768&site=ehost-live
724,Securing the ZigBee Protocol in the Smart Grid.,Duminda Wijesekera,Computer (00189162),00189162,,Apr2012,45,4,92,0.0,74133618,10.1109/MC.2012.146,IEEE,Article,"COMPUTER network protocols; CYBERTERRORISM; COMPUTER security; SMART power grids; COMPUTER science; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",Authentication; Educational institutions; formal methods; NIST; protocol verification; Protocols; security; smart grid; Smart grids; Zigbee,"The design-implement-fix process is sufficient for previously unknown attack vectors. However, engineers should use established knowledge, such as known attack patterns, in the analysis of security protocols prior to their acceptance and implementation. [ABSTRACT FROM AUTHOR] Copyright of Computer (00189162) is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=74133618&site=ehost-live
725,SPUTERS: An Integrated Traffic Surveillance and Emergency Response Architecture.,Duminda Wijesekera,Journal of Intelligent Transportation Systems,15472450,,Jan-Mar2005,9,1,11,12.0,16908858,10.1080/15472450590912538,Taylor & Francis Ltd,Article,"INFORMATION storage & retrieval systems; TRAFFIC accidents; TRAFFIC surveys; TRAFFIC accident investigation; EMERGENCY communication systems; EMERGENCY management; TRAFFIC engineering; COMMUNICATIONS industries; Wired Telecommunications Carriers; Wireless Telecommunications Carriers (except Satellite); Satellite Telecommunications; Telecommunications Resellers; All Other Telecommunications; Other federal protective services; Other provincial protective services; Other municipal protective services; Emergency and Other Relief Services; Other Justice, Public Order, and Safety Activities; Engineering Services",Incident Response; Privacy; Real-Time Multimedia; Security; Traffic Surveillance,"Secure Progressively Updatable Traffic Emergency Response System (SPUTERS) is a framework for collecting traffic-surveillance data in crash-prone areas of roadways. SPUTERS receives as input video, audio, and text-based data from an integrated collection of distributed cameras and sensors, both vehicle-borne and those embedded in the roadway infrastructure. Under nominal roadway operating conditions, the multimedia data are used to develop incident countermeasures. During degraded modes of roadway operation, SPUTERS generates progressive updates to create a shared situational awareness among the people who orchestrate responses to traffic incidents and emergencies. In addition to addressing performance-oriented quality-of-service requirements for interactive display of data on handheld devices, SPUTERS relies on role-based access control to safeguard the privacy of roadway users under surveillance, preventing privileged information collected and managed by the system from being leaked to unauthorized persons. In order to enhance security, SPUTERS uses symmetric key encryption to guard against the unauthorized alteration of the surveillance data. [ABSTRACT FROM AUTHOR] Copyright of Journal of Intelligent Transportation Systems is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=16908858&site=ehost-live
726,SS7 Over IP: Signaling Interworking Vulnerabilities.,Duminda Wijesekera,IEEE Network,08908044,,Nov/Dec2006,20,6,32,10.0,23194297,10.1109/MNET.2006.273119,IEEE,Article,SWITCHING systems (Telecommunication); INTERNETWORKING devices; INTERNET telephony signaling; TELEPHONE signaling; COMPUTER networks; DIGITAL telephone systems; INTERNET; COMPUTER network architectures; COMPUTER security; INTERNETWORKING; Computer Systems Design Services; Wired Telecommunications Carriers; Internet Publishing and Broadcasting and Web Search Portals,,"The article discusses the solution on how to avoid exploitation of intersignaling potential provided by SIGTRAN, a signaling interface which provides seamless interconnectivity between public telephony and Internet Protocol telephony. The proposed solution is based on access control, signaling screening and detecting anomalous signaling. It argues that syntactic correctness, semantic validity of the signal content and the appropriateness of a particular signal in the context of earlier exchanged messages should be considered.",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=23194297&site=ehost-live
727,Tableaux for constructive concurrent dynamic logic,Duminda Wijesekera,Annals of Pure & Applied Logic,01680072,,Sep2005,135,3-Jan,1,72.0,18150453,10.1016/j.apal.2004.12.001,Elsevier B.V.,Article,"INFORMATION theory; COMPUTER science; LOGIC design; LOGIC circuits; Research and development in the physical, engineering and life sciences; Research and Development in the Physical, Engineering, and Life Sciences (except Biotechnology)",,"Abstract: This is the first paper on constructive concurrent dynamic logic (CCDL). For the first time, either for concurrent or sequential dynamic logic, we give a satisfactory treatment of what statements are forced to be true by partial information about the underlying computer. Dynamic logic was developed by Pratt [V. Pratt, Semantical considerations on Floyd–Hoare logic, in: 17th Annual IEEE Symp. on Found. Comp. Sci., New York, 1976, pp. 109–121, V. Pratt, Applications of modal logic to programming, Studia Logica 39 (1980) 257–274] for nondeterministic sequential programs, and by Peleg [D. Peleg, Concurrent dynamic logic, Journal of the Association for Computing Machinery 34 (2) (1987), D. Peleg, Communication in concurrent dynamic logic, Journal of Computer and System Sciences 35 (1987)] for concurrent programs, for the purpose of proving properties of programs such as correctness. Here we define what it means for a dynamic logic formula to be forced to be true knowing only partial information about the results of assignments and tests. This informal CCDL semantics is formalized by intuitionistic Kripke frames modeling this partial information, and each such frame is interpreted as an idealized concurrent machine (a concurrent transition system). In CCDL, proofs and deductions are -height, -branching, well-founded labeled subtrees of . These are a generalization of the signed tableaux of Nerode [A. Nerode, Some lectures in modal logic, Technical Report, M.S.I. Cornell University, 1989, CIME Logic and Computer Science Montecatini Volume, Springer-Verlag Lecture Notes, 1990, A. Nerode, Some lectures in intuitionistic logic, Technical Report, M.S.I. Cornell University, 1988, Marktoberdorf Logic and Computation NATO Summer School Volume, NATO Science Series, 1990 (in press)] stemming from the prefix tableaux of Fitting [M.C. Fitting, Proof Methods for Modal and Intuitionistic Logic, Reidel, 1983]. We demonstrate the correctness of our tableau proofs, define consistency properties, prove that consistency properties yield models, construct systematic tableaux, prove that systematic tableaux yield a consistency property, and conclude that CCDL is complete. This infinitary semantics and proof procedure will be the primary guide for defining, in a sequel, the correct finitary CCDL (FCCDL) based on induction principles. FCCDL is suitable for implementation in constructive logic software systems such as Constable’s NUPRL or Huet-Coquand’s CONSTRUCTIONS. Our goal is to develop a constructive logic programming tool for specification and modular verification of programs in any imperative concurrent language, and for the extraction of concurrent programs from constructive proofs. Subsequent papers will introduce analogous logics for declarative and functional concurrent languages. [Copyright &y& Elsevier] Copyright of Annals of Pure & Applied Logic is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=18150453&site=ehost-live
728,A Comparison Study of Validity Indices on Swarm-Intelligence-Based Clustering.,Jie Xu,"IEEE Transactions on Systems, Man & Cybernetics: Part B",10834419,,Aug2012,42,4,1243,14.0,77875159,10.1109/TSMCB.2012.2188509,IEEE,Article,COMPARATIVE studies; CLUSTER analysis (Statistics); SWARM intelligence; PARTICLE swarm optimization; PARALLEL algorithms; DIFFERENTIAL evolution,Clustering; Clustering algorithms; Context; differential evolution (DE); Encoding; Indexes; Particle swarm optimization; particle swarm optimization (PSO); Partitioning algorithms; swarm intelligence; validity index; Vectors,"Swarm intelligence has emerged as a worthwhile class of clustering methods due to its convenient implementation, parallel capability, ability to avoid local minima, and other advantages. In such applications, clustering validity indices usually operate as fitness functions to evaluate the qualities of the obtained clusters. However, as the validity indices are usually data dependent and are designed to address certain types of data, the selection of different indices as the fitness functions may critically affect cluster quality. Here, we compare the performances of eight well-known and widely used clustering validity indices, namely, the Caliński–Harabasz index, the CS index, the Davies–Bouldin index, the Dunn index with two of its generalized versions, the I index, and the silhouette statistic index, on both synthetic and real data sets in the framework of differential-evolution–particle-swarm-optimization (DEPSO)-based clustering. DEPSO is a hybrid evolutionary algorithm of the stochastic optimization approach (differential evolution) and the swarm intelligence method (particle swarm optimization) that further increases the search capability and achieves higher flexibility in exploring the problem space. According to the experimental results, we find that the silhouette statistic index stands out in most of the data sets that we examined. Meanwhile, we suggest that users reach their conclusions not just based on only one index, but after considering the results of several indices to achieve reliable clustering structures. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Systems, Man & Cybernetics: Part B is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=77875159&site=ehost-live
729,An Optimal Computing Budget Allocation Tree Policy for Monte Carlo Tree Search.,Jie Xu,IEEE Transactions on Automatic Control,00189286,,Jun2022,67,6,2685,15.0,157192214,10.1109/TAC.2021.3088792,IEEE,Article,"STOCHASTIC control theory; MONTE Carlo method; BUDGET cuts; TREES; Flower, Nursery Stock, and Florists' Supplies Merchant Wholesalers",Dynamic programming; Games; Heuristic algorithms; Machine learning; Machine learning algorithms; Monte Carlo methods; Monte Carlo tree search (MCTS); optimization algorithms; Search problems; Space exploration; stochastic optimal control,"We analyze a tree search problem with an underlying Markov decision process, in which the goal is to identify the best action at the root that achieves the highest cumulative reward. We present a new tree policy that optimally allocates a limited computing budget to maximize a lower bound on the probability of correctly selecting the best action at each node. Compared to widely used upper confidence bound (UCB) tree policies, the new tree policy presents a more balanced approach to manage the exploration and exploitation tradeoff when the sampling budget is limited. Furthermore, UCB assumes that the support of reward distribution is known, whereas our algorithm relaxes this assumption. Numerical experiments demonstrate the efficiency of our algorithm in selecting the best action at the root. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157192214&site=ehost-live
730,Multi-fidelity sampling for efficient simulation-based decision making in manufacturing management.,Jie Xu,IISE Transactions,24725854,,Jul2019,51,7,792,14.0,136690083,10.1080/24725854.2019.1576951,Taylor & Francis Ltd,Article,"DECISION making; PRODUCTION planning; PRODUCTION management (Manufacturing); RESOURCE allocation; ROBUST control; Process, Physical Distribution, and Logistics Consulting Services",convergence rate; multi-fidelity models; optimal sampling; production planning; resource allocation; robust manufacturing; Simulation-based decision making,"Today's manufacturers operate in highly dynamic and uncertain market environments. Process-level disturbances present further challenges. Consequently, it is of strategic importance for a manufacturing company to develop robust manufacturing capabilities that can quickly adapt to varying customer demands in the presence of external and internal uncertainty and stochasticity. Discrete-event simulations have been used by manufacturing managers to conduct ""look-ahead"" analysis and optimize resource allocation and production plan. However, simulations of complex manufacturing systems are time-consuming. Therefore, there is a great need for a highly efficient procedure to allocate a limited number of simulations to improve a system's performance. In this article, we propose a multi-fidelity sampling algorithm that greatly increases the efficiency of simulation-based robust manufacturing management by utilizing ordinal estimates obtained from a low-fidelity, but fast, approximate model. We show that the multi-fidelity optimal sampling policy minimizes the expected optimality gap of the selected solution, and thus optimally uses a limited simulation budget. We derive an upper bound for the multi-fidelity sampling policy and compare it with other sampling policies to illustrate the efficiency improvement. We demonstrate its computational efficiency improvement and validate the convergence results derived using both benchmark test functions and two robust manufacturing management case studies. [ABSTRACT FROM AUTHOR] Copyright of IISE Transactions is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=136690083&site=ehost-live
731,Optimal selection of media vehicles using customer databases,Jie Xu,Expert Systems with Applications,09574174,,Dec2012,39,17,13035,11.0,78431424,10.1016/j.eswa.2012.05.095,Elsevier B.V.,Article,DATABASES; RANKING (Statistics); MARKETING; RATE of return; DIRECT costing; DECISION making; PROFIT maximization; CONSTRAINT satisfaction; Marketing Consulting Services,Advertising; Depth of purchase; Dynamic programing; Internet marketing; Mailing list; Media vehicle; Optimization; Revenue-to-cost ratio,"Abstract: This paper investigates the problem where an organization must select among multiple media vehicles for a marketing campaign, and determine how many names from each vehicle to impress (the contact depth). The organization can estimate the return from contacting each prospective customer, and this return decreases as depth increases. Different vehicles have different marginal costs per impression, and may have minimum-spend requirements or activation costs to use the vehicle. Decisions are to be made to maximize profit subject to a constraint on the total amount spent. We proposed an optimization model and two computationally efficient methods that often lead to global optimal solutions under practical assumptions. The model is illustrated with two data sets. [Copyright &y& Elsevier] Copyright of Expert Systems with Applications is the property of Pergamon Press - An Imprint of Elsevier Science and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=78431424&site=ehost-live
732,Stochastic Control Framework for Determining Feasible Alternatives in Sampling Allocation.,Jie Xu,IEEE Transactions on Automatic Control,00189286,,Jun2020,65,6,2647,7.0,143576746,10.1109/TAC.2019.2942005,IEEE,Article,DYNAMIC programming; STATISTICAL decision making; GAUSSIAN distribution; NETWORK hubs,Approximate dynamic programming (ADP); Bayes methods; Dynamic programming; Dynamic scheduling; feasibility determination (FD); Gaussian distribution; multiarmed bandit (MAB); Optimization; ranking and selection (R&S); Resource management; Standards; stochastic control problem (SCP); value function approximation (VFA),"We formulate the optimal dynamic sampling allocation decision problem for feasibility determination as a stochastic control problem in a Bayesian setting. This new formulation addresses the limitations of previous static optimization formulations. In an approximate dynamic programming paradigm, we propose an approximately optimal allocation policy that maximizes a single feature of the value function one step ahead. Numerical results demonstrate the efficiency of the proposed method. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Automatic Control is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143576746&site=ehost-live
733,VMSA: a performance preserving online VM splitting and placement algorithm in dynamic cloud environments.,Jie Xu,Journal of Supercomputing,09208542,,Aug2016,72,8,3169,25.0,117321475,10.1007/s11227-015-1590-x,Springer Nature,Article,"CLOUD computing; ONLINE algorithms; VIRTUAL machine systems; SOFTWARE as a service; CLOUD storage; VIRTUAL storage (Computer science); ONLINE data processing; Data Processing, Hosting, and Related Services",Cloud computing; Dynamic resource demand; Online algorithm; Requirement splitting; Virtual machine placement,"Server consolidation schemes whereby each server is replaced with a virtual machine (VM) and multiple such VMs are run on a single physical server can reduce the number of physical servers needed, and in turn, both the cost and energy consumption in data centers. However, existing schemes have not fully exploited the flexibility in the usage and allocation of virtualization resources, so as to allow one application originally deployed on a single large VM (LVM) to be split and hosted by multiple smaller VMs (SVM). Using multiple SVMs instead of an LVM enables resource allocation at a smaller granularity and thus may further increase the utilization and reduce the number of physical servers. However, a major challenge to overcome when deploying multiple SVMs for one application is to preserve the performance of the application in terms of response delay. In this paper, we show through theoretical analysis and experiments that in order to preserve the performance of the application, one needs to allocate sufficient resources to each SVM, and the total amount of resources required by all the SVMs will exceed that required by the LVM. Nevertheless, we also show that by using the proposed heuristic algorithm called VM splitting and assignment (VMSA), we can substantially improve the utilization and reduce the number of physical servers. [ABSTRACT FROM AUTHOR] Copyright of Journal of Supercomputing is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=117321475&site=ehost-live
734,Dissecting Latency in 360° Video Camera Sensing Systems †.,Zhisheng Yan,Sensors (14248220),14248220,,Aug2022,22,16,6001,25.0,158948175,10.3390/s22166001,MDPI,Article,CAMCORDERS; COMPUTER systems; WIRELESS sensor networks; Audio and Video Equipment Manufacturing; Computer Systems Design Services; Computer systems design and related services (except video game design and development),360° video camera sensing; latency; measurement study; wireless multimedia sensor networks,"360° video camera sensing is an increasingly popular technology. Compared with traditional 2D video systems, it is challenging to ensure the viewing experience in 360° video camera sensing because the massive omnidirectional data introduce adverse effects on start-up delay, event-to-eye delay, and frame rate. Therefore, understanding the time consumption of computing tasks in 360° video camera sensing becomes the prerequisite to improving the system's delay performance and viewing experience. Despite the prior measurement studies on 360° video systems, none of them delves into the system pipeline and dissects the latency at the task level. In this paper, we perform the first in-depth measurement study of task-level time consumption for 360° video camera sensing. We start with identifying the subtle relationship between the three delay metrics and the time consumption breakdown across the system computing task. Next, we develop an open research prototype Zeus to characterize this relationship in various realistic usage scenarios. Our measurement of task-level time consumption demonstrates the importance of the camera CPU-GPU transfer and the server initialization, as well as the negligible effect of 360° video stitching on the delay metrics. Finally, we compare Zeus with a commercial system to validate that our results are representative and can be used to improve today's 360° video camera sensing systems. [ABSTRACT FROM AUTHOR] Copyright of Sensors (14248220) is the property of MDPI and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158948175&site=ehost-live
735,Embedding Pose Information for Multiview Vehicle Model Recognition.,Zhisheng Yan,IEEE Transactions on Circuits & Systems for Video Technology,10518215,,Aug2022,32,8,5467,14.0,158333584,10.1109/TCSVT.2022.3151116,IEEE,Article,POSE estimation (Computer vision); VEHICLE models; CONVOLUTIONAL neural networks; COMPUTER vision; CLASSIFICATION algorithms; VISUAL fields,Computational modeling; Convolutional neural network; Data mining; Feature extraction; fine-grained classification; Integrated circuit modeling; Measurement; pose estimation; scale-aware features; Task analysis; Urban areas; vehicle model recognition,"Vehicle model recognition is a typical fine-grained classification task that has a wide range of application prospects in safe cities and constitutes a research hotspot in the field of computer vision. Vehicles in images can appear at various angles, resulting in large differences in appearance. The existence of “multiviews” renders vehicle model recognition challenging. Recent research on vehicle model recognition has not fully explored the pose information of vehicles in different images, resulting in low model performance. In this study, we use vehicle pose information to solve the multiview vehicle model recognition (MV-VMR) problem and design a convolutional neural network (CNN) model with embedded vehicle pose information, known as the embedding pose CNN (EP-CNN). The proposed model includes two subnetworks: the pose estimation subnetwork (PE-SubNet) and vehicle model classification subnetwork (VMC-SubNet). PE-SubNet extracts the vehicle pose information, including the pose features and vehicle viewpoint. In VMC-SubNet, considering the scale variation of vehicles, an improved squeeze-and-excitation (SE) block, named the MultiSE block is implemented. We embed the vehicle viewpoint into the MultiSE block, which reweighs each channel such that the extracted features elicit different responses to different viewpoints. Subsequently, the pose features and classification features are integrated for classification. Experiments are conducted on the benchmark CompCars web-nature and Stanford Cars datasets. The results demonstrate that the proposed EP-CNN method can achieve higher recognition accuracy than most classic CNN models and several state-of-the-art fine-grained vehicle model classification algorithms. Code has been made available at: https://github.com/HFUT-CV/EP-CNN. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Circuits & Systems for Video Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=158333584&site=ehost-live
736,Targeted delivery of lopinavir to HIV reservoirs in the mesenteric lymphatic system by lipophilic ester prodrug approach.,Lei Yang,Journal of Controlled Release,01683659,,Jan2021,329,,1077,13.0,148861576,10.1016/j.jconrel.2020.10.036,Elsevier B.V.,Article,PRODRUGS; LYMPHATICS; RESERVOIRS; ESTERS; ANTIRETROVIRAL agents; LYMPH nodes; Water and Sewer Line and Related Structures Construction,Chylomicrons; HIV reservoirs; Intestinal lymphatic transport; Lipophilic ester prodrug approach; Lopinavir; Mesenteric lymphatic system,"The combined antiretroviral therapy (cART) can efficiently suppress HIV replication, but the cessation of cART usually results in viral rebound, mostly due to the presence of viral reservoirs. The mesenteric lymphatic system, including mesenteric lymph nodes (MLNs), is an important viral reservoir into which antiretroviral drugs poorly penetrate. In this work, we proposed a novel lipophilic ester prodrug approach, combined with oral lipid-based formulation, to efficiently deliver lopinavir (LPV) to the mesenteric lymph and MLNs. A series of prodrugs was designed using an in-silico model for prediction of affinity to chylomicrons (CMs), and then synthesized. The potential for mesenteric lymphatic targeting and bioconversion to LPV in physiologically relevant media was assessed in vitro and ex vivo. Subsequently, LPV and selected prodrug candidates were evaluated for their in vivo pharmacokinetics and biodistribution in rats. Oral co-administration of lipids alone could not facilitate the delivery of unmodified LPV to the mesenteric lymphatic system and resulted in undetectable levels of LPV in these tissues. However, a combination of the lipophilic prodrug approach with lipid-based formulation resulted in efficient targeting of LPV to HIV reservoirs in mesenteric lymph and MLNs. The maximum levels of LPV in mesenteric lymph were 1.6- and 16.9-fold higher than protein binding-adjusted IC 90 (PA-IC 90) of LPV for HIV-1 (140 ng/mL) following oral administration of simple alkyl ester prodrug and activated ester prodrug, respectively. Moreover, the concentrations of LPV in MLNs were 1.1- and 7.2-fold higher than PA-IC 90 following administration of simple alkyl ester prodrug and activated ester prodrug, respectively. Furthermore, the bioavailability of LPV was also substantially increased following oral administration of activated ester prodrug compared to unmodified LPV. This approach, especially if can be translated to other antiretroviral drugs, has potential for reducing the size of HIV reservoirs within the mesenteric lymphatic system. Unlabelled Image • Antiretrovirals fail to penetrate into HIV reservoirs in mesenteric lymphatic system. • Delivery of prodrugs and lopinavir to HIV reservoirs in lymphatic system. • Activated ester prodrug approach achieved the most efficient delivery of lopinavir. • This approach also improved the systemic exposure to lopinavir. [ABSTRACT FROM AUTHOR] Copyright of Journal of Controlled Release is the property of Elsevier B.V. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=148861576&site=ehost-live
737,Real-Time Task Scheduling for Machine Perception in Intelligent Cyber-Physical Systems.,Shuochao Yao,IEEE Transactions on Computers,00189340,,Aug2022,71,8,1770,14.0,157931348,10.1109/TC.2021.3106496,IEEE,Article,"CYBER physical systems; ARTIFICIAL intelligence; ARTIFICIAL neural networks; PARTITIONS (Building); RESOURCE allocation; NVIDIA Corp.; Other Building Finishing Contractors; Showcase, Partition, Shelving, and Locker Manufacturing",algorithmic priority inversion; Computer architecture; Cyber-physical systems; cyber-physical systems (CPS); Distance measurement; machine intelligence; Neural networks; Pipelines; Real-time scheduling; Real-time systems; Resource management,"This paper explores criticality-based real-time scheduling of neural-network-based machine inference pipelines in cyber-physical systems (CPS) to mitigate the effect of algorithmic priority inversion. We specifically focus on the perception subsystem, an important subsystem feeding other components (e.g., planning and control). In general, priority inversion occurs in real-time systems when computations that are of lower priority are performed together with or ahead of those that are of higher priority. In current machine perception software, significant priority inversion occurs because resource allocation to the underlying neural network models does not differentiate between critical and less critical data within a scene. To remedy this problem, in recent work, we proposed an architecture to partition the input data into regions of different criticality, then formulated a utility-based optimization problem to batch and schedule their processing in a manner that maximizes confidence in perception results, subject to criticality-based time constraints. This journal extension matures the work in several directions: (i) We extend confidence maximization to a generalized utility optimization formulation that accounts for criticality in the utility function itself, offering finer-grained control over resource allocation within the perception pipeline; (ii) we further instantiate and compare two different criticality metrics (distance-based and relative velocity-based) to understand their relative advantages; and (iii) we explore the limitations of the approach, specifically how inaccuracies in criticality-based attention cueing affect performance. All experiments are conducted on the NVIDIA Jetson AGX Xavier platform with a real-world driving dataset. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Computers is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157931348&site=ehost-live
738,A Practical Downlink NOMA Scheme for Wireless LANs.,Kai Zeng,IEEE Transactions on Communications,00906778,,Apr2020,68,4,2236,15.0,143316039,10.1109/TCOMM.2020.2965520,IEEE,Article,WIRELESS LANs; CHANNEL estimation; ALGORITHMS; CELL analysis,Antennas; Downlink; experimentation; Interference; NOMA; Receivers; Resource management; Silicon carbide; successive interference cancellation (SIC); WLAN,"Non-orthogonal multiple access (NOMA) has emerged as a new multiple access paradigm for wireless networks. Although many results have been produced for NOMA, most of them are limited to theoretical exploration and performance analysis in cellular networks. Very limited progress has been made so far in the design of practical NOMA schemes for wireless local area networks (WLANs). In this paper, we propose a practical downlink NOMA scheme for WLANs and evaluate its performance in real-world wireless environments. Our NOMA scheme has three key components: precoder design, user grouping, and successive interference cancellation (SIC). On the transmitter side, we first formulate the precoding design problem as an optimization problem and then devise an efficient algorithm to construct precoders for downlink NOMA transmissions. We further propose a lightweight user grouping algorithm to ensure the success of SIC at the receivers. On the receiver side, we propose a new SIC method to decode the desired signal in the presence of strong interference. In contrast to existing SIC methods, our SIC method does not require channel estimation to decode the signals, thereby improving its resilience to interference. We have built a prototype of the proposed NOMA scheme on a wireless testbed. Experimental results show that, compared to orthogonal multiple access (OMA), the proposed NOMA scheme can significantly improve the weak user’s date rate (93% on average) and considerably improve WLAN’s weighted sum rate (36% on average). [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Communications is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=143316039&site=ehost-live
739,Delay Analysis of Physical-Layer Key Generation in Dynamic Roadside-to-Vehicle Networks.,Kai Zeng,IEEE Transactions on Vehicular Technology,00189545,,Mar2017,66,3,2526,10.0,121854216,10.1109/TVT.2016.2582853,IEEE,Article,VEHICULAR ad hoc networks; WIRELESS communications; MARKOV processes; SIGNAL quantization; POISSON distribution; SAFETY; Wireless Telecommunications Carriers (except Satellite); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing,Channel probing; delay analysis; Delays; dynamic wireless networks; Physical layer; physical layer key generation; Probes; roadside unit; Vehicle dynamics; Vehicles; vehicular networks; Wireless networks,"Secret key generation by extracting the shared randomness in a wireless fading channel is a promising way to ensure wireless communication security. Previous studies only consider key generation in static networks, but real-world key establishments are usually dynamic. In this paper, for the first time, we investigate the pairwise key generation in dynamic wireless networks with a center node and random arrival users (e.g., roadside units (RSUs) with vehicles). We establish the key generation model for these kinds of networks. We propose a method based on discrete Markov chain to calculate the average time a user will spend on waiting and completing the key generation, called average key generation delay (AKGD). Our method can tackle both serial and parallel key generation scheduling under various conditions. We propose a novel scheduling method, which exploits wireless broadcast characteristic to reduce AKGD and probing energy. We conduct extensive simulations to show the effectiveness of our model and method. The analytical and simulation results match each other. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Transactions on Vehicular Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=121854216&site=ehost-live
740,Friendly spectrum jamming against MIMO eavesdropping.,Kai Zeng,Wireless Networks (10220038),10220038,,Aug2022,28,6,2437,17.0,157629771,10.1007/s11276-022-02967-1,Springer Nature,Article,INDEPENDENT component analysis; WIRELESS communications; EAVESDROPPING; AMPLITUDE modulation; PHASE modulation; RADAR interference; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),Energy modulation; Friendly jamming; MIMO technology,"Friendly spectrum jamming is a flexible scheme to establish secure communications among heterogeneous wireless devices without the need of encryption. Previous works have indicated that this scheme however has weak security strength against multiple antenna eavesdropper in today's wireless communication systems, which limits its wide applicability. To tackle this challenge, we propose a novel modulation method, called energy modulation. The basic idea of our method is to keep the secrecy of the channel state information in modulation, so as to bring high uncertainty to the MIMO's separation and the eavesdropper's decoding. As a result, the security strength of friendly jamming notably increases facing multiple antenna eavesdropper. To demonstrate the effectiveness of our method, we perform independent component analysis to decouple the components of the measured signals with maximum likelihood separation. We find that our solution dramatically decreases the eavesdropper's partial information and has much less bits being compromised comparing with common amplitude and phase modulation. [ABSTRACT FROM AUTHOR] Copyright of Wireless Networks (10220038) is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=157629771&site=ehost-live
741,Impact of Jamming Attacks on Vehicular Cooperative Adaptive Cruise Control Systems.,Kai Zeng,IEEE Transactions on Vehicular Technology,00189545,,Nov2020,69,11,12679,15.0,147041807,10.1109/TVT.2020.3030251,IEEE,Article,MONTE Carlo method; ADAPTIVE control systems; WIRELESS channels; CRUISE control; WIRELESS communications; EQUATIONS of state; Wireless Telecommunications Carriers (except Satellite); Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Motor Vehicle Electrical and Electronic Equipment Manufacturing,Cooperative adaptive cruise control (CACC) security; Couplings; Jamming; Mathematical model; Numerical stability; Safety; Stability analysis; string stability; vehicle-to-vehicle (v2v) communication; Wireless communication; wireless jamming attacks,"Cooperative Adaptive Cruise Control (CACC) is considered as a key enabling technology to automatically regulate the inter-vehicle distances in a vehicle string, and improve the traffic throughput efficiency. In the existing CACC systems, the coupling between wireless communication uncertainty, and system states is not well modeled. In this paper, we integrate the jamming attacks, and wireless channel fading effects into the CACC state space equations such that it effectively captures the coupling impact. Then, we propose a novel time domain approach to analyze the mean string stability (MSS) of such a model. Based on the proposed model, we analyze the impact of the jammer's location on the string stability. We derive a sufficient condition for the packet successful delivery probability which indicates that the jammer has a higher probability to destabilize the string when it is closer to the first vehicle following the lead vehicle. We also propose a methodology to compute the upper, and lower bounds of the inter-vehicle distance trajectories between the lead vehicle, and its follower. Furthermore, string safety is investigated by numerically estimating the collision probability across the string. We conduct comprehensive Monte Carlo simulations to evaluate the stability, and safety of the string in various scenarios. We identify that string stability, and safety are highly influenced by the jamming attacks signal, and jammer's location. We show the consistency between the main results achieved by MSS analysis, and the Monte Carlo simulations. [ABSTRACT FROM AUTHOR] Copyright of IEEE Transactions on Vehicular Technology is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=147041807&site=ehost-live
742,Physical layer key generation in wireless networks: challenges and opportunities.,Kai Zeng,IEEE Communications Magazine,01636804,,Jun2015,53,6,33,7.0,103222661,10.1109/MCOM.2015.7120014,IEEE,Article,WIRELESS channels; RECIPROCITY theorems; WIRELESS communications; TELECOMMUNICATION channels; INTERNET of things; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),Communication system security; Jamming; Network security; Physical layer; Wireless communication; Wireless sensor networks,"Physical layer key generation that exploits reciprocity and randomness of wireless fading channels has attracted considerable research attention in recent years. Although theoretical study has shown its potential to generate information- theoretic secure keys, great challenges remain when transforming the theory into practice. This article provides an overview of the physical layer key generation process and discusses its practical challenges. Different passive and active attacks are analyzed and evaluated through numerical study. A new key generation scheme using random probing signals, and combining user generated randomness and channel randomness, is introduced as a countermeasure against active attacks. The numerical results show that the proposed scheme achieves higher security strength than existing schemes using constant probing signals under active attacks. Future research topics on physical layer key generation are discussed. [ABSTRACT FROM PUBLISHER] Copyright of IEEE Communications Magazine is the property of IEEE and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=103222661&site=ehost-live
743,Physical layer multi-user key generation in wireless networks.,Kai Zeng,Wireless Networks (10220038),10220038,,May2018,24,4,1043,12.0,128888434,10.1007/s11276-016-1389-6,Springer Nature,Article,WIRELESS sensor networks; MULTIUSER computer systems; WIRELESS communications; INTERFERENCE (Telecommunication); WIRELESS sensor nodes; Radio and Television Broadcasting and Wireless Communications Equipment Manufacturing; Wireless Telecommunications Carriers (except Satellite),Lower bound; Multiple users; Parallel probing; Physical layer key generation; Waiting time,"Secret key generation by extracting the shared randomness in the wireless fading channel from physical layer is an interesting topic of practical value. Previous works have focused on the study of physical layer key generation with two nodes from the view point of key generation rate (KGR). Information theoretic limits and the KGRs in implementation have been derived. However, in real-world applications, the physical layer key generation problem involving multiple nodes is the common case, which lacks sufficient study so far. Multi-node case differs from two-node case in that there are two more important considerations: (1) the trade-off between KGR and probing efficiency at individual node pair; (2) channel probing schedule among multiple node pairs. This paper aims at minimizing the <italic>Overall Waiting Time of physical layer key generation with multiple users</italic> (shorten as OWT) through the optimization of probing rates at individual node pair and channel probing schedule. The theoretical lower bound of OWT is derived first, then a practical method (MUKEM) is proposed to compute reasonable probing rates and channel probing schedule for multiple node pairs to obtain a short OWT. Simulations are conducted to evaluate the effectiveness of our method. The results show that 70 % of OWT can be reduced by using our method comparing with one-by-one key generations; while it is only about 8 % longer than the lower bound of OWT. [ABSTRACT FROM AUTHOR] Copyright of Wireless Networks (10220038) is the property of Springer Nature and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=128888434&site=ehost-live
744,Predicting locality phases for dynamic memory optimization,Yutao Zhong,Journal of Parallel & Distributed Computing,07437315,,Jul2007,67,7,783,14.0,25255293,10.1016/j.jpdc.2007.01.010,Academic Press Inc.,Article,DIGITAL signal processing; COMPUTER storage devices; DIGITAL electronics; COMPUTER input-output equipment; Computer Terminal and Other Computer Peripheral Equipment Manufacturing; Computer and peripheral equipment manufacturing; Computer Storage Device Manufacturing,Dynamic optimization; Locality analysis and optimization; Phase hierarchy; Program phase prediction; Reconfigurable architecture,"Abstract: Dynamic data, cache, and memory adaptation can significantly improve program performance when they are applied on long continuous phases of execution that have dynamic but predictable locality. To support phase-based adaptation, this paper defines the concept of locality phases and describes a four-component analysis technique. Locality-based phase detection uses locality analysis and signal processing techniques to identify phases from the data access trace of a program; frequency-based phase marking inserts code markers that mark phases in all executions of the program; phase hierarchy construction identifies the structure of multiple phases; and phase-sequence prediction predicts the phase sequence from program input parameters. The paper shows the accuracy and the granularity of phase and phase-sequence prediction as well as its uses in dynamic data packing, memory remapping, and cache resizing. [Copyright &y& Elsevier] Copyright of Journal of Parallel & Distributed Computing is the property of Academic Press Inc. and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)",http://mutex.gmu.edu/login?url=https://search.ebscohost.com/login.aspx?direct=true&db=a9h&AN=25255293&site=ehost-live
